apiVersion: batch/v1
kind: Job
metadata:
  name: add-avro-container-itgtests-job
spec:
  template:
    metadata:
      name: add-avro-container-itgtests-job
    spec:
      serviceAccountName: itgtests
      containers:
      - name: ibmcase-python
        image: docker-registry.default.svc:5000/itgtests/ibmcase-python:test
        imagePullPolicy: Always
        command: ["bash", "-c" ]
        args:
          - >
              echo "This is the job for adding a container to the containers Kafka topic" &&
              mkdir ${WORKING_DIR} && cd ${WORKING_DIR} &&
              git clone ${GITHUB_REPO} && cd ${REPO_NAME} && if [ "master" != "${BRANCH}" ]; then git checkout ${BRANCH}; fi &&
              cd ${ITGTESTS_LOCATION} &&
              export DATA_SCHEMAS=${WORKING_DIR}/${REPO_NAME}/${DATA_SCHEMAS_LOCATION} &&
              export PYTHONPATH=${PYTHONPATH}:${WORKING_DIR}/${REPO_NAME}/itg-tests:${DATA_SCHEMAS} && 
              python ProduceAvroContainerES.py ${CONTAINER_ID} ${TOPIC_NAME}
        env:
        # Your working directory within the container
        - name: WORKING_DIR
          value: "/tmp/github"
        # Github repository containing the integration tests to be executed
        - name: GITHUB_REPO
          value: "https://github.com/ibm-cloud-architecture/refarch-kc.git"
        # Specific branch for the integration tests
        - name: BRANCH
          value: "master"
        # Name of the repository
        - name: REPO_NAME
          value: "refarch-kc"
        # Location of the integration tests folder within the repository
        - name: ITGTESTS_LOCATION
          value: "itg-tests/ContainersPython"
        # Location of the data schemas folder within the repository
        - name: DATA_SCHEMAS_LOCATION
          value: data_schemas
        # Container ID to be send
        - name: CONTAINER_ID
          value: "avroOCP_01"
        # Event Streams topic name which the event will be sent to
        - name: TOPIC_NAME
          value: "itgtestavro"
        # Event Streams Kafka brokers. You MUST create a configmap in advance with the brokers information:
        # oc create configmap kafka-brokers --from-literal=brokers='<replace with comma-separated list of brokers>' -n <namespace>
        # You can find the list of brokers either in the Event Streams UI or when you login through the CLI: cloudctl es init
        - name: KAFKA_BROKERS
          valueFrom:
            configMapKeyRef:
              name: "kafka-brokers"
              key: brokers
        # Your Event Streams API Key. You can find it in the Event Streams UI on the connect to this cluster link
        # You MUST create this secret in advance with the Event Streams API key:
        # oc create secret generic eventstreams-apikey --from-literal=binding='<replace with api key>' -n <namespace>
        - name: KAFKA_APIKEY
          valueFrom:
            secretKeyRef:
              name: "eventstreams-apikey"
              key: binding
        # Your Event Streams schema registry url. This is different from the Event Streams brokers url.
        # You MUST create this secret in advance.
        # oc create secret generic eventstreams-schemaregistry --from-literal=binding='https://token:<YOUR_EVENT_STREAMS_API_KEY>@<YOUR_EVENT_STREAMS_SCHEMA_REGISTRY_ULR>
        - name: SCHEMA_REGISTRY_URL
          valueFrom:
            secretKeyRef:
              name: "eventstreams-schemaregistry"
              key: binding
        # Based on this parameter we add the appropriate certificates for when Event Streams is hosted either on ICP or OCP
        - name: KAFKA_ENV
          value: "OCP"
        # Location of your Event Streams pem certificate for ssl connections. You MUST download it in advance.
        # This can be downloaded from the Event Streams UI.
        - name: PEM_CERT
          value: "/tmp/certs/es-cert.pem"
        # Location where the Event Streams pem certificate contained in the eventstreams-pem-file secret will be loaded to.
        volumeMounts:
          - mountPath: "/tmp/certs"
            name: eventstreams-pem-file
      volumes:
        # You MUST create this secret with the Event Streams pem certificate in advance. First, download the Event Streams pem certificate.
        # Then create the secret: oc create secret generic eventstreams-pem-file --from-file=es-cert.pem=<LOCATION_OF_YOUR_EVENT_STREAMS_PEM_CERTIFICATE> -n <namespace>
        - name: eventstreams-pem-file
          secret:
            secretName: "eventstreams-pem-file"
      restartPolicy: Never
  backoffLimit: 0