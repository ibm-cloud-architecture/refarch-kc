{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Reefer Container Shipment solution - EDA reference implementation The IBM Event Driven architecture reference implementation illustrates the deployment of real time analytics on event streams in the context of container shipment in an event driven architecture with event backbone, functions as service and microservices. It aims to illustrate the different event driven patterns like event sourcing, CQRS and Saga and give a lot of best practices around implementing event driven microservices. What you will learn How to apply the event storming methodology and workshop to analyze the business process for fresh good shipment over sees. How to transform Domain Driven Design aggregates to microservices. How to implement the different microservices using the event-driven pattern like CQRS pattern with event sourcing done in Apache Kafka or IBM Events Streams. How to deploy your solution to IBM Cloud Kubernetes service (Public cloud), or to IBM Cloud Private (kubernetes based) or run locally with Docker compose. How to use event store (Kafka topics) as source for machine learning data source to build training and test sets. How to implement a Test Driven Development for the Order microservice uisng mockito to avoid Kafka dependency. How to implement the same container management microservices in python, nodejs, java microprofile 2 with Kafka streams API, and springboot, kafka template and PostgreSQL. This is a lot, not all is done yet. But consider this content as a living book, with the left side representing major subjects and each subject with its own table of contents. Target Audiences You will be greatly interested by the subjects addressed in this solution if you are... An architect, you will get a deeper understanding on how all the components work together, and how to address resiliency, high availability. A developer, you will get a broader view of the solution end to end and get existing starting code, and practices you may want to reuse during your future implementation. We focus on event driven solution in hybrid cloud addressing patterns and non-functional requirements as CI/CD, Test Driven Development, ... A project manager, you may understand all the artifacts to develop in an EDA solution, and we may help in the future to do project estimation. Business process statement In this first chapter we are presenting the business process for shipping fresh good over sees and we are detailing the analysis we have done in San Francisco in November 2108 using the event storming analysis workshop. You could read more on how to organize and execute such workshop here . The workshop execution transcript is detailed in a second chapter . Design considerations In the third chapter we are detailing how to transform the analysis outcomes into some light design, conceptual view, just enough to start coding some microservices. Architecture This quick architecture chapter presents the solution components working together with the event backbone. Build and Run The end to end solution can be demonstrated from a unique user interface and it involves multiple microservices deployed independently. As some of those components are using IBM products or IBM Cloud services, you need to provision such services. We propose to develop with an hybrid environment, using IBM Cloud services, local environment running on your laptop and IBM private cloud cluster (optional). As of now only Mac and Linux development workstation are supported. For the Java development we used Eclipse 2019 edition. So basically we need the following: Event Streams instance on IBM Cloud Public or Event streams on IBM Cloud private or a Kafka Docker image. Streaming Analytics on IBM Cloud public or on ICP for Data. Kubernetes Cluster (IBM Cloud Private or IBM Kubernetes Service on cloud) or Docker compose to run locally. Postgresql service in IBM Cloud. This database is used by one service, built with Spring boot, that can be plug and play. It is optional. We want to illustrate with this implementation a reversibility practice where we start development on the cloud and migrate to private cloud. The instructions to build, deploy and test all the solution components, are defined in this source repository: https://github.com/ibm-cloud-architecture/refarch-kc/tree/master/docs . Also, each project, part of the solution, has its own installation explanations and scripts to build, package, test and deploy to the different Kubernetes deployment (private and public). We recommend studying those scripts. Deployments We can deploy the components of the solution into any Kubernetes-based environment - including OpenShift, IBM Cloud Kubernetes Service, and vanilla Kubernetes: Backing Services documents the required environment configuration, as well as Kafka and Postgresql options to satisfy application dependencies. Application Components documents the deployment steps required for the K Container Reference Implementation, as well as integration tests to validate deployment. Still project under development The following items are not yet completed: Microprofile 2.2 container service with kafka streams An end to end automated test scenario A demonstration script to present the process execution from end users point of view A set of tests to validate event sourcing, and Saga patterns Cold chain predictive model Container predictive maintenance model and deployment. Further readings Event driven architecture in IBM Garage method Event driven compagnion github with other best practices Event-driven training journey","title":"Introduction"},{"location":"#reefer-container-shipment-solution-eda-reference-implementation","text":"The IBM Event Driven architecture reference implementation illustrates the deployment of real time analytics on event streams in the context of container shipment in an event driven architecture with event backbone, functions as service and microservices. It aims to illustrate the different event driven patterns like event sourcing, CQRS and Saga and give a lot of best practices around implementing event driven microservices.","title":"Reefer Container Shipment solution - EDA reference implementation"},{"location":"#what-you-will-learn","text":"How to apply the event storming methodology and workshop to analyze the business process for fresh good shipment over sees. How to transform Domain Driven Design aggregates to microservices. How to implement the different microservices using the event-driven pattern like CQRS pattern with event sourcing done in Apache Kafka or IBM Events Streams. How to deploy your solution to IBM Cloud Kubernetes service (Public cloud), or to IBM Cloud Private (kubernetes based) or run locally with Docker compose. How to use event store (Kafka topics) as source for machine learning data source to build training and test sets. How to implement a Test Driven Development for the Order microservice uisng mockito to avoid Kafka dependency. How to implement the same container management microservices in python, nodejs, java microprofile 2 with Kafka streams API, and springboot, kafka template and PostgreSQL. This is a lot, not all is done yet. But consider this content as a living book, with the left side representing major subjects and each subject with its own table of contents.","title":"What you will learn"},{"location":"#target-audiences","text":"You will be greatly interested by the subjects addressed in this solution if you are... An architect, you will get a deeper understanding on how all the components work together, and how to address resiliency, high availability. A developer, you will get a broader view of the solution end to end and get existing starting code, and practices you may want to reuse during your future implementation. We focus on event driven solution in hybrid cloud addressing patterns and non-functional requirements as CI/CD, Test Driven Development, ... A project manager, you may understand all the artifacts to develop in an EDA solution, and we may help in the future to do project estimation.","title":"Target Audiences"},{"location":"#business-process-statement","text":"In this first chapter we are presenting the business process for shipping fresh good over sees and we are detailing the analysis we have done in San Francisco in November 2108 using the event storming analysis workshop. You could read more on how to organize and execute such workshop here . The workshop execution transcript is detailed in a second chapter .","title":"Business process statement"},{"location":"#design-considerations","text":"In the third chapter we are detailing how to transform the analysis outcomes into some light design, conceptual view, just enough to start coding some microservices.","title":"Design considerations"},{"location":"#architecture","text":"This quick architecture chapter presents the solution components working together with the event backbone.","title":"Architecture"},{"location":"#build-and-run","text":"The end to end solution can be demonstrated from a unique user interface and it involves multiple microservices deployed independently. As some of those components are using IBM products or IBM Cloud services, you need to provision such services. We propose to develop with an hybrid environment, using IBM Cloud services, local environment running on your laptop and IBM private cloud cluster (optional). As of now only Mac and Linux development workstation are supported. For the Java development we used Eclipse 2019 edition. So basically we need the following: Event Streams instance on IBM Cloud Public or Event streams on IBM Cloud private or a Kafka Docker image. Streaming Analytics on IBM Cloud public or on ICP for Data. Kubernetes Cluster (IBM Cloud Private or IBM Kubernetes Service on cloud) or Docker compose to run locally. Postgresql service in IBM Cloud. This database is used by one service, built with Spring boot, that can be plug and play. It is optional. We want to illustrate with this implementation a reversibility practice where we start development on the cloud and migrate to private cloud. The instructions to build, deploy and test all the solution components, are defined in this source repository: https://github.com/ibm-cloud-architecture/refarch-kc/tree/master/docs . Also, each project, part of the solution, has its own installation explanations and scripts to build, package, test and deploy to the different Kubernetes deployment (private and public). We recommend studying those scripts.","title":"Build and Run"},{"location":"#deployments","text":"We can deploy the components of the solution into any Kubernetes-based environment - including OpenShift, IBM Cloud Kubernetes Service, and vanilla Kubernetes: Backing Services documents the required environment configuration, as well as Kafka and Postgresql options to satisfy application dependencies. Application Components documents the deployment steps required for the K Container Reference Implementation, as well as integration tests to validate deployment.","title":"Deployments"},{"location":"#still-project-under-development","text":"The following items are not yet completed: Microprofile 2.2 container service with kafka streams An end to end automated test scenario A demonstration script to present the process execution from end users point of view A set of tests to validate event sourcing, and Saga patterns Cold chain predictive model Container predictive maintenance model and deployment.","title":"Still project under development"},{"location":"#further-readings","text":"Event driven architecture in IBM Garage method Event driven compagnion github with other best practices Event-driven training journey","title":"Further readings"},{"location":"introduction/","text":"Introduction As part of producing the IBM event driven point of view and reference architecture, we wanted to bring together a complete scenario which would cover all aspects of developing an event driven solutions including extended connections to devices/IOT and blockchain for trusted business trading networks. We felt that the shipping business could provide a good foundation for this and would enable us to show how to develop event driven solutions following the architecture patterns. Business process statement The high level process can be represented in the following diagram, and is described in detailed in this section : In developing the scenario, it became apparent that the event driven nature of business, extends across the business network, so we have widened the view in the scenario to consider the chain of parties involved in the shipping process, including importer, exporter, land transport and customs. To keep the scenario easy to understand, we have only considered the following cases: Importer Orders goods from exporter overseas Exporter becomes the customer of the shipping agent and uses 'K.Container' shipping service Shipping agent manages process of land transport loading, unloading and shipping. Through the scenario we can see the impact of \u201cevents\u201d, which may delay or change the shipping process across all three parties. We are using goods to be transported in refrigerator containers or reefer containers to keep the 'cold chain' of transported products. Event storming analysis We met with the business users and project stakeholder during an event storming workshop, and we are detailing the outcomes in the next chapter >>","title":"Introduction"},{"location":"introduction/#introduction","text":"As part of producing the IBM event driven point of view and reference architecture, we wanted to bring together a complete scenario which would cover all aspects of developing an event driven solutions including extended connections to devices/IOT and blockchain for trusted business trading networks. We felt that the shipping business could provide a good foundation for this and would enable us to show how to develop event driven solutions following the architecture patterns.","title":"Introduction"},{"location":"introduction/#business-process-statement","text":"The high level process can be represented in the following diagram, and is described in detailed in this section : In developing the scenario, it became apparent that the event driven nature of business, extends across the business network, so we have widened the view in the scenario to consider the chain of parties involved in the shipping process, including importer, exporter, land transport and customs. To keep the scenario easy to understand, we have only considered the following cases: Importer Orders goods from exporter overseas Exporter becomes the customer of the shipping agent and uses 'K.Container' shipping service Shipping agent manages process of land transport loading, unloading and shipping. Through the scenario we can see the impact of \u201cevents\u201d, which may delay or change the shipping process across all three parties. We are using goods to be transported in refrigerator containers or reefer containers to keep the 'cold chain' of transported products.","title":"Business process statement"},{"location":"introduction/#event-storming-analysis","text":"We met with the business users and project stakeholder during an event storming workshop, and we are detailing the outcomes in the next chapter >>","title":"Event storming analysis"},{"location":"pre-requisites/","text":"Pre-requisites To be able to build and execute the solution, you need to do the following tasks: Get a Git client If not already done, get a git client. See the following installation instructions . On a Centos box we did: yum install git Clone all the repositories Start by cloning the root repository using the command: git clone https : // github . com / ibm - cloud - architecture / refarch - kc / Then go to the refarch-kc folder and use the command: . / script / clone . sh to get all the solution repositories. You should have at least the following repositories: refarch - kc - container - ms refarch - kc - order - ms refarch - kc - ui refarch - kc refarch - kc - ms refarch - kc - streams Get docker Get docker engine and install it (if not yet done). To verify docker runs fine use the command docker version . We run on v19.03 community edition. or use one of the packaged solution like on Centos: yum install docker Verifying current environment To assess the tools installed on your computer, run the following command under the refarch-kc project: . / scripts / prepareEnv The script will create the docker images for maven, nodejs, and python if those tools are not found on your computer. The images are built from our docker files you can find under the refarch-kc/dockers folder. docker images REPOSITORY TAG IMAGE ID CREATED SIZE ibmcase / python latest 8 d38aefd0346 2 minutes ago 1 . 14 GB ibmcase / nodetools latest 7 a736a07ba09 2 minutes ago 959 MB ibmcase / javatools latest bdf79f64d721 23 minutes ago 715 MB You to have two choices to build the solution: installing node, python and maven on your computer use our own docker images for running those tools You can mix the two. If, for example, you already developed with Nodejs or Java you may want to leverage your own configurations. If you do not want to impact your python environment, you can user our docker images. To be able to build without our docker images do the following: Get Java Do something like this: yum install java - 1 . 8 . 0 - openjdk - devel Get Maven get maven and add it to your PATH. or for a Centos linux: yum install maven Get nodejs Get node and npm or for a Centos linux: yum install node Get Python 3.7 Our integration tests are done in python. To avoid impacting your own python environment, we defined a docker file to build an image with the necessary python library. The image may have been already built with the preparenv script run in previous step. If you want to rebuild it, go to the docker folder and run the following command: docker build - f docker - python - tools - t ibmcase / python . Global environment settings In the refarch-kc rename ./script/setenv.sh.tmpl to ./script/setenv.sh : mv setenv . sh . tmpl setenv . sh Then modify the environment variables according to your environment you are using. This file is used by a lot of scripts in the solution to set the target deployment environment: LOCAL, IBMCLOUD, ICP, MINIKUBE.","title":"Pre-requisites"},{"location":"pre-requisites/#pre-requisites","text":"To be able to build and execute the solution, you need to do the following tasks:","title":"Pre-requisites"},{"location":"pre-requisites/#get-a-git-client","text":"If not already done, get a git client. See the following installation instructions . On a Centos box we did: yum install git","title":"Get a Git client"},{"location":"pre-requisites/#clone-all-the-repositories","text":"Start by cloning the root repository using the command: git clone https : // github . com / ibm - cloud - architecture / refarch - kc / Then go to the refarch-kc folder and use the command: . / script / clone . sh to get all the solution repositories. You should have at least the following repositories: refarch - kc - container - ms refarch - kc - order - ms refarch - kc - ui refarch - kc refarch - kc - ms refarch - kc - streams","title":"Clone all the repositories"},{"location":"pre-requisites/#get-docker","text":"Get docker engine and install it (if not yet done). To verify docker runs fine use the command docker version . We run on v19.03 community edition. or use one of the packaged solution like on Centos: yum install docker","title":"Get docker"},{"location":"pre-requisites/#verifying-current-environment","text":"To assess the tools installed on your computer, run the following command under the refarch-kc project: . / scripts / prepareEnv The script will create the docker images for maven, nodejs, and python if those tools are not found on your computer. The images are built from our docker files you can find under the refarch-kc/dockers folder. docker images REPOSITORY TAG IMAGE ID CREATED SIZE ibmcase / python latest 8 d38aefd0346 2 minutes ago 1 . 14 GB ibmcase / nodetools latest 7 a736a07ba09 2 minutes ago 959 MB ibmcase / javatools latest bdf79f64d721 23 minutes ago 715 MB You to have two choices to build the solution: installing node, python and maven on your computer use our own docker images for running those tools You can mix the two. If, for example, you already developed with Nodejs or Java you may want to leverage your own configurations. If you do not want to impact your python environment, you can user our docker images. To be able to build without our docker images do the following:","title":"Verifying current environment"},{"location":"pre-requisites/#get-java","text":"Do something like this: yum install java - 1 . 8 . 0 - openjdk - devel","title":"Get Java"},{"location":"pre-requisites/#get-maven","text":"get maven and add it to your PATH. or for a Centos linux: yum install maven","title":"Get Maven"},{"location":"pre-requisites/#get-nodejs","text":"Get node and npm or for a Centos linux: yum install node","title":"Get nodejs"},{"location":"pre-requisites/#get-python-37","text":"Our integration tests are done in python. To avoid impacting your own python environment, we defined a docker file to build an image with the necessary python library. The image may have been already built with the preparenv script run in previous step. If you want to rebuild it, go to the docker folder and run the following command: docker build - f docker - python - tools - t ibmcase / python .","title":"Get Python 3.7"},{"location":"pre-requisites/#global-environment-settings","text":"In the refarch-kc rename ./script/setenv.sh.tmpl to ./script/setenv.sh : mv setenv . sh . tmpl setenv . sh Then modify the environment variables according to your environment you are using. This file is used by a lot of scripts in the solution to set the target deployment environment: LOCAL, IBMCLOUD, ICP, MINIKUBE.","title":"Global environment settings"},{"location":"security/","text":"Security This section addresses some challenges and best practices to support security for event-driven microservice integrating Kafka consumer or producer, or both, into the code, and CI/CD platform. Kafka consumers and producers need at least the broker URLs, the security API key in the case of Event Stream deployed on IBM Cloud public or private, and the SSL certificate as the communication is encrypted. Applying the \"store config in the environment\" rule of the 12 factors manisfesto, we are using the following environment variables: export KAFKA_BROKERS = \"localhost:9092\" export KAFKA_APIKEY = \"nokey\" export KAFKA_ENV = \"LOCAL\" export POSTGRESQL_URL = \"jdbc:postgresql://localhost:5432/postgres\" export POSTGRESQL_USER = \"postgres\" export POSTGRESQL_PWD = \"supersecret\" export POSTGRESQL_CA_PEM = \"\" The settings above are for running locally on the developer's laptop. To support the different environment (LOCAL, ICP, IBMCLOUD), we have a scripts/setenv.sh bash script to help export those variables. We are providing a template for this file in the file: setenv.sh.tmpl The settings are a little bit different for public cloud and private cloud. IBM Cloud For public cloud we need to get the credentials for Event Streams service (see the explanations and figures in this note ) and for the postgresql service in this note . For the Postgresql certificate the following command helps to get it. You may need to install the cloud database CLI with the command ibmcloud plugin install cloud-databases ibmcloud cdb deployment - cacert < name - of - postgres - service > > postgresql . crt This file will be used for adding certificate in the Java Truststore. Certificate - Java Truststore - Docker We have encoutered some challenges to inject SSL certificate in truststore when using Docker build stage. We have implemented scripts to do the following: add certificate to trustore using openssl and keytool. This script has to be run during the build stage, as unit tests are run inside the docker it needs to get certificate for maven, but it also need to execute when the container start in the deployed cluster. See the multi-stages dockerfile here . a startup script to start the java application, but calling the add certificate script and set the Java options. The Springboot implementation of the container management microservice uses this approach. Environment variables are used for passing values to maven, for the build and unit tests, and to Java for runtime.","title":"Security"},{"location":"security/#security","text":"This section addresses some challenges and best practices to support security for event-driven microservice integrating Kafka consumer or producer, or both, into the code, and CI/CD platform. Kafka consumers and producers need at least the broker URLs, the security API key in the case of Event Stream deployed on IBM Cloud public or private, and the SSL certificate as the communication is encrypted. Applying the \"store config in the environment\" rule of the 12 factors manisfesto, we are using the following environment variables: export KAFKA_BROKERS = \"localhost:9092\" export KAFKA_APIKEY = \"nokey\" export KAFKA_ENV = \"LOCAL\" export POSTGRESQL_URL = \"jdbc:postgresql://localhost:5432/postgres\" export POSTGRESQL_USER = \"postgres\" export POSTGRESQL_PWD = \"supersecret\" export POSTGRESQL_CA_PEM = \"\" The settings above are for running locally on the developer's laptop. To support the different environment (LOCAL, ICP, IBMCLOUD), we have a scripts/setenv.sh bash script to help export those variables. We are providing a template for this file in the file: setenv.sh.tmpl The settings are a little bit different for public cloud and private cloud.","title":"Security"},{"location":"security/#ibm-cloud","text":"For public cloud we need to get the credentials for Event Streams service (see the explanations and figures in this note ) and for the postgresql service in this note . For the Postgresql certificate the following command helps to get it. You may need to install the cloud database CLI with the command ibmcloud plugin install cloud-databases ibmcloud cdb deployment - cacert < name - of - postgres - service > > postgresql . crt This file will be used for adding certificate in the Java Truststore.","title":"IBM Cloud"},{"location":"security/#certificate-java-truststore-docker","text":"We have encoutered some challenges to inject SSL certificate in truststore when using Docker build stage. We have implemented scripts to do the following: add certificate to trustore using openssl and keytool. This script has to be run during the build stage, as unit tests are run inside the docker it needs to get certificate for maven, but it also need to execute when the container start in the deployed cluster. See the multi-stages dockerfile here . a startup script to start the java application, but calling the add certificate script and set the Java options. The Springboot implementation of the container management microservice uses this approach. Environment variables are used for passing values to maven, for the build and unit tests, and to Java for runtime.","title":"Certificate - Java Truststore - Docker"},{"location":"analysis/readme/","text":"Container Shipment Analysis This section defines the overall steps in the methodology to analyse a specific global shipping example and derive the event driven solution. We combined some elements of the design thinking methodology with the event storming and domain driven design to extract the following analysis of the business domain. Output from Domain Driven Design workshop From the design thinking workshop we extracted the following artifacts: * a persona list * the MVP hills Personas for each stakeholder We develop personas for each of the business stakeholders to better understand their work environment, motivations and challenges. Personas helps to capture the decisions and questions that these stakeholders must address with respect to the targeted key business initiative. Persona name Objectives Challenges Retailer Receive shipped goods on time, on date contracted with manufacturer Receive assurance that temperature sensitive goods have remained with bounds Late delivery may miss market opportunity long delivery time makes market opportunitiy prediction more difficult Manufacturer Good enough estimates of shipment times from Shipment Company to close sale and delivery with Retailer Pickup of containers by land transporter Timely delivery of container to Retailer as contracted with Shipment company Able to get information on current location and state of container in transit Contract with Shipment company will include timing estimates and penalty clauses must update Retailer as sonn as schedule changes known Must receive and communicate to retailer assurance on history of temperature sensitive goods Shipping Company Provide good enough estimates of shipment time to close shipment contract with Manufacturer Execute shipment contracts on time profitably ( with minimal cost) Fixed ship and itinerary schedule variability in ship leg travel times and costs variability in port congestion and load / unload times at dock variability in Land transport timings Land Transporter Pick up and drop off containers at times and locations agreed with Shipment company May be short notice requests may actually use bids in a market to resolve at lowest cost best response etc. Port Dock Operator Load and unload containers from docked ship as specified by Shipping Company with minimal time and effort free up dock asset quickly to become available for next ship Highly complex sequence of operation in Dockyard to be coordinated to minimize effort and time Customs Officer Clear containers for export and assess duty on import containers Depends on quality of manifest and certification of origin documentation for each container from Manufacturer MVP hills The challenges listed in the persona table above identify a possible set of MVP hills for this end to end solution. The event storming methodology described below will lead to picking out specific subareas of the solution with most value as initial MVPs. High level view of the shipment process flow At the high level the shipment process flow is suggested and illustrated in the diagram below. For the purposes of showing how to design a reference EDA solution we select on a simple subcase of all actual and possible variations of the global container flow. Very high level steps in this flow are as follows: Retailer and manufacturer interact to create agreement to deliver specific goods in a container from manufacturer's location to retailer's location with an expected arrival date. Manufacturer places shipping order with 'Shipping Company' to pickup container and deliver under the condition expected above. Shipping Company arranges for land transport to pick up loaded container and required documentation from Manufacturer and deliver the container to dockside at source port (adjacent to Maufacturer) for loading onto container carrier. Shipping company works with Customs Officer at source port to clear outbound container for export. When Container Ship is in dock at source port Shipping company arranges with Port Dock Operator to load and unload containers at this port. Loaded container ship leaves dock in source port adjacent to Manufacturer and sails to destination port. Container ship arrives at destination port (adjacent to Retailer) and queues to enter Port Docking area. Shipment company arranges with Port Docking Operator to unload specific containers needed at this port and reload additional ones for next shipping leg. Shipment company works with Import Export office at destination port to clear and collect any import duties. Shipment company works with Land Transporter at destination port to pick up container and deliver to Retailer. Container is delivered by Land Transporter to Retailer's location - transaction is complete. Event storming analysis of the container shipping flow We use the event storming analysis to move from the high level description of a business process flow above to a specific event timeline with identified bounded contexts each of which could be a target MVP component linked through EDA architecture. Event storming is a rapid lightweight design process enabling the team of business owners and stake holders, architects and IT specialists to fomalize a complex solution in a clearly communicable event timeline. This step is effective in developing event-based microservices linked through an EDA architecture in one or more MVP contexts. Steps in an eight hours event storming analysis workshop of the container shipping example are illustrated and described below. Step 1: Capture the Domain Event Timeline, swim lanes and key phases (This section of the example description covers activities identified as event storming workshop steps 1,2,3 in the generic description of the event storming method . The initial step in event storming analysis is to capture all events, things which have happened at a point in time, and organize them into a timeline: Each event goes on an orange \"sticky note\" Parallel or independent processes may be separated with blue horizontal swim lanes Critical event indicate a new stage, or pivot, in the flow shown with vertical blue bars. For the global shipment example described at a very high level above we came up with an event timeline shown in the set of diagrams below. (The event storming process captures these event timeline sections in charts on walls around the meeting room). Container shipping event timeline section 1 This section of the event time line deals with initial contracts to ship container and startup actions - specifically: Retailer and Manufacturer setting on an initial order for delivery of goods in a container. Manufacturer placing order for shipment with Shipping Company. Land transport arranged to pick up container and deliver to source port. Container ship approach source port adjacent to Manufacturer's location. The events are organized into separate swim lanes for Manufacturer, Retailer and Ship perspectives operating in parallel. Swim lanes help to clearly separate ship events as it approaches the source port from container specific events with agreements to ship etc. There is no time coupling or precise causality between events in these two swim lanes. The red sticky note is a comment. In this case we make the particular simplification to limit the scenario to shipping complete containers only. This avoids having to deal with additional warehousing, container load aggregation and packing events - together with correspondng unpacking and disaggregation. Container shipping event timeline section 2 This section continues event time line development with a swim lane now focussing on loading and pickup of a specific container at the Manufacturer's location and its delivery to the source port dockside. There is a critical event (indicated by vertical blue bar) separating the \"source dockside\" phase of the solution. Before this critical event we are dealing with container specific activities in collecting and transporting the container from Manufacturer's location to dockside. In the following dockside phase there are interactions with Customs Officer to get the container cleared for export. The Manufacturer will need an empty container (refrigerated if necessary for the shipment of interest) to load the goods into. We show an event for empty container being delivered. The solution is simplified if we assume that the Manufacture has a pool of empty containers always available. Alternatively this can be analyzed fully in some more complete generalized version of the solution. When the container arrives at source port dockside it may or may not be intime for the cutoff time required by the Customs Officer to get containers cleared for export before the scheduled departure of a particular container ship. If the cutoff deadline is missed the shipment will need to be rebooked on a later container ship and the client Manufacturer notified of expected delay in delivery. Container shipping event timeline section 3 This section continues the event timelines with swim lanes relating to a specific container shipment and also to the movement of a ship potentially carrying hundreds of containers. It introduces two new critical events: The Customs decision phase of event ends with a specific decision to clear a container for export or not, or possibly a request for additional inspecions or documents requiring more decision time: If the container is approved for export it can proceed to loading. If additional time is required for the clearance process, the original booking and expected delivery date may need to be modified. If export clearance is denied, then shipmen is cancelled and requesting parties notified. Ship enters dock ready to start unloading and loading is a new critical event: Previous ship events in Event Timeline section 1 dealt with ship \"booking\" a load/unload timeslot at a dock in the source port. Also getting national authority or Customs clearance to enter that jurisdiction. Now on arrival at the source port anchorage area, the ship requests permission to moor at an available dock facility. The critical event when a ship is cleared and moored at a dock hence ready to start unloading and loading containers is the start of the next event phase - container loading (and unloading). Container shipping event timeline section 4 This segment of the event timeline deals with a single swim lane for the ship while it is moored in a dock facility at the source port, is having arriving containers destined for this port unloaded and new containers being loaded at his port. The port dock facility operator is coordinating many events in the yard to perform load unload operations. These steps - as noted in a red discussion \"sticky\" in the event storming timeline are repeated for many containers. The time line presented here captures representative high level events. It is straightforward to extend the analysis to open up additional layers of detail touching on operational optimizations and coordination at the cost of addiional complexity not essential to our reference example here. Some of the events in this phase are now specialized to address needs of particular type of container - refrigerated containers - able to maintain specific temperature bounds and to report on their global location and temperature status on a continuous basis. This is a natural outcome of the event storming analysis involving free parallel capture of event types by a team of individuals with different points of view and interests. Working forward towards one or more MVP implementations of key components of this solution linked through EDA architecture we will need to characterize event types more uniformly end to end but imposing that level of consistency checking on the initial event storming process will slow down progess without providing significant benefit. Container shipping event timeline section 5 This segment of the event timeline captures events which occure in the blue water phase of the shipping, after the container ship has left the source port and is travelling owards but has not yet reached the destination port. It is divided into two swim lanes the ship perspective and individual container perspectives. The ship perspective includes events relating to the entire ship: leaving port. reporting its current position. deciding to alter planned course to avoid a weather event. The upper swim lane capture events which are specific to a particular container. container sensors reporting on geolocation. refrigerated container sensors reporting on humidity, carbon dioxide, temperature in the container and power consumption of the refrigeration unit. Container shipping event timeline sections 6 and 7 The remining event time line segments 6 and 7 deal with arrival at the destination port unload of the container and delivery to the Retailer's location. At the level of simplification in the reference architecture example, the steps for unloading a container at the destination port, clearing Customs and delivering it to are Retailer location are the symmetric image of the steps to pick up the container from the Manufacture location, clear it through export permissions and load onto the ship. For these reason we just provide event timeline digrams for these steps withou going into further explanatory detail. Step 2: Identify commands and event linkages This section of the example description covers activities identified as event storming workshop steps 4,5) in the generic description of the event storming method . After capturing all events for the scenario and organizing them in a time line, the next step in event storming analysis is to identify the triggers for events and causal linkages between events. For each identified event in the timeline we ask \"What triggered this event to occur?\". Expected event trigger types are: A human operator makes a decision and issues a command. Some external system or sensor provides a stimulus. An event results from some policy - typically automated processing of a precursor event. Completion of some determined period of elapsed time. For each event trigerred by a command, the triggering command is identified in a blue (sticky) note. This may become a microservice api in a later implementation. The human persona issuing the command is identified and shown in a yellow note above this. For events trigerred by processing of some precursor events the trigerring policy explaining when and why this event occurs is summarized in a lilac colored note. Specific causal event linkages are added to the event storming diagram as blue directed (arrow) linkages. In the following subsections we show the results of command and event linkage analysis for some selected areas of the container shipping example. Container shipping Commands for order placement and land transport setup This diagram shows the command, agent issuing events and policies triggering events for the order placement and land transport set up (at manufacturer location) sections of the event timeline generated in step 1 Container shipping event linkages for order placement setup The above diagram adds event linkages showing the causality chaining of events and business rules. Container shipping commands for pickup at Manufacturer's location The above diagram is generated for the command and policies associated with pick up of a loaded container from the Manufacturer's location and delivery to the source port dockside. Container shipping commands in port to port (Blue water) section of the event time line The diagram is self explanatory. Step 3: Decision data, predictive insights and insight storming: This section of the example description covers activities identified as event storming workshop step 8 in the generic description of the event storming method . Insight storming is extending the event storming workshop to identify and capture insightful predictive analytics, and it is introduced and described in workshop execution Step 8 - Insight . Predictive analytic insights are effectively probabilistic statements about which future events are likely to occur and what are the like properties of those events. They are typicaly generated using models created by data scientists or using artificial intelligence (AI) or machine learning (ML). Business owners and stakeholders in the event driven solution have good intuitions on: * Which probabilistic insights are likely to lead to better decision making and action when a particular event occurs. * What sources of information are likely to help create a model to predict this insight. So in event storming for an EDA system, we recommend generalizing the step of identifying data (properties of past definite events) to help make good decision and replacing this with an insight storming step which will look for: * data which will help make good decisions about how to act when an event occurs * predictive insights which could help guide our actions in response to proactively before some future event. * sources of data which are likely to enable the building of reliable predictive insight models. This additional step of insight storming takes advantage of the fact that we already have a time line for the problem being analysed with all events designed, commands, policies and event linkages already identified, and the business owners and stakeholders in the room whose insights for the business problem enable them to identify potentially valuable predictive insights. Working through insight storming in this way leads to a business value driven specification of possible predictive analytics opportunities in the solution. Event driven architecture provides a mature pattern to models addressing the identified needs. Event Stream Processing analytics infrastructure is then availalable to support scoring of these models and uses of the resulting insights in decision making and action in real time. Container shipping event stream processing diagram - including event stream processing flows The shipping example includes the case where continuous sensor measurement data is available from each refrigerated container while it is stacked on board the container ship and is in between ports on a blue water phase of the scenario. We show how streaming analytics can process the arriving continuous sensor measures in real-time and to deliver additional capabilites in the EDA solution. A diagram for this flow generated from Insight storming is shown below. In this diagram it is made clear the delivery of measured temperature, probably GPS position, and power consumption of the refrigeration unit for that container is a recurring \"continuous\" event stream. Each container might report once a minute; this ensures that an auditable record of container temperature is available from the event backbone or event sourcing. We show a policy test to decide whether the temperature has gone outside the specified range committed to in that shipment contract for the goods in that container. If this violation has occured, this is an (unusual) alert event reporting that temperature has gone out of range. This information is available as data to subject matter expert's dashboard seen by the shipping company operator who must make the business decision whether the contents of the container are spoiled. It is likely that involvement of human operator is necessary since this is a business decision with possibly significant $ consequences. It is possible that a bad sensor reading could have been received or that in this contract violation of the temperature range for a very short interval of time is permissable. Some stateful analysis of the container temperature reports would make the readings more reliable; perhaps there need to be more than one out of range reading to issue the alert to avoid corrupted data false positives. If the business decision is made that the container's contents are spoiled: A command is invoked to act on this decision. The container refrigeration may be powered down (possible other sensing left active) A policy based on terms and class of service of this particular shipment will determine: Whether a replacement shipment will be initiated and booked Usually shipping and receiving parties need to be notified The shipping company will schedule some salvage or disposal action for the content of the container at next port Each of the actions above will be an event captured in the event backbone - trigerring further loosely coupled commands and policies to take defined actions. Container shipping event stream processing with predictive Insight flows included The previous section defines how event stream processing could detect when a shipment was spoiled and trigger recovery actions. But shipping experts in an insight storming session will note that it is much better to predict when a spoilage temperature event is likely to occur and to take automated immediate (real-time) action to avert the spoilage. The simplest form of prediction of a temperature likely to go outside of its permissible range is to have checks on whether the temperature is approaching these bounds. If the temperature must stay below T degrees, take corrective action if it reaches T - delta degrees. More complex models for predicting temperature, could take into account diurnal variation due to external temperatures, possible predicted external temperatures forecast for the current course of the ship, and whether ther container is stacked above deck and hence particularly exposed to external temperatures. We assume that possible corrective action includes resetting the thermostatic controls on the refrigeration unit for the cotainer, possibly resetting the controls which may have drifted from their calibrated settings... An insight storming diagram which could be generated from discussion of these potentially useful insights and predictions is shown in the diagram below. We have added an additional insight - namely that it may be possible to predict from the temperature observed in a container and the trend of power consumption of that refrigeration unit, that the unit is in danger of failing and should be inspected and possibly services as soon as possible. Insights about predicted risk of temperature based spoilage, and prediction of refrigeration unit probable need for maintenance are presented in light blue. These are probabilistic prediction for properties and likely occurence of future events. Loose coupling and reuse of these insights by allowing publish subscribe to insight topics is helpful. Insights are conceptually different from events since they are probabilistic predictions for the future rather than events which by definition have already happened at some specific point in time. Event stream processing for insights relating to the ship Step 4: Commands, linkages, data and context for order placement This section covers activities identified as EventStorming Workshop steps 6,7 in the generic description of the event storming method . In particular, we look at identifying bounded contexts and identifying aggregates which will lead to a loosely coupled collection of microservices providing an agile event-driven design. We drill down on understanding the order placement process when a container shipment is booked as the MVP context focus in which to explore our design at the next level of detail. We will focus the implementation on at least two aggregates: the shipment order, and container transportation. An actor will create a shipment order from a user interface (a manufacturer staff) that will call a command for creating the order. The shipment order context will be mapped to a microservice. The implementation will generate a set of events. Same for container transportation. For more information on the design considerations for this solution see this related note.","title":"Event storming analysis"},{"location":"analysis/readme/#container-shipment-analysis","text":"This section defines the overall steps in the methodology to analyse a specific global shipping example and derive the event driven solution. We combined some elements of the design thinking methodology with the event storming and domain driven design to extract the following analysis of the business domain.","title":"Container Shipment Analysis"},{"location":"analysis/readme/#output-from-domain-driven-design-workshop","text":"From the design thinking workshop we extracted the following artifacts: * a persona list * the MVP hills","title":"Output from Domain Driven Design workshop"},{"location":"analysis/readme/#personas-for-each-stakeholder","text":"We develop personas for each of the business stakeholders to better understand their work environment, motivations and challenges. Personas helps to capture the decisions and questions that these stakeholders must address with respect to the targeted key business initiative. Persona name Objectives Challenges Retailer Receive shipped goods on time, on date contracted with manufacturer Receive assurance that temperature sensitive goods have remained with bounds Late delivery may miss market opportunity long delivery time makes market opportunitiy prediction more difficult Manufacturer Good enough estimates of shipment times from Shipment Company to close sale and delivery with Retailer Pickup of containers by land transporter Timely delivery of container to Retailer as contracted with Shipment company Able to get information on current location and state of container in transit Contract with Shipment company will include timing estimates and penalty clauses must update Retailer as sonn as schedule changes known Must receive and communicate to retailer assurance on history of temperature sensitive goods Shipping Company Provide good enough estimates of shipment time to close shipment contract with Manufacturer Execute shipment contracts on time profitably ( with minimal cost) Fixed ship and itinerary schedule variability in ship leg travel times and costs variability in port congestion and load / unload times at dock variability in Land transport timings Land Transporter Pick up and drop off containers at times and locations agreed with Shipment company May be short notice requests may actually use bids in a market to resolve at lowest cost best response etc. Port Dock Operator Load and unload containers from docked ship as specified by Shipping Company with minimal time and effort free up dock asset quickly to become available for next ship Highly complex sequence of operation in Dockyard to be coordinated to minimize effort and time Customs Officer Clear containers for export and assess duty on import containers Depends on quality of manifest and certification of origin documentation for each container from Manufacturer","title":"Personas for each stakeholder"},{"location":"analysis/readme/#mvp-hills","text":"The challenges listed in the persona table above identify a possible set of MVP hills for this end to end solution. The event storming methodology described below will lead to picking out specific subareas of the solution with most value as initial MVPs.","title":"MVP hills"},{"location":"analysis/readme/#high-level-view-of-the-shipment-process-flow","text":"At the high level the shipment process flow is suggested and illustrated in the diagram below. For the purposes of showing how to design a reference EDA solution we select on a simple subcase of all actual and possible variations of the global container flow. Very high level steps in this flow are as follows: Retailer and manufacturer interact to create agreement to deliver specific goods in a container from manufacturer's location to retailer's location with an expected arrival date. Manufacturer places shipping order with 'Shipping Company' to pickup container and deliver under the condition expected above. Shipping Company arranges for land transport to pick up loaded container and required documentation from Manufacturer and deliver the container to dockside at source port (adjacent to Maufacturer) for loading onto container carrier. Shipping company works with Customs Officer at source port to clear outbound container for export. When Container Ship is in dock at source port Shipping company arranges with Port Dock Operator to load and unload containers at this port. Loaded container ship leaves dock in source port adjacent to Manufacturer and sails to destination port. Container ship arrives at destination port (adjacent to Retailer) and queues to enter Port Docking area. Shipment company arranges with Port Docking Operator to unload specific containers needed at this port and reload additional ones for next shipping leg. Shipment company works with Import Export office at destination port to clear and collect any import duties. Shipment company works with Land Transporter at destination port to pick up container and deliver to Retailer. Container is delivered by Land Transporter to Retailer's location - transaction is complete.","title":"High level view of the shipment process flow"},{"location":"analysis/readme/#event-storming-analysis-of-the-container-shipping-flow","text":"We use the event storming analysis to move from the high level description of a business process flow above to a specific event timeline with identified bounded contexts each of which could be a target MVP component linked through EDA architecture. Event storming is a rapid lightweight design process enabling the team of business owners and stake holders, architects and IT specialists to fomalize a complex solution in a clearly communicable event timeline. This step is effective in developing event-based microservices linked through an EDA architecture in one or more MVP contexts. Steps in an eight hours event storming analysis workshop of the container shipping example are illustrated and described below.","title":"Event storming analysis of the container shipping flow"},{"location":"analysis/readme/#step-1-capture-the-domain-event-timeline-swim-lanes-and-key-phases","text":"(This section of the example description covers activities identified as event storming workshop steps 1,2,3 in the generic description of the event storming method . The initial step in event storming analysis is to capture all events, things which have happened at a point in time, and organize them into a timeline: Each event goes on an orange \"sticky note\" Parallel or independent processes may be separated with blue horizontal swim lanes Critical event indicate a new stage, or pivot, in the flow shown with vertical blue bars. For the global shipment example described at a very high level above we came up with an event timeline shown in the set of diagrams below. (The event storming process captures these event timeline sections in charts on walls around the meeting room).","title":"Step 1: Capture the Domain Event Timeline, swim lanes and key phases"},{"location":"analysis/readme/#container-shipping-event-timeline-section-1","text":"This section of the event time line deals with initial contracts to ship container and startup actions - specifically: Retailer and Manufacturer setting on an initial order for delivery of goods in a container. Manufacturer placing order for shipment with Shipping Company. Land transport arranged to pick up container and deliver to source port. Container ship approach source port adjacent to Manufacturer's location. The events are organized into separate swim lanes for Manufacturer, Retailer and Ship perspectives operating in parallel. Swim lanes help to clearly separate ship events as it approaches the source port from container specific events with agreements to ship etc. There is no time coupling or precise causality between events in these two swim lanes. The red sticky note is a comment. In this case we make the particular simplification to limit the scenario to shipping complete containers only. This avoids having to deal with additional warehousing, container load aggregation and packing events - together with correspondng unpacking and disaggregation.","title":"Container shipping event timeline section 1"},{"location":"analysis/readme/#container-shipping-event-timeline-section-2","text":"This section continues event time line development with a swim lane now focussing on loading and pickup of a specific container at the Manufacturer's location and its delivery to the source port dockside. There is a critical event (indicated by vertical blue bar) separating the \"source dockside\" phase of the solution. Before this critical event we are dealing with container specific activities in collecting and transporting the container from Manufacturer's location to dockside. In the following dockside phase there are interactions with Customs Officer to get the container cleared for export. The Manufacturer will need an empty container (refrigerated if necessary for the shipment of interest) to load the goods into. We show an event for empty container being delivered. The solution is simplified if we assume that the Manufacture has a pool of empty containers always available. Alternatively this can be analyzed fully in some more complete generalized version of the solution. When the container arrives at source port dockside it may or may not be intime for the cutoff time required by the Customs Officer to get containers cleared for export before the scheduled departure of a particular container ship. If the cutoff deadline is missed the shipment will need to be rebooked on a later container ship and the client Manufacturer notified of expected delay in delivery.","title":"Container shipping event timeline section 2"},{"location":"analysis/readme/#container-shipping-event-timeline-section-3","text":"This section continues the event timelines with swim lanes relating to a specific container shipment and also to the movement of a ship potentially carrying hundreds of containers. It introduces two new critical events: The Customs decision phase of event ends with a specific decision to clear a container for export or not, or possibly a request for additional inspecions or documents requiring more decision time: If the container is approved for export it can proceed to loading. If additional time is required for the clearance process, the original booking and expected delivery date may need to be modified. If export clearance is denied, then shipmen is cancelled and requesting parties notified. Ship enters dock ready to start unloading and loading is a new critical event: Previous ship events in Event Timeline section 1 dealt with ship \"booking\" a load/unload timeslot at a dock in the source port. Also getting national authority or Customs clearance to enter that jurisdiction. Now on arrival at the source port anchorage area, the ship requests permission to moor at an available dock facility. The critical event when a ship is cleared and moored at a dock hence ready to start unloading and loading containers is the start of the next event phase - container loading (and unloading).","title":"Container shipping event timeline section 3"},{"location":"analysis/readme/#container-shipping-event-timeline-section-4","text":"This segment of the event timeline deals with a single swim lane for the ship while it is moored in a dock facility at the source port, is having arriving containers destined for this port unloaded and new containers being loaded at his port. The port dock facility operator is coordinating many events in the yard to perform load unload operations. These steps - as noted in a red discussion \"sticky\" in the event storming timeline are repeated for many containers. The time line presented here captures representative high level events. It is straightforward to extend the analysis to open up additional layers of detail touching on operational optimizations and coordination at the cost of addiional complexity not essential to our reference example here. Some of the events in this phase are now specialized to address needs of particular type of container - refrigerated containers - able to maintain specific temperature bounds and to report on their global location and temperature status on a continuous basis. This is a natural outcome of the event storming analysis involving free parallel capture of event types by a team of individuals with different points of view and interests. Working forward towards one or more MVP implementations of key components of this solution linked through EDA architecture we will need to characterize event types more uniformly end to end but imposing that level of consistency checking on the initial event storming process will slow down progess without providing significant benefit.","title":"Container shipping event timeline section 4"},{"location":"analysis/readme/#container-shipping-event-timeline-section-5","text":"This segment of the event timeline captures events which occure in the blue water phase of the shipping, after the container ship has left the source port and is travelling owards but has not yet reached the destination port. It is divided into two swim lanes the ship perspective and individual container perspectives. The ship perspective includes events relating to the entire ship: leaving port. reporting its current position. deciding to alter planned course to avoid a weather event. The upper swim lane capture events which are specific to a particular container. container sensors reporting on geolocation. refrigerated container sensors reporting on humidity, carbon dioxide, temperature in the container and power consumption of the refrigeration unit.","title":"Container shipping event timeline section 5"},{"location":"analysis/readme/#container-shipping-event-timeline-sections-6-and-7","text":"The remining event time line segments 6 and 7 deal with arrival at the destination port unload of the container and delivery to the Retailer's location. At the level of simplification in the reference architecture example, the steps for unloading a container at the destination port, clearing Customs and delivering it to are Retailer location are the symmetric image of the steps to pick up the container from the Manufacture location, clear it through export permissions and load onto the ship. For these reason we just provide event timeline digrams for these steps withou going into further explanatory detail.","title":"Container shipping event timeline sections 6 and 7"},{"location":"analysis/readme/#step-2-identify-commands-and-event-linkages","text":"This section of the example description covers activities identified as event storming workshop steps 4,5) in the generic description of the event storming method . After capturing all events for the scenario and organizing them in a time line, the next step in event storming analysis is to identify the triggers for events and causal linkages between events. For each identified event in the timeline we ask \"What triggered this event to occur?\". Expected event trigger types are: A human operator makes a decision and issues a command. Some external system or sensor provides a stimulus. An event results from some policy - typically automated processing of a precursor event. Completion of some determined period of elapsed time. For each event trigerred by a command, the triggering command is identified in a blue (sticky) note. This may become a microservice api in a later implementation. The human persona issuing the command is identified and shown in a yellow note above this. For events trigerred by processing of some precursor events the trigerring policy explaining when and why this event occurs is summarized in a lilac colored note. Specific causal event linkages are added to the event storming diagram as blue directed (arrow) linkages. In the following subsections we show the results of command and event linkage analysis for some selected areas of the container shipping example.","title":"Step 2: Identify commands and event linkages"},{"location":"analysis/readme/#container-shipping-commands-for-order-placement-and-land-transport-setup","text":"This diagram shows the command, agent issuing events and policies triggering events for the order placement and land transport set up (at manufacturer location) sections of the event timeline generated in step 1","title":"Container shipping Commands for order placement and land transport setup"},{"location":"analysis/readme/#container-shipping-event-linkages-for-order-placement-setup","text":"The above diagram adds event linkages showing the causality chaining of events and business rules.","title":"Container shipping  event linkages for order placement setup"},{"location":"analysis/readme/#container-shipping-commands-for-pickup-at-manufacturers-location","text":"The above diagram is generated for the command and policies associated with pick up of a loaded container from the Manufacturer's location and delivery to the source port dockside.","title":"Container shipping commands for pickup at Manufacturer's location"},{"location":"analysis/readme/#container-shipping-commands-in-port-to-port-blue-water-section-of-the-event-time-line","text":"The diagram is self explanatory.","title":"Container shipping commands in port to port (Blue water) section of the event time line"},{"location":"analysis/readme/#step-3-decision-data-predictive-insights-and-insight-storming","text":"This section of the example description covers activities identified as event storming workshop step 8 in the generic description of the event storming method . Insight storming is extending the event storming workshop to identify and capture insightful predictive analytics, and it is introduced and described in workshop execution Step 8 - Insight . Predictive analytic insights are effectively probabilistic statements about which future events are likely to occur and what are the like properties of those events. They are typicaly generated using models created by data scientists or using artificial intelligence (AI) or machine learning (ML). Business owners and stakeholders in the event driven solution have good intuitions on: * Which probabilistic insights are likely to lead to better decision making and action when a particular event occurs. * What sources of information are likely to help create a model to predict this insight. So in event storming for an EDA system, we recommend generalizing the step of identifying data (properties of past definite events) to help make good decision and replacing this with an insight storming step which will look for: * data which will help make good decisions about how to act when an event occurs * predictive insights which could help guide our actions in response to proactively before some future event. * sources of data which are likely to enable the building of reliable predictive insight models. This additional step of insight storming takes advantage of the fact that we already have a time line for the problem being analysed with all events designed, commands, policies and event linkages already identified, and the business owners and stakeholders in the room whose insights for the business problem enable them to identify potentially valuable predictive insights. Working through insight storming in this way leads to a business value driven specification of possible predictive analytics opportunities in the solution. Event driven architecture provides a mature pattern to models addressing the identified needs. Event Stream Processing analytics infrastructure is then availalable to support scoring of these models and uses of the resulting insights in decision making and action in real time.","title":"Step 3: Decision data, predictive insights and insight storming:"},{"location":"analysis/readme/#container-shipping-event-stream-processing-diagram-including-event-stream-processing-flows","text":"The shipping example includes the case where continuous sensor measurement data is available from each refrigerated container while it is stacked on board the container ship and is in between ports on a blue water phase of the scenario. We show how streaming analytics can process the arriving continuous sensor measures in real-time and to deliver additional capabilites in the EDA solution. A diagram for this flow generated from Insight storming is shown below. In this diagram it is made clear the delivery of measured temperature, probably GPS position, and power consumption of the refrigeration unit for that container is a recurring \"continuous\" event stream. Each container might report once a minute; this ensures that an auditable record of container temperature is available from the event backbone or event sourcing. We show a policy test to decide whether the temperature has gone outside the specified range committed to in that shipment contract for the goods in that container. If this violation has occured, this is an (unusual) alert event reporting that temperature has gone out of range. This information is available as data to subject matter expert's dashboard seen by the shipping company operator who must make the business decision whether the contents of the container are spoiled. It is likely that involvement of human operator is necessary since this is a business decision with possibly significant $ consequences. It is possible that a bad sensor reading could have been received or that in this contract violation of the temperature range for a very short interval of time is permissable. Some stateful analysis of the container temperature reports would make the readings more reliable; perhaps there need to be more than one out of range reading to issue the alert to avoid corrupted data false positives. If the business decision is made that the container's contents are spoiled: A command is invoked to act on this decision. The container refrigeration may be powered down (possible other sensing left active) A policy based on terms and class of service of this particular shipment will determine: Whether a replacement shipment will be initiated and booked Usually shipping and receiving parties need to be notified The shipping company will schedule some salvage or disposal action for the content of the container at next port Each of the actions above will be an event captured in the event backbone - trigerring further loosely coupled commands and policies to take defined actions.","title":"Container shipping event stream processing diagram - including event stream processing flows"},{"location":"analysis/readme/#container-shipping-event-stream-processing-with-predictive-insight-flows-included","text":"The previous section defines how event stream processing could detect when a shipment was spoiled and trigger recovery actions. But shipping experts in an insight storming session will note that it is much better to predict when a spoilage temperature event is likely to occur and to take automated immediate (real-time) action to avert the spoilage. The simplest form of prediction of a temperature likely to go outside of its permissible range is to have checks on whether the temperature is approaching these bounds. If the temperature must stay below T degrees, take corrective action if it reaches T - delta degrees. More complex models for predicting temperature, could take into account diurnal variation due to external temperatures, possible predicted external temperatures forecast for the current course of the ship, and whether ther container is stacked above deck and hence particularly exposed to external temperatures. We assume that possible corrective action includes resetting the thermostatic controls on the refrigeration unit for the cotainer, possibly resetting the controls which may have drifted from their calibrated settings... An insight storming diagram which could be generated from discussion of these potentially useful insights and predictions is shown in the diagram below. We have added an additional insight - namely that it may be possible to predict from the temperature observed in a container and the trend of power consumption of that refrigeration unit, that the unit is in danger of failing and should be inspected and possibly services as soon as possible. Insights about predicted risk of temperature based spoilage, and prediction of refrigeration unit probable need for maintenance are presented in light blue. These are probabilistic prediction for properties and likely occurence of future events. Loose coupling and reuse of these insights by allowing publish subscribe to insight topics is helpful. Insights are conceptually different from events since they are probabilistic predictions for the future rather than events which by definition have already happened at some specific point in time.","title":"Container shipping event stream processing with predictive Insight flows included"},{"location":"analysis/readme/#event-stream-processing-for-insights-relating-to-the-ship","text":"","title":"Event stream processing for insights relating to the ship"},{"location":"analysis/readme/#step-4-commands-linkages-data-and-context-for-order-placement","text":"This section covers activities identified as EventStorming Workshop steps 6,7 in the generic description of the event storming method . In particular, we look at identifying bounded contexts and identifying aggregates which will lead to a loosely coupled collection of microservices providing an agile event-driven design. We drill down on understanding the order placement process when a container shipment is booked as the MVP context focus in which to explore our design at the next level of detail. We will focus the implementation on at least two aggregates: the shipment order, and container transportation. An actor will create a shipment order from a user interface (a manufacturer staff) that will call a command for creating the order. The shipment order context will be mapped to a microservice. The implementation will generate a set of events. Same for container transportation. For more information on the design considerations for this solution see this related note.","title":"Step 4: Commands, linkages, data and context for order placement"},{"location":"avro/avro/","text":"Apache Avro Introduction Here we explain the Apache Avro messaging integration we have done in one of our integration tests for the refarch-kc-container-ms component, which is part of the Reefer Containers reference implementation of the IBM Event Driven Architectures reference architecture . The Reefer Containers reference implementation is a simulation of what a container shipment process could look like in reality. From a manufacturer creating some goods to the delivery of those to a retailer, going through requesting a container, loading the goods into the container, finding a voyage for that container on a ship, monitoring the container's temperature and GPS location, delivering the container, unloading the goods, etc. As you can imagine, this scenario is ideal for an Event Driven architecture where we not only have a microservices based application but also the integration of these using Event Driven Architecture components (such as Kafka) and patterns (such as Saga, CQRS, Event Sourcing, etc). What is Apache Avro Avro is an open source data serialization system that helps with data exchange between systems, programming languages, and processing frameworks. Avro helps define a binary format for your data, as well as map it to the programming language of your choice. Why Apache Avro There are several websites that discuss the Apache Avro data serialization system benefits over other messaging data protocols. A simple google search will list dozens of them. Here, we will highlight just a few of those benefits from a Confluent blog post : It has a direct mapping to and from JSON It has a very compact format. The bulk of JSON, repeating every field name with every single record, is what makes JSON inefficient for high-volume usage. It is very fast. It has great bindings for a wide variety of programming languages so you can generate Java objects that make working with event data easier, but it does not require code generation so tools can be written generically for any data stream. It has a rich, extensible schema language defined in pure JSON It has the best notion of compatibility for evolving your data over time. Avro, Kafka and Schema Registry Avro relies on schemas. When Avro data is produced or read, the Avro schema for such piece of data is always present. This permits each datum to be written with no per-value overheads, making serialization both fast and small. An Avro schema defines the structure of the Avro data format. Schema Registry defines a scope in which schemas can evolve, and that scope is the subject. The name of the subject depends on the configured subject name strategy, which by default is set to derive subject name from topic name. In our case, this Avro data are messages sent to a kafka topic. Each message is a key-value pair. Either the message key or the message value, or both, can be serialized as Avro. Integration with Schema Registry means that Kafka messages do not need to be written with the entire Avro schema. Instead, Kafka messages are written with the schema id . The producers writing the messages and the consumers reading the messages must be using the same Schema Registry to get the same mapping between a schema and schema id. Kafka is used as Schema Registry storage backend. The special Kafka topic <kafkastore.topic> (default _schemas ), with a single partition, is used as a highly available write ahead log. All schemas, subject/version and ID metadata, and compatibility settings are appended as messages to this log. A Schema Registry instance therefore both produces and consumes messages under the _schemas topic. It produces messages to the log when, for example, new schemas are registered under a subject, or when updates to compatibility settings are registered. Schema Registry consumes from the _schemas log in a background thread, and updates its local caches on consumption of each new _schemas message to reflect the newly added schema or compatibility setting. Updating local state from the Kafka log in this manner ensures durability, ordering, and easy recoverability. How does it all work When the producer sends a message/event to a Kafka topic for the first time, it sends the schema for that message/event to the Schema Registry. The Schema Registry registers this schema to the subject for the Kafka topic we want to send the message/event to, and returns the schema id to the producer. The producer caches this mapping between the schema and schema id for subsequent message writes, so it only contacts Schema Registry on the first message/event write (unless the schema has changed, that is evolved, when the schema registry will be contacted again for validation and storage of this new version of the schema). Kafka messages are written along with the schema id rather than with the entire data schema. When a consumer reads this data, it sees the Avro schema id and sends a schema request to Schema Registry . Schema Registry retrieves the schema associated to that schema id, and returns the schema to the consumer . The consumer caches this mapping between the schema and schema id for subsequent message reads, so it only contacts Schema Registry on the first schema id read. Our implementation As mentioned in the introduction, the integration of the Apache Avro data serialization system has been done in one of the integration test for the refarch-kc-container-ms component of the Reefer Containers reference implementation of the IBM Event Driven Architectures reference architecture . The refarch-kc-container-ms component will take care of the reefer containers status. From adding new reefers to the available containers list to assigning a container to a particular order and managing the status of that reefer throughout the shipment process aforementioned. The integration tests for our Reefer Containers reference implementation can be found here . The integration tests are being developed in python and their main goal is to validate the successful deployment of the Reefer Containers reference implementation end-to-end. The particular integration test (still under development) where we have integrated the Apache Avro serialization system can be found under the ContainersPython folder. More precisely, these are the files and folders involved in our implementation: \u251c\u2500\u2500 data_schemas \u2502 \u251c\u2500\u2500 container_event.avsc \u2502 \u251c\u2500\u2500 container_event_key.avsc \u2502 \u251c\u2500\u2500 container_event_payload.avsc \u2502 \u251c\u2500\u2500 container_event_type.avsc \u2502 \u251c\u2500\u2500 container_status.avsc \u2502 \u2514\u2500\u2500 utils \u2502 \u2514\u2500\u2500 avroEDAUtils.py \u2514\u2500\u2500 itg-tests \u251c\u2500\u2500 ContainersPython \u2502 \u251c\u2500\u2500 ConsumeAvroContainers.py \u2502 \u2514\u2500\u2500 ContainerAvroProducer.py \u2514\u2500\u2500 kafka \u251c\u2500\u2500 KcAvroConsumer.py \u2514\u2500\u2500 KcAvroProducer.py that will allow us to send container events into the containers Kafka topic and read from such topic. By using these python scripts, we will be able to validate: Sending/Receiving Apache Avro encoded messages . Apache Avro data schema definitions for data correctness. Schema Registry for Apache Avro data schema management. Data Schemas Avro schemas are defined with JSON. An example of a Container Event for creating a new reefer container to the available list of containers for our reference application looks like: { \"containerID\" : \"container01\" , \"timestamp\" : 1569410690 , \"type\" : \"ContainerAdded\" , \"payload\" : { \"containerID\" : \"container01\" , \"type\" : \"Reefer\" , \"status\" : \"Empty\" , \"latitude\" : 37.8 , \"longitude\" : -122.25 , \"capacity\" : 110 , \"brand\" : \"itg-brand\" } } An Avro schema could be a nested schema which allows us to have a smaller reusable data schemas to define bigger and more complex ones. This is the case for our Container Event data schema. For instance, the payload is defined on its own data schema ( container_event_payload.avsc ) which the Container Event data schema refers to: { \"namespace\" : \"ibm.eda.kc.container.event\" , \"name\" : \"payload\" , \"type\" : \"record\" , \"fields\" : [ { \"name\" : \"containerID\" , \"type\" : \"string\" }, { \"name\" : \"type\" , \"type\" : \"string\" }, { \"name\" : \"status\" , \"type\" : \"ibm.eda.kc.container.status\" }, { \"name\" : \"latitude\" , \"type\" : \"float\" }, { \"name\" : \"longitude\" , \"type\" : \"float\" }, { \"name\" : \"capacity\" , \"type\" : \"int\" }, { \"name\" : \"brand\" , \"type\" : \"string\" } ] } As you can see, the status attribute of the payload is yet another data schema itself which, in this case, is of type enum: { \"namespace\" : \"ibm.eda.kc.container\" , \"name\" : \"status\" , \"type\" : \"enum\" , \"symbols\" : [ \"Loaded\" , \"Empty\" , \"Unassignable\" , \"PartiallyLoaded\" ] } All the different data schemas for a Container Event can be found under the data_schemas folder. In that folder we have also developed a util script ( avroEDAUtils.py ) to be able to construct the final Container Event data schema that is needed by our producer: def getContainerEventSchema ( schema_files_location ): # Read all the schemas needed in order to produce the final Container Event Schema known_schemas = avro . schema . Names () container_status_schema = LoadAvsc ( schema_files_location + \"container_status.avsc\" , known_schemas ) container_event_payload_schema = LoadAvsc ( schema_files_location + \"container_event_payload.avsc\" , known_schemas ) container_event_type_schema = LoadAvsc ( schema_files_location + \"container_event_type.avsc\" , known_schemas ) container_event_schema = LoadAvsc ( schema_files_location + \"container_event.avsc\" , known_schemas ) return container_event_schema def LoadAvsc ( file_path , names = None ): # Load avsc file # file_path: path to schema file # names(optional): avro.schema.Names object file_text = open ( file_path ) . read () json_data = json . loads ( file_text ) schema = avro . schema . SchemaFromJSONData ( json_data , names ) return schema See it in action Here, we are going to see how data schemas help with data correctness. Using the payload for our container messages/events as the example, this is the output of a correct message being sent: --- Container event to be published: --- { \"containerID\": \"container01\", \"type\": \"Reefer\", \"status\": \"Empty\", \"latitude\": 37.8, \"longitude\": -122.25, \"capacity\": 110, \"brand\": \"itg-brand\" } Message delivered to containers [0] However, if we try to send a payload where, for instance the container ID is an integer rather than a string, we will get an avro.io.AvroTypeException : avro . io . AvroTypeException : The datum { 'containerID' : 12345 , 'type' : 'Reefer' , 'status' : 'Empty' , 'latitude' : 37 . 8 , 'longitude' : - 122 . 25 , 'capacity' : 110 , 'brand' : 'itg-brand' } is not an example of the schema { \"type\" : { \"type\" : \"record\" , \"name\" : \"payload\" , \"namespace\" : \"ibm.eda.kc.container.event\" , \"fields\" : [ { \"type\" : \"string\" , \"name\" : \"containerID\" } , { \"type\" : \"string\" , \"name\" : \"type\" } , { \"type\" : { \"type\" : \"enum\" , \"name\" : \"status\" , \"namespace\" : \"ibm.eda.kc.container\" , \"symbols\" : [ \"Loaded\" , \"Empty\" , \"Unassignable\" , \"PartiallyLoaded\" ] } , \"name\" : \"status\" } , { \"type\" : \"float\" , \"name\" : \"latitude\" } , { \"type\" : \"float\" , \"name\" : \"longitude\" } , { \"type\" : \"int\" , \"name\" : \"capacity\" } , { \"type\" : \"string\" , \"name\" : \"brand\" } ] } Producer and Consumer The python scripts developed to implement a producer and consumer to a kafka topic that sends Avro messages whose data schemas are managed by a schema registry are: \u2514\u2500\u2500 itg-tests \u251c\u2500\u2500 ContainersPython \u2502 \u251c\u2500\u2500 ConsumeAvroContainers.py \u2502 \u2514\u2500\u2500 ContainerAvroProducer.py \u2514\u2500\u2500 kafka \u251c\u2500\u2500 KcAvroConsumer.py \u2514\u2500\u2500 KcAvroProducer.py We have used the confluent_kafka avro libraries to implement our producer and consumer. from confluent_kafka.avro import AvroProducer , AvroConsumer Producer We create our KafkaProducer object where we define some of the AvroProducer options such as the schema registry url for data schema registration and management. But it is not until we call the prepareProducer method that we actually create the AvroProducer with that schema registry to be used as well as the data schemas for the key and value of our Container Event to be sent. Finally, in the publishEvent method we send a value plus a key to a kafka topic. producer when we call prepareProducer import json from confluent_kafka import KafkaError from confluent_kafka.avro import AvroProducer class KafkaProducer : def __init__ ( self , kafka_brokers = \"\" , kafka_apikey = \"\" , schema_registry_url = \"\" ): self . kafka_brokers = kafka_brokers self . kafka_apikey = kafka_apikey self . schema_registry_url = schema_registry_url def prepareProducer ( self , groupID = \"pythonproducers\" , key_schema = \"\" , value_schema = \"\" ): options = { 'bootstrap.servers' : self . kafka_brokers , 'schema.registry.url' : self . schema_registry_url , 'group.id' : groupID } self . producer = AvroProducer ( options , default_key_schema = key_schema , default_value_schema = value_schema ) def publishEvent ( self , topicName , value , key ): # Important: value DOES NOT come in JSON format from ContainerAvroProducer.py. Therefore, we must convert it to JSON format first self . producer . produce ( topic = topicName , value = json . loads ( value ), key = json . loads ( value )[ key ], callback = self . delivery_report ) self . producer . flush () To use this class you need to do the following steps: # load schema definitions for key and value from utils.avroEDAUtils import getContainerEventSchema , getContainerKeySchema container_event_value_schema = getContainerEventSchema ( \"/data_schemas/\" ) container_event_key_schema = getContainerKeySchema ( \"/data_schemas/\" ) # Create a producer with the schema registry URL end point kp = KafkaProducer ( KAFKA_ENV , KAFKA_BROKERS , KAFKA_APIKEY , SCHEMA_REGISTRY_URL ) kp . prepareProducer ( \"ContainerProducerPython\" , container_event_key_schema , container_event_value_schema ) # loop on publishing events kp . publishEvent ( TOPIC_NAME , container_event ) Consumer Similarly to the producer, when we create a KafkaConsumer object we are just setting some of its attributes such as the kafka topic we will listen to and the schema registry url the producer will retrieve the data schemas from based on the schema ids messages comes with. It is only when we call the prepareConsumer method that we actually create the AvroConsumer and subscribe it to the intended kafka topic. import json from confluent_kafka.avro import AvroConsumer class KafkaConsumer : def __init__ ( self , kafka_brokers = \"\" , kafka_apikey = \"\" , topic_name = \"\" , schema_registry_url = \"\" ): self . kafka_brokers = kafka_brokers self . kafka_apikey = kafka_apikey self . topic_name = topic_name self . schema_registry_url = schema_registry_url def prepareConsumer ( self , groupID = \"pythonconsumers\" ): options = { 'bootstrap.servers' : self . kafka_brokers , 'group.id' : groupID , 'auto.offset.reset' : 'earliest' , 'schema.registry.url' : self . schema_registry_url , } self . consumer = AvroConsumer ( options ) self . consumer . subscribe ([ self . topic_name ]) # ... Schema registry For now, we have used the Confluent schema registry for our work although our goal is to use IBM Event Streams . The integration of the schema registry with your kafka broker is quite easy. In fact, all you need is to provide the schema registry with your zookeeper cluster url and give your schema registry a hostname: https://docs.confluent.io/current/installation/docker/config-reference.html#schema-registry-configuration Once you have your schema registry up and running, this provides a rich API endpoint to operate with: https://docs.confluent.io/current/schema-registry/using.html#common-sr-usage-examples For example: Let's assume we have created a new kafka topic called avrotest for testing our work. And let's also assume we are sending persona messages/events whose data schema is the following: { \"namespace\" : \"avro.test\" , \"name\" : \"persona\" , \"type\" : \"record\" , \"fields\" : [ { \"name\" : \"name\" , \"type\" : \"string\" }, { \"name\" : \"age\" , \"type\" : \"int\" }, { \"name\" : \"gender\" , \"type\" : \"string\" } ] } Get the subjects (that is, the kafka topics to which we have a schema registered against. As explained before, we either have registered the schema manually ourselves or the Avro producer has registered it when we have sent the first message) curl -X GET http://localhost:8081/subjects [\"avrotest-value\",\"avrotest-key\"] Get versions for a subject: curl -X GET http://localhost:8081/subjects/avrotest-value/versions [1] Get a specific version: curl -X GET http://localhost:8081/subjects/avrotest-value/versions/1/ { \"subject\": \"avrotest-value\", \"version\": 1, \"id\": 1, \"schema\": \"{\\\"type\\\":\\\"record\\\",\\\"name\\\":\\\"persona\\\",\\\"namespace\\\":\\\"avro.test\\\",\\\"fields\\\":[{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"},{\\\"name\\\":\\\"age\\\",\\\"type\\\":\\\"int\\\"},{\\\"name\\\":\\\"gender\\\",\\\"type\\\":\\\"string\\\"}]}\" } Get the schema of a specific subject version: curl -X GET http://localhost:8081/subjects/avrotest-value/versions/1/schema { \"type\": \"record\", \"name\": \"persona\", \"namespace\": \"avro.test\", \"fields\": [ { \"name\": \"name\", \"type\": \"string\" }, { \"name\": \"age\", \"type\": \"int\" }, { \"name\": \"gender\", \"type\": \"string\" } ] } Get the schema of a specific subject latest version: curl -X GET http://localhost:8081/subjects/avrotest-value/versions/latest/schema { \"type\": \"record\", \"name\": \"persona\", \"namespace\": \"avro.test\", \"fields\": [ { \"name\": \"name\", \"type\": \"string\" }, { \"name\": \"age\", \"type\": \"int\" }, { \"name\": \"gender\", \"type\": \"string\" } ] } Data evolution So far we have seen what Avro is, what a data schema is, what a schema registry is and how this all works together. From creating a data schema for your messages/events to comply with to how the schema registry and data schemas work together. And we have also seen the code for doing all this, from the python code to send and receive Avro encoded messages based on their schemas to the rich API the Confluent schema registry provides to interact with. However, we have said little about the need for data to evolve. When you design an Event Driven architecture for your application (by applying Event Storming or Domain Driven Design for example), it is very hard to come up with data structures/schemas that will not need to evolve/change in time. That is, your data like your use or business cases may need to evolve. As a result, Avro data schemas must be somehow flexible to allow your data to evolve along with your application and use cases. But it is not as easy as adding or removing data that travels in your events/messages or modifying the type of such data. And one of the reasons for this is that Kafka (or any other type of event broker) is many times used as the source of truth. That is, a place that you can trust as to what has happened. Hence, Kafka will serve as the event source of truth where all the events (that is, data) that happened (which could be bank transactions, communications, etc) will get stored (sometimes up to hundreds of years ) and will be able to be replayed if needed. As a result, there must be a data schema management and data schema evolution put in place that allow the compatibility of old and new data schemas and, in fact, data at the end of the day. There are mainly three types of data compatibility: Backward Forward Full Backward compatibility Backward compatibility means that consumers using the new schema can read data produced with the last schema . Using the persona data schema already mentioned throughout this readme, what if we decide to change the data schema to add a new attribute such as place of birth? That is, the new schema would look like: { \"namespace\" : \"avro.test\" , \"name\" : \"persona\" , \"type\" : \"record\" , \"fields\" : [ { \"name\" : \"name\" , \"type\" : \"string\" }, { \"name\" : \"age\" , \"type\" : \"int\" }, { \"name\" : \"gender\" , \"type\" : \"string\" }, { \"name\" : \"place_of_birth\" , \"type\" : \"string\" } ] } here is the output when we try to produce an event/message with the above data schema: ### Persona event to be published: ### { 'name' : 'david' , 'age' : '25' , 'gender' : 'male' , 'place_of_birth' : 'USA' } ###################################### Traceback ( most recent call last ): File \"ContainerAvroProducer.py\" , line 73 , in < module > kp . publishEvent ( TOPIC_NAME , container_event , \"1\" ) File \"/home/kafka/KcAvroProducer.py\" , line 42 , in publishEvent self . producer . produce ( topic = topicName , value = json . loads ( value ), key = json . loads ( key ), callback = self . delivery_report ) File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/__init__.py\" , line 80 , in produce value = self . _serializer . encode_record_with_schema ( topic , value_schema , value ) File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/serializer/message_serializer.py\" , line 105 , in encode_record_with_schema schema_id = self . registry_client . register ( subject , schema ) File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/cached_schema_registry_client.py\" , line 223 , in register raise ClientError ( \"Invalid Avro schema:\" + str ( code )) confluent_kafka . avro . error . ClientError : Invalid Avro schema : 422 And the reason for such error is that, because new schemas must be backward compatible (default compatibility mode for Confluent kafka data schemas topics), we can't just simply add a new attribute. Consumers using the new schema must be able to read data produced with the last schema. That is, if a consumer was to read old messages with the schema above, it would expect the place_of_birth attribute and its value on these old messages. However, the messages were produced with the old schema that did not enforce such attribute. Hence, the problem. We can also check the compatibility of this new schema using the API: curl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\" --data '{\"schema\": \"{\\\"type\\\":\\\"record\\\", \\\"name\\\":\\\"persona\\\", \\\"namespace\\\":\\\"avro.test\\\", \\\"fields\\\":[{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"}, {\\\"name\\\":\\\"age\\\",\\\"type\\\":\\\"int\\\"}, {\\\"name\\\":\\\"gender\\\",\\\"type\\\":\\\"string\\\"}, {\\\"name\\\":\\\"place_of_birth\\\",\\\"type\\\":\\\"string\\\"}]}\"}' http://localhost:8081/compatibility/subjects/avrotest-value/versions/latest {\"is_compatible\":false} How do we evolve our schema to add new attributes in a way that the schema is BACKWARD compatible? Adding a default value for such attribute so the consumer can use it when reading old messages that were produced without that attribute: { \"namespace\" : \"avro.test\" , \"name\" : \"persona\" , \"type\" : \"record\" , \"fields\" : [ { \"name\" : \"name\" , \"type\" : \"string\" }, { \"name\" : \"age\" , \"type\" : \"int\" }, { \"name\" : \"gender\" , \"type\" : \"string\" }, { \"name\" : \"place_of_birth\" , \"type\" : \"string\" , \"default\" : \"nonDefined\" } ] } Rather than changing it straight in the code, we can do some sort of validation through the API: curl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\" --data '{\"schema\": \"{\\\"type\\\":\\\"record\\\", \\\"name\\\":\\\"persona\\\", \\\"namespace\\\":\\\"avro.test\\\", \\\"fields\\\":[{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"}, {\\\"name\\\":\\\"age\\\",\\\"type\\\":\\\"int\\\"}, {\\\"name\\\":\\\"gender\\\",\\\"type\\\":\\\"string\\\"}, {\\\"name\\\":\\\"place_of_birth\\\",\\\"type\\\":\\\"string\\\", \\\"default\\\":\\\"nonDefined\\\"}]}\"}' http://localhost:8081/compatibility/subjects/avrotest-value/versions/latest {\"is_compatible\":true} We now can evolve our data schema to enforce a new attribute with our new messages/events being produced but making sure the consumer is able to read old messages that do not contain such attribute. We do so by sending a new persona event/message along with this new data schema. This will make the schema registry to register the new data schema. We can validate the new data schema version has been registered by using the schema registry API: curl -X GET http://localhost:8081/subjects/avrotest-value/versions [1,2] curl -X GET http://localhost:8081/subjects/avrotest-value/versions/latest/schema {\"type\":\"record\", \"name\":\"persona\", \"namespace\":\"avro.test\", \"fields\":[{\"name\":\"name\",\"type\":\"string\"}, {\"name\":\"age\",\"type\":\"int\"}, {\"name\":\"gender\",\"type\":\"string\"}, {\"name\":\"place_of_birth\",\"type\":\"string\",\"default\":\"nonDefined\"}]} What if we want to remove an attribute from our persona events/messages now? Well, this one is easy since the Avro consumer will simply ignore/drop all those attributes in the old persona events/messages that are not defined in the new data schema and just take in those that are defined. Let's try to remove the gender attribute: curl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\" --data '{\"schema\": \"{\\\"type\\\":\\\"record\\\", \\\"name\\\":\\\"persona\\\", \\\"namespace\\\":\\\"avro.test\\\", \\\"fields\\\":[{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"}, {\\\"name\\\":\\\"age\\\",\\\"type\\\":\\\"int\\\"}, {\\\"name\\\":\\\"place_of_birth\\\",\\\"type\\\":\\\"string\\\", \\\"default\\\":\\\"nonDefined\\\"}]}\"}' http://localhost:8081/compatibility/subjects/avrotest-value/versions/latest {\"is_compatible\":true} Forward compatibility Forwards compatibility means that data produced with a new schema can be read by consumers using the last schema . First, let's set the compatibility type to FORWARD (default compatibility mode in Confluent kafka is backward): curl -X PUT -H \"Content-Type: application/vnd.schemaregistry.v1+json\" --data '{\"compatibility\": \"FORWARD\"}' http://localhost:8081/config {\"compatibility\":\"FORWARD\"} curl -X GET http://localhost:8081/config {\"compatibilityLevel\":\"FORWARD\"} Now, how about removing an attribute when the compatibility type configured is set to FORWARD ? In this case, it is not as simple as removing the attribute from the new schema as the consumer will expect such attribute that the producer will not add to the events/messages. Let's try to remove the gender attribute from the persona messages/events: curl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\" --data '{\"schema\": \"{\\\"type\\\":\\\"record\\\", \\\"name\\\":\\\"persona\\\", \\\"namespace\\\":\\\"avro.test\\\", \\\"fields\\\":[{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"}, {\\\"name\\\":\\\"age\\\",\\\"type\\\":\\\"int\\\"}, {\\\"name\\\":\\\"place_of_birth\\\",\\\"type\\\":\\\"string\\\", \\\"default\\\":\\\"nonDefined\\\"}]}\"}' http://localhost:8081/compatibility/subjects/avrotest-value/versions/latest {\"is_compatible\":false} So, how can we produce new persona events/messages (without the gender attribute) that are compatible with the last data schema used by consumers (that expects an attribute called gender)? The trick here is to first register an \"intermediate\" data schema that adds a default value to gender if it is not defined. This way, the \"intermediate\" data schema will become the last data schema for the consumers and when we producer sent messages that do not contain the gender attribute, the consumer will know what to do: Intermediate schema: curl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\" --data '{\"schema\": \"{\\\"type\\\":\\\"record\\\", \\\"name\\\":\\\"persona\\\", \\\"namespace\\\":\\\"avro.test\\\", \\\"fields\\\":[{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"}, {\\\"name\\\":\\\"age\\\",\\\"type\\\":\\\"int\\\"}, {\\\"name\\\":\\\"gender\\\",\\\"type\\\": \\\"string\\\",\\\"default\\\": \\\"nonProvided\\\"}, {\\\"name\\\":\\\"place_of_birth\\\",\\\"type\\\":\\\"string\\\", \\\"default\\\":\\\"nonDefined\\\"}]}\"}' http://localhost:8081/compatibility/subjects/avrotest-value/versions/latest {\"is_compatible\":true} We register this data schema either by sending it along with a message/event using our producer or we simply register it using the schema registry API. Once we have this \"intermediate\" schema registered that will actually become the last data schema for the consumer, we check if our end goal data schema without the gender attribute is forward compatible or not: curl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\" --data '{\"schema\": \"{\\\"type\\\":\\\"record\\\", \\\"name\\\":\\\"persona\\\", \\\"namespace\\\":\\\"avro.test\\\", \\\"fields\\\":[{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"}, {\\\"name\\\":\\\"age\\\",\\\"type\\\":\\\"int\\\"}, {\\\"name\\\":\\\"place_of_birth\\\",\\\"type\\\":\\\"string\\\", \\\"default\\\":\\\"nonDefined\\\"}]}\"}' http://localhost:8081/compatibility/subjects/avrotest-value/versions/latest {\"is_compatible\":true} If we send a persona message that does not contain the gender attribute now, we should succeed: ### Persona event to be published: ### { 'name' : 'david' , 'age' : 25 , 'place_of_birth' : 'USA' } ###################################### Message delivered to avrotest [ 0 ] Contrary to the backward compatibility, in forward compatibility, adding a new attribute to your events/messages is not a problem because the consumers will simply ignore/drop this new attribute since the schema they are still using (the last one) does not include it. So let's say we want to add a new attribute called hair to represent the color of a persona's hair: curl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\" --data '{\"schema\": \"{\\\"type\\\":\\\"record\\\", \\\"name\\\":\\\"persona\\\", \\\"namespace\\\":\\\"avro.test\\\", \\\"fields\\\":[{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"}, {\\\"name\\\":\\\"age\\\",\\\"type\\\":\\\"int\\\"}, {\\\"name\\\":\\\"place_of_birth\\\",\\\"type\\\":\\\"string\\\", \\\"default\\\":\\\"nonDefined\\\"}, {\\\"name\\\":\\\"hair\\\",\\\"type\\\":\\\"string\\\"}]}\"}' http://localhost:8081/compatibility/subjects/avrotest-value/versions/latest {\"is_compatible\":true} We see there is no problem at all and if we try to send a message/event containing this new attribute along with the new schema: ### Persona event to be published: ### { 'name' : 'John' , 'age' : 25 , 'place_of_birth' : 'London' , 'hair' : 'brown' } ###################################### Message delivered to avrotest [ 0 ] The new data schema is registered and new messages/events complying with that new data schema are sent with no problem at all. Full compatibility Full compatibility means data schemas are both backward and forward compatible . Data schemas evolve in a fully compatible way: old data can be read with the new data schema, and new data can also be read with the last data schema . In some data formats, such as JSON, there are no full-compatible changes. Every modification is either only forward or only backward compatible. But in other data formats, like Avro, you can define fields with default values. In that case adding or removing a field with a default value is a fully compatible change. So let's see if we can delete the place_of_birth attribute, the only attribute in our data schema that defines a default value: curl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\" --data '{\"schema\": \"{\\\"type\\\":\\\"record\\\", \\\"name\\\":\\\"persona\\\", \\\"namespace\\\":\\\"avro.test\\\", \\\"fields\\\":[{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"}, {\\\"name\\\":\\\"age\\\",\\\"type\\\":\\\"int\\\"}, {\\\"name\\\":\\\"hair\\\",\\\"type\\\":\\\"string\\\"}]}\"}' http://localhost:8081/compatibility/subjects/avrotest-value/versions/latest {\"is_compatible\":true} It looks like it may work. Let's send a message without that attribute along with the new data schema: ### Persona event to be published: ### { 'name' : 'John' , 'age' : 25 , 'hair' : 'brown' } ###################################### Message delivered to avrotest [ 0 ] Let's now try to add an attribute with a default value. Let's say we want to add an attribute for the hobbies of a persona whose default value will be none curl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\" --data '{\"schema\": \"{\\\"type\\\":\\\"record\\\", \\\"name\\\":\\\"persona\\\", \\\"namespace\\\":\\\"avro.test\\\", \\\"fields\\\":[{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"}, {\\\"name\\\":\\\"age\\\",\\\"type\\\":\\\"int\\\"}, {\\\"name\\\":\\\"hair\\\",\\\"type\\\":\\\"string\\\"}, {\\\"name\\\":\\\"hobbies\\\",\\\"type\\\":\\\"string\\\", \\\"default\\\":\\\"none\\\"}]}\"}' \u00dfhttp://localhost:8081/compatibility/subjects/avrotest-value/versions/latest {\"is_compatible\":true} Let's send a message along with the new data schema to be completely sure: ### Persona event to be published: ### { 'name' : 'John' , 'age' : 25 , 'hair' : 'brown' , 'hobbies' : 'dance,music,food' } ###################################### Message delivered to avrotest [ 0 ] As expected, it did work. We now know how a data schema can evolve when full compatibility is required. That is, we know what attributes can be removed and how to add new attributes. Compendium Here are some links we have visited to carry out our work and found interesting to read: https://www.confluent.io/blog/avro-kafka-data/ https://avro.apache.org/docs/current/ https://docs.confluent.io/current/schema-registry/index.html https://docs.confluent.io/current/schema-registry/schema_registry_tutorial.html#schema-registry-tutorial-definition https://docs.oracle.com/database/nosql-11.2.2.0/GettingStartedGuide/schemaevolution.html","title":"Adopting Schema Registry and Avro"},{"location":"avro/avro/#apache-avro","text":"","title":"Apache Avro"},{"location":"avro/avro/#introduction","text":"Here we explain the Apache Avro messaging integration we have done in one of our integration tests for the refarch-kc-container-ms component, which is part of the Reefer Containers reference implementation of the IBM Event Driven Architectures reference architecture . The Reefer Containers reference implementation is a simulation of what a container shipment process could look like in reality. From a manufacturer creating some goods to the delivery of those to a retailer, going through requesting a container, loading the goods into the container, finding a voyage for that container on a ship, monitoring the container's temperature and GPS location, delivering the container, unloading the goods, etc. As you can imagine, this scenario is ideal for an Event Driven architecture where we not only have a microservices based application but also the integration of these using Event Driven Architecture components (such as Kafka) and patterns (such as Saga, CQRS, Event Sourcing, etc).","title":"Introduction"},{"location":"avro/avro/#what-is-apache-avro","text":"Avro is an open source data serialization system that helps with data exchange between systems, programming languages, and processing frameworks. Avro helps define a binary format for your data, as well as map it to the programming language of your choice.","title":"What is Apache Avro"},{"location":"avro/avro/#why-apache-avro","text":"There are several websites that discuss the Apache Avro data serialization system benefits over other messaging data protocols. A simple google search will list dozens of them. Here, we will highlight just a few of those benefits from a Confluent blog post : It has a direct mapping to and from JSON It has a very compact format. The bulk of JSON, repeating every field name with every single record, is what makes JSON inefficient for high-volume usage. It is very fast. It has great bindings for a wide variety of programming languages so you can generate Java objects that make working with event data easier, but it does not require code generation so tools can be written generically for any data stream. It has a rich, extensible schema language defined in pure JSON It has the best notion of compatibility for evolving your data over time.","title":"Why Apache Avro"},{"location":"avro/avro/#avro-kafka-and-schema-registry","text":"Avro relies on schemas. When Avro data is produced or read, the Avro schema for such piece of data is always present. This permits each datum to be written with no per-value overheads, making serialization both fast and small. An Avro schema defines the structure of the Avro data format. Schema Registry defines a scope in which schemas can evolve, and that scope is the subject. The name of the subject depends on the configured subject name strategy, which by default is set to derive subject name from topic name. In our case, this Avro data are messages sent to a kafka topic. Each message is a key-value pair. Either the message key or the message value, or both, can be serialized as Avro. Integration with Schema Registry means that Kafka messages do not need to be written with the entire Avro schema. Instead, Kafka messages are written with the schema id . The producers writing the messages and the consumers reading the messages must be using the same Schema Registry to get the same mapping between a schema and schema id. Kafka is used as Schema Registry storage backend. The special Kafka topic <kafkastore.topic> (default _schemas ), with a single partition, is used as a highly available write ahead log. All schemas, subject/version and ID metadata, and compatibility settings are appended as messages to this log. A Schema Registry instance therefore both produces and consumes messages under the _schemas topic. It produces messages to the log when, for example, new schemas are registered under a subject, or when updates to compatibility settings are registered. Schema Registry consumes from the _schemas log in a background thread, and updates its local caches on consumption of each new _schemas message to reflect the newly added schema or compatibility setting. Updating local state from the Kafka log in this manner ensures durability, ordering, and easy recoverability.","title":"Avro, Kafka and Schema Registry"},{"location":"avro/avro/#how-does-it-all-work","text":"When the producer sends a message/event to a Kafka topic for the first time, it sends the schema for that message/event to the Schema Registry. The Schema Registry registers this schema to the subject for the Kafka topic we want to send the message/event to, and returns the schema id to the producer. The producer caches this mapping between the schema and schema id for subsequent message writes, so it only contacts Schema Registry on the first message/event write (unless the schema has changed, that is evolved, when the schema registry will be contacted again for validation and storage of this new version of the schema). Kafka messages are written along with the schema id rather than with the entire data schema. When a consumer reads this data, it sees the Avro schema id and sends a schema request to Schema Registry . Schema Registry retrieves the schema associated to that schema id, and returns the schema to the consumer . The consumer caches this mapping between the schema and schema id for subsequent message reads, so it only contacts Schema Registry on the first schema id read.","title":"How does it all work"},{"location":"avro/avro/#our-implementation","text":"As mentioned in the introduction, the integration of the Apache Avro data serialization system has been done in one of the integration test for the refarch-kc-container-ms component of the Reefer Containers reference implementation of the IBM Event Driven Architectures reference architecture . The refarch-kc-container-ms component will take care of the reefer containers status. From adding new reefers to the available containers list to assigning a container to a particular order and managing the status of that reefer throughout the shipment process aforementioned. The integration tests for our Reefer Containers reference implementation can be found here . The integration tests are being developed in python and their main goal is to validate the successful deployment of the Reefer Containers reference implementation end-to-end. The particular integration test (still under development) where we have integrated the Apache Avro serialization system can be found under the ContainersPython folder. More precisely, these are the files and folders involved in our implementation: \u251c\u2500\u2500 data_schemas \u2502 \u251c\u2500\u2500 container_event.avsc \u2502 \u251c\u2500\u2500 container_event_key.avsc \u2502 \u251c\u2500\u2500 container_event_payload.avsc \u2502 \u251c\u2500\u2500 container_event_type.avsc \u2502 \u251c\u2500\u2500 container_status.avsc \u2502 \u2514\u2500\u2500 utils \u2502 \u2514\u2500\u2500 avroEDAUtils.py \u2514\u2500\u2500 itg-tests \u251c\u2500\u2500 ContainersPython \u2502 \u251c\u2500\u2500 ConsumeAvroContainers.py \u2502 \u2514\u2500\u2500 ContainerAvroProducer.py \u2514\u2500\u2500 kafka \u251c\u2500\u2500 KcAvroConsumer.py \u2514\u2500\u2500 KcAvroProducer.py that will allow us to send container events into the containers Kafka topic and read from such topic. By using these python scripts, we will be able to validate: Sending/Receiving Apache Avro encoded messages . Apache Avro data schema definitions for data correctness. Schema Registry for Apache Avro data schema management.","title":"Our implementation"},{"location":"avro/avro/#data-schemas","text":"Avro schemas are defined with JSON. An example of a Container Event for creating a new reefer container to the available list of containers for our reference application looks like: { \"containerID\" : \"container01\" , \"timestamp\" : 1569410690 , \"type\" : \"ContainerAdded\" , \"payload\" : { \"containerID\" : \"container01\" , \"type\" : \"Reefer\" , \"status\" : \"Empty\" , \"latitude\" : 37.8 , \"longitude\" : -122.25 , \"capacity\" : 110 , \"brand\" : \"itg-brand\" } } An Avro schema could be a nested schema which allows us to have a smaller reusable data schemas to define bigger and more complex ones. This is the case for our Container Event data schema. For instance, the payload is defined on its own data schema ( container_event_payload.avsc ) which the Container Event data schema refers to: { \"namespace\" : \"ibm.eda.kc.container.event\" , \"name\" : \"payload\" , \"type\" : \"record\" , \"fields\" : [ { \"name\" : \"containerID\" , \"type\" : \"string\" }, { \"name\" : \"type\" , \"type\" : \"string\" }, { \"name\" : \"status\" , \"type\" : \"ibm.eda.kc.container.status\" }, { \"name\" : \"latitude\" , \"type\" : \"float\" }, { \"name\" : \"longitude\" , \"type\" : \"float\" }, { \"name\" : \"capacity\" , \"type\" : \"int\" }, { \"name\" : \"brand\" , \"type\" : \"string\" } ] } As you can see, the status attribute of the payload is yet another data schema itself which, in this case, is of type enum: { \"namespace\" : \"ibm.eda.kc.container\" , \"name\" : \"status\" , \"type\" : \"enum\" , \"symbols\" : [ \"Loaded\" , \"Empty\" , \"Unassignable\" , \"PartiallyLoaded\" ] } All the different data schemas for a Container Event can be found under the data_schemas folder. In that folder we have also developed a util script ( avroEDAUtils.py ) to be able to construct the final Container Event data schema that is needed by our producer: def getContainerEventSchema ( schema_files_location ): # Read all the schemas needed in order to produce the final Container Event Schema known_schemas = avro . schema . Names () container_status_schema = LoadAvsc ( schema_files_location + \"container_status.avsc\" , known_schemas ) container_event_payload_schema = LoadAvsc ( schema_files_location + \"container_event_payload.avsc\" , known_schemas ) container_event_type_schema = LoadAvsc ( schema_files_location + \"container_event_type.avsc\" , known_schemas ) container_event_schema = LoadAvsc ( schema_files_location + \"container_event.avsc\" , known_schemas ) return container_event_schema def LoadAvsc ( file_path , names = None ): # Load avsc file # file_path: path to schema file # names(optional): avro.schema.Names object file_text = open ( file_path ) . read () json_data = json . loads ( file_text ) schema = avro . schema . SchemaFromJSONData ( json_data , names ) return schema","title":"Data Schemas"},{"location":"avro/avro/#see-it-in-action","text":"Here, we are going to see how data schemas help with data correctness. Using the payload for our container messages/events as the example, this is the output of a correct message being sent: --- Container event to be published: --- { \"containerID\": \"container01\", \"type\": \"Reefer\", \"status\": \"Empty\", \"latitude\": 37.8, \"longitude\": -122.25, \"capacity\": 110, \"brand\": \"itg-brand\" } Message delivered to containers [0] However, if we try to send a payload where, for instance the container ID is an integer rather than a string, we will get an avro.io.AvroTypeException : avro . io . AvroTypeException : The datum { 'containerID' : 12345 , 'type' : 'Reefer' , 'status' : 'Empty' , 'latitude' : 37 . 8 , 'longitude' : - 122 . 25 , 'capacity' : 110 , 'brand' : 'itg-brand' } is not an example of the schema { \"type\" : { \"type\" : \"record\" , \"name\" : \"payload\" , \"namespace\" : \"ibm.eda.kc.container.event\" , \"fields\" : [ { \"type\" : \"string\" , \"name\" : \"containerID\" } , { \"type\" : \"string\" , \"name\" : \"type\" } , { \"type\" : { \"type\" : \"enum\" , \"name\" : \"status\" , \"namespace\" : \"ibm.eda.kc.container\" , \"symbols\" : [ \"Loaded\" , \"Empty\" , \"Unassignable\" , \"PartiallyLoaded\" ] } , \"name\" : \"status\" } , { \"type\" : \"float\" , \"name\" : \"latitude\" } , { \"type\" : \"float\" , \"name\" : \"longitude\" } , { \"type\" : \"int\" , \"name\" : \"capacity\" } , { \"type\" : \"string\" , \"name\" : \"brand\" } ] }","title":"See it in action"},{"location":"avro/avro/#producer-and-consumer","text":"The python scripts developed to implement a producer and consumer to a kafka topic that sends Avro messages whose data schemas are managed by a schema registry are: \u2514\u2500\u2500 itg-tests \u251c\u2500\u2500 ContainersPython \u2502 \u251c\u2500\u2500 ConsumeAvroContainers.py \u2502 \u2514\u2500\u2500 ContainerAvroProducer.py \u2514\u2500\u2500 kafka \u251c\u2500\u2500 KcAvroConsumer.py \u2514\u2500\u2500 KcAvroProducer.py We have used the confluent_kafka avro libraries to implement our producer and consumer. from confluent_kafka.avro import AvroProducer , AvroConsumer","title":"Producer and Consumer"},{"location":"avro/avro/#producer","text":"We create our KafkaProducer object where we define some of the AvroProducer options such as the schema registry url for data schema registration and management. But it is not until we call the prepareProducer method that we actually create the AvroProducer with that schema registry to be used as well as the data schemas for the key and value of our Container Event to be sent. Finally, in the publishEvent method we send a value plus a key to a kafka topic. producer when we call prepareProducer import json from confluent_kafka import KafkaError from confluent_kafka.avro import AvroProducer class KafkaProducer : def __init__ ( self , kafka_brokers = \"\" , kafka_apikey = \"\" , schema_registry_url = \"\" ): self . kafka_brokers = kafka_brokers self . kafka_apikey = kafka_apikey self . schema_registry_url = schema_registry_url def prepareProducer ( self , groupID = \"pythonproducers\" , key_schema = \"\" , value_schema = \"\" ): options = { 'bootstrap.servers' : self . kafka_brokers , 'schema.registry.url' : self . schema_registry_url , 'group.id' : groupID } self . producer = AvroProducer ( options , default_key_schema = key_schema , default_value_schema = value_schema ) def publishEvent ( self , topicName , value , key ): # Important: value DOES NOT come in JSON format from ContainerAvroProducer.py. Therefore, we must convert it to JSON format first self . producer . produce ( topic = topicName , value = json . loads ( value ), key = json . loads ( value )[ key ], callback = self . delivery_report ) self . producer . flush () To use this class you need to do the following steps: # load schema definitions for key and value from utils.avroEDAUtils import getContainerEventSchema , getContainerKeySchema container_event_value_schema = getContainerEventSchema ( \"/data_schemas/\" ) container_event_key_schema = getContainerKeySchema ( \"/data_schemas/\" ) # Create a producer with the schema registry URL end point kp = KafkaProducer ( KAFKA_ENV , KAFKA_BROKERS , KAFKA_APIKEY , SCHEMA_REGISTRY_URL ) kp . prepareProducer ( \"ContainerProducerPython\" , container_event_key_schema , container_event_value_schema ) # loop on publishing events kp . publishEvent ( TOPIC_NAME , container_event )","title":"Producer"},{"location":"avro/avro/#consumer","text":"Similarly to the producer, when we create a KafkaConsumer object we are just setting some of its attributes such as the kafka topic we will listen to and the schema registry url the producer will retrieve the data schemas from based on the schema ids messages comes with. It is only when we call the prepareConsumer method that we actually create the AvroConsumer and subscribe it to the intended kafka topic. import json from confluent_kafka.avro import AvroConsumer class KafkaConsumer : def __init__ ( self , kafka_brokers = \"\" , kafka_apikey = \"\" , topic_name = \"\" , schema_registry_url = \"\" ): self . kafka_brokers = kafka_brokers self . kafka_apikey = kafka_apikey self . topic_name = topic_name self . schema_registry_url = schema_registry_url def prepareConsumer ( self , groupID = \"pythonconsumers\" ): options = { 'bootstrap.servers' : self . kafka_brokers , 'group.id' : groupID , 'auto.offset.reset' : 'earliest' , 'schema.registry.url' : self . schema_registry_url , } self . consumer = AvroConsumer ( options ) self . consumer . subscribe ([ self . topic_name ]) # ...","title":"Consumer"},{"location":"avro/avro/#schema-registry","text":"For now, we have used the Confluent schema registry for our work although our goal is to use IBM Event Streams . The integration of the schema registry with your kafka broker is quite easy. In fact, all you need is to provide the schema registry with your zookeeper cluster url and give your schema registry a hostname: https://docs.confluent.io/current/installation/docker/config-reference.html#schema-registry-configuration Once you have your schema registry up and running, this provides a rich API endpoint to operate with: https://docs.confluent.io/current/schema-registry/using.html#common-sr-usage-examples For example: Let's assume we have created a new kafka topic called avrotest for testing our work. And let's also assume we are sending persona messages/events whose data schema is the following: { \"namespace\" : \"avro.test\" , \"name\" : \"persona\" , \"type\" : \"record\" , \"fields\" : [ { \"name\" : \"name\" , \"type\" : \"string\" }, { \"name\" : \"age\" , \"type\" : \"int\" }, { \"name\" : \"gender\" , \"type\" : \"string\" } ] } Get the subjects (that is, the kafka topics to which we have a schema registered against. As explained before, we either have registered the schema manually ourselves or the Avro producer has registered it when we have sent the first message) curl -X GET http://localhost:8081/subjects [\"avrotest-value\",\"avrotest-key\"] Get versions for a subject: curl -X GET http://localhost:8081/subjects/avrotest-value/versions [1] Get a specific version: curl -X GET http://localhost:8081/subjects/avrotest-value/versions/1/ { \"subject\": \"avrotest-value\", \"version\": 1, \"id\": 1, \"schema\": \"{\\\"type\\\":\\\"record\\\",\\\"name\\\":\\\"persona\\\",\\\"namespace\\\":\\\"avro.test\\\",\\\"fields\\\":[{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"},{\\\"name\\\":\\\"age\\\",\\\"type\\\":\\\"int\\\"},{\\\"name\\\":\\\"gender\\\",\\\"type\\\":\\\"string\\\"}]}\" } Get the schema of a specific subject version: curl -X GET http://localhost:8081/subjects/avrotest-value/versions/1/schema { \"type\": \"record\", \"name\": \"persona\", \"namespace\": \"avro.test\", \"fields\": [ { \"name\": \"name\", \"type\": \"string\" }, { \"name\": \"age\", \"type\": \"int\" }, { \"name\": \"gender\", \"type\": \"string\" } ] } Get the schema of a specific subject latest version: curl -X GET http://localhost:8081/subjects/avrotest-value/versions/latest/schema { \"type\": \"record\", \"name\": \"persona\", \"namespace\": \"avro.test\", \"fields\": [ { \"name\": \"name\", \"type\": \"string\" }, { \"name\": \"age\", \"type\": \"int\" }, { \"name\": \"gender\", \"type\": \"string\" } ] }","title":"Schema registry"},{"location":"avro/avro/#data-evolution","text":"So far we have seen what Avro is, what a data schema is, what a schema registry is and how this all works together. From creating a data schema for your messages/events to comply with to how the schema registry and data schemas work together. And we have also seen the code for doing all this, from the python code to send and receive Avro encoded messages based on their schemas to the rich API the Confluent schema registry provides to interact with. However, we have said little about the need for data to evolve. When you design an Event Driven architecture for your application (by applying Event Storming or Domain Driven Design for example), it is very hard to come up with data structures/schemas that will not need to evolve/change in time. That is, your data like your use or business cases may need to evolve. As a result, Avro data schemas must be somehow flexible to allow your data to evolve along with your application and use cases. But it is not as easy as adding or removing data that travels in your events/messages or modifying the type of such data. And one of the reasons for this is that Kafka (or any other type of event broker) is many times used as the source of truth. That is, a place that you can trust as to what has happened. Hence, Kafka will serve as the event source of truth where all the events (that is, data) that happened (which could be bank transactions, communications, etc) will get stored (sometimes up to hundreds of years ) and will be able to be replayed if needed. As a result, there must be a data schema management and data schema evolution put in place that allow the compatibility of old and new data schemas and, in fact, data at the end of the day. There are mainly three types of data compatibility: Backward Forward Full","title":"Data evolution"},{"location":"avro/avro/#backward-compatibility","text":"Backward compatibility means that consumers using the new schema can read data produced with the last schema . Using the persona data schema already mentioned throughout this readme, what if we decide to change the data schema to add a new attribute such as place of birth? That is, the new schema would look like: { \"namespace\" : \"avro.test\" , \"name\" : \"persona\" , \"type\" : \"record\" , \"fields\" : [ { \"name\" : \"name\" , \"type\" : \"string\" }, { \"name\" : \"age\" , \"type\" : \"int\" }, { \"name\" : \"gender\" , \"type\" : \"string\" }, { \"name\" : \"place_of_birth\" , \"type\" : \"string\" } ] } here is the output when we try to produce an event/message with the above data schema: ### Persona event to be published: ### { 'name' : 'david' , 'age' : '25' , 'gender' : 'male' , 'place_of_birth' : 'USA' } ###################################### Traceback ( most recent call last ): File \"ContainerAvroProducer.py\" , line 73 , in < module > kp . publishEvent ( TOPIC_NAME , container_event , \"1\" ) File \"/home/kafka/KcAvroProducer.py\" , line 42 , in publishEvent self . producer . produce ( topic = topicName , value = json . loads ( value ), key = json . loads ( key ), callback = self . delivery_report ) File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/__init__.py\" , line 80 , in produce value = self . _serializer . encode_record_with_schema ( topic , value_schema , value ) File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/serializer/message_serializer.py\" , line 105 , in encode_record_with_schema schema_id = self . registry_client . register ( subject , schema ) File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/cached_schema_registry_client.py\" , line 223 , in register raise ClientError ( \"Invalid Avro schema:\" + str ( code )) confluent_kafka . avro . error . ClientError : Invalid Avro schema : 422 And the reason for such error is that, because new schemas must be backward compatible (default compatibility mode for Confluent kafka data schemas topics), we can't just simply add a new attribute. Consumers using the new schema must be able to read data produced with the last schema. That is, if a consumer was to read old messages with the schema above, it would expect the place_of_birth attribute and its value on these old messages. However, the messages were produced with the old schema that did not enforce such attribute. Hence, the problem. We can also check the compatibility of this new schema using the API: curl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\" --data '{\"schema\": \"{\\\"type\\\":\\\"record\\\", \\\"name\\\":\\\"persona\\\", \\\"namespace\\\":\\\"avro.test\\\", \\\"fields\\\":[{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"}, {\\\"name\\\":\\\"age\\\",\\\"type\\\":\\\"int\\\"}, {\\\"name\\\":\\\"gender\\\",\\\"type\\\":\\\"string\\\"}, {\\\"name\\\":\\\"place_of_birth\\\",\\\"type\\\":\\\"string\\\"}]}\"}' http://localhost:8081/compatibility/subjects/avrotest-value/versions/latest {\"is_compatible\":false} How do we evolve our schema to add new attributes in a way that the schema is BACKWARD compatible? Adding a default value for such attribute so the consumer can use it when reading old messages that were produced without that attribute: { \"namespace\" : \"avro.test\" , \"name\" : \"persona\" , \"type\" : \"record\" , \"fields\" : [ { \"name\" : \"name\" , \"type\" : \"string\" }, { \"name\" : \"age\" , \"type\" : \"int\" }, { \"name\" : \"gender\" , \"type\" : \"string\" }, { \"name\" : \"place_of_birth\" , \"type\" : \"string\" , \"default\" : \"nonDefined\" } ] } Rather than changing it straight in the code, we can do some sort of validation through the API: curl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\" --data '{\"schema\": \"{\\\"type\\\":\\\"record\\\", \\\"name\\\":\\\"persona\\\", \\\"namespace\\\":\\\"avro.test\\\", \\\"fields\\\":[{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"}, {\\\"name\\\":\\\"age\\\",\\\"type\\\":\\\"int\\\"}, {\\\"name\\\":\\\"gender\\\",\\\"type\\\":\\\"string\\\"}, {\\\"name\\\":\\\"place_of_birth\\\",\\\"type\\\":\\\"string\\\", \\\"default\\\":\\\"nonDefined\\\"}]}\"}' http://localhost:8081/compatibility/subjects/avrotest-value/versions/latest {\"is_compatible\":true} We now can evolve our data schema to enforce a new attribute with our new messages/events being produced but making sure the consumer is able to read old messages that do not contain such attribute. We do so by sending a new persona event/message along with this new data schema. This will make the schema registry to register the new data schema. We can validate the new data schema version has been registered by using the schema registry API: curl -X GET http://localhost:8081/subjects/avrotest-value/versions [1,2] curl -X GET http://localhost:8081/subjects/avrotest-value/versions/latest/schema {\"type\":\"record\", \"name\":\"persona\", \"namespace\":\"avro.test\", \"fields\":[{\"name\":\"name\",\"type\":\"string\"}, {\"name\":\"age\",\"type\":\"int\"}, {\"name\":\"gender\",\"type\":\"string\"}, {\"name\":\"place_of_birth\",\"type\":\"string\",\"default\":\"nonDefined\"}]} What if we want to remove an attribute from our persona events/messages now? Well, this one is easy since the Avro consumer will simply ignore/drop all those attributes in the old persona events/messages that are not defined in the new data schema and just take in those that are defined. Let's try to remove the gender attribute: curl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\" --data '{\"schema\": \"{\\\"type\\\":\\\"record\\\", \\\"name\\\":\\\"persona\\\", \\\"namespace\\\":\\\"avro.test\\\", \\\"fields\\\":[{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"}, {\\\"name\\\":\\\"age\\\",\\\"type\\\":\\\"int\\\"}, {\\\"name\\\":\\\"place_of_birth\\\",\\\"type\\\":\\\"string\\\", \\\"default\\\":\\\"nonDefined\\\"}]}\"}' http://localhost:8081/compatibility/subjects/avrotest-value/versions/latest {\"is_compatible\":true}","title":"Backward compatibility"},{"location":"avro/avro/#forward-compatibility","text":"Forwards compatibility means that data produced with a new schema can be read by consumers using the last schema . First, let's set the compatibility type to FORWARD (default compatibility mode in Confluent kafka is backward): curl -X PUT -H \"Content-Type: application/vnd.schemaregistry.v1+json\" --data '{\"compatibility\": \"FORWARD\"}' http://localhost:8081/config {\"compatibility\":\"FORWARD\"} curl -X GET http://localhost:8081/config {\"compatibilityLevel\":\"FORWARD\"} Now, how about removing an attribute when the compatibility type configured is set to FORWARD ? In this case, it is not as simple as removing the attribute from the new schema as the consumer will expect such attribute that the producer will not add to the events/messages. Let's try to remove the gender attribute from the persona messages/events: curl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\" --data '{\"schema\": \"{\\\"type\\\":\\\"record\\\", \\\"name\\\":\\\"persona\\\", \\\"namespace\\\":\\\"avro.test\\\", \\\"fields\\\":[{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"}, {\\\"name\\\":\\\"age\\\",\\\"type\\\":\\\"int\\\"}, {\\\"name\\\":\\\"place_of_birth\\\",\\\"type\\\":\\\"string\\\", \\\"default\\\":\\\"nonDefined\\\"}]}\"}' http://localhost:8081/compatibility/subjects/avrotest-value/versions/latest {\"is_compatible\":false} So, how can we produce new persona events/messages (without the gender attribute) that are compatible with the last data schema used by consumers (that expects an attribute called gender)? The trick here is to first register an \"intermediate\" data schema that adds a default value to gender if it is not defined. This way, the \"intermediate\" data schema will become the last data schema for the consumers and when we producer sent messages that do not contain the gender attribute, the consumer will know what to do: Intermediate schema: curl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\" --data '{\"schema\": \"{\\\"type\\\":\\\"record\\\", \\\"name\\\":\\\"persona\\\", \\\"namespace\\\":\\\"avro.test\\\", \\\"fields\\\":[{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"}, {\\\"name\\\":\\\"age\\\",\\\"type\\\":\\\"int\\\"}, {\\\"name\\\":\\\"gender\\\",\\\"type\\\": \\\"string\\\",\\\"default\\\": \\\"nonProvided\\\"}, {\\\"name\\\":\\\"place_of_birth\\\",\\\"type\\\":\\\"string\\\", \\\"default\\\":\\\"nonDefined\\\"}]}\"}' http://localhost:8081/compatibility/subjects/avrotest-value/versions/latest {\"is_compatible\":true} We register this data schema either by sending it along with a message/event using our producer or we simply register it using the schema registry API. Once we have this \"intermediate\" schema registered that will actually become the last data schema for the consumer, we check if our end goal data schema without the gender attribute is forward compatible or not: curl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\" --data '{\"schema\": \"{\\\"type\\\":\\\"record\\\", \\\"name\\\":\\\"persona\\\", \\\"namespace\\\":\\\"avro.test\\\", \\\"fields\\\":[{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"}, {\\\"name\\\":\\\"age\\\",\\\"type\\\":\\\"int\\\"}, {\\\"name\\\":\\\"place_of_birth\\\",\\\"type\\\":\\\"string\\\", \\\"default\\\":\\\"nonDefined\\\"}]}\"}' http://localhost:8081/compatibility/subjects/avrotest-value/versions/latest {\"is_compatible\":true} If we send a persona message that does not contain the gender attribute now, we should succeed: ### Persona event to be published: ### { 'name' : 'david' , 'age' : 25 , 'place_of_birth' : 'USA' } ###################################### Message delivered to avrotest [ 0 ] Contrary to the backward compatibility, in forward compatibility, adding a new attribute to your events/messages is not a problem because the consumers will simply ignore/drop this new attribute since the schema they are still using (the last one) does not include it. So let's say we want to add a new attribute called hair to represent the color of a persona's hair: curl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\" --data '{\"schema\": \"{\\\"type\\\":\\\"record\\\", \\\"name\\\":\\\"persona\\\", \\\"namespace\\\":\\\"avro.test\\\", \\\"fields\\\":[{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"}, {\\\"name\\\":\\\"age\\\",\\\"type\\\":\\\"int\\\"}, {\\\"name\\\":\\\"place_of_birth\\\",\\\"type\\\":\\\"string\\\", \\\"default\\\":\\\"nonDefined\\\"}, {\\\"name\\\":\\\"hair\\\",\\\"type\\\":\\\"string\\\"}]}\"}' http://localhost:8081/compatibility/subjects/avrotest-value/versions/latest {\"is_compatible\":true} We see there is no problem at all and if we try to send a message/event containing this new attribute along with the new schema: ### Persona event to be published: ### { 'name' : 'John' , 'age' : 25 , 'place_of_birth' : 'London' , 'hair' : 'brown' } ###################################### Message delivered to avrotest [ 0 ] The new data schema is registered and new messages/events complying with that new data schema are sent with no problem at all.","title":"Forward compatibility"},{"location":"avro/avro/#full-compatibility","text":"Full compatibility means data schemas are both backward and forward compatible . Data schemas evolve in a fully compatible way: old data can be read with the new data schema, and new data can also be read with the last data schema . In some data formats, such as JSON, there are no full-compatible changes. Every modification is either only forward or only backward compatible. But in other data formats, like Avro, you can define fields with default values. In that case adding or removing a field with a default value is a fully compatible change. So let's see if we can delete the place_of_birth attribute, the only attribute in our data schema that defines a default value: curl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\" --data '{\"schema\": \"{\\\"type\\\":\\\"record\\\", \\\"name\\\":\\\"persona\\\", \\\"namespace\\\":\\\"avro.test\\\", \\\"fields\\\":[{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"}, {\\\"name\\\":\\\"age\\\",\\\"type\\\":\\\"int\\\"}, {\\\"name\\\":\\\"hair\\\",\\\"type\\\":\\\"string\\\"}]}\"}' http://localhost:8081/compatibility/subjects/avrotest-value/versions/latest {\"is_compatible\":true} It looks like it may work. Let's send a message without that attribute along with the new data schema: ### Persona event to be published: ### { 'name' : 'John' , 'age' : 25 , 'hair' : 'brown' } ###################################### Message delivered to avrotest [ 0 ] Let's now try to add an attribute with a default value. Let's say we want to add an attribute for the hobbies of a persona whose default value will be none curl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\" --data '{\"schema\": \"{\\\"type\\\":\\\"record\\\", \\\"name\\\":\\\"persona\\\", \\\"namespace\\\":\\\"avro.test\\\", \\\"fields\\\":[{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"}, {\\\"name\\\":\\\"age\\\",\\\"type\\\":\\\"int\\\"}, {\\\"name\\\":\\\"hair\\\",\\\"type\\\":\\\"string\\\"}, {\\\"name\\\":\\\"hobbies\\\",\\\"type\\\":\\\"string\\\", \\\"default\\\":\\\"none\\\"}]}\"}' \u00dfhttp://localhost:8081/compatibility/subjects/avrotest-value/versions/latest {\"is_compatible\":true} Let's send a message along with the new data schema to be completely sure: ### Persona event to be published: ### { 'name' : 'John' , 'age' : 25 , 'hair' : 'brown' , 'hobbies' : 'dance,music,food' } ###################################### Message delivered to avrotest [ 0 ] As expected, it did work. We now know how a data schema can evolve when full compatibility is required. That is, we know what attributes can be removed and how to add new attributes.","title":"Full compatibility"},{"location":"avro/avro/#compendium","text":"Here are some links we have visited to carry out our work and found interesting to read: https://www.confluent.io/blog/avro-kafka-data/ https://avro.apache.org/docs/current/ https://docs.confluent.io/current/schema-registry/index.html https://docs.confluent.io/current/schema-registry/schema_registry_tutorial.html#schema-registry-tutorial-definition https://docs.oracle.com/database/nosql-11.2.2.0/GettingStartedGuide/schemaevolution.html","title":"Compendium"},{"location":"demo/readme/","text":"Demo Script This demo script is using the localhost deployment with a mapping of the host name kcsolution to localhost defined in the /etc/hosts file. For IBM Cloud change the hostname accordingly. Here is how to execute the business process step by step using the demonstration APIs and some scripts. Pre-requisites Set the following IP address - hostname mapping: Hostname kcsolution to map to localhost when running on local computer Step 1: Manufacturer create an order: Go to the http://kcsolution:3110 URL to access the demonstration home page: This page presents the simple version of the business process and the user interface tiles that can be used to simulate the progression within this business process. The grey shadowed tiles have not implemented logic. From the Initiate Orders - Manufacturer we can have the manufacturer creating a new fresh product order to ship over sea. To represent different manufacturers the first select box is used to support different scenarios in the future. 'GoodManuf' should be used. Once the manufacturer is selected a list of existing orders may be displayed. You can add order with the UI, but you can also use a script in the order command microservice project: https://github.com/ibm-cloud-architecture/refarch-kc-order-ms/blob/master/order-command-ms/scripts/createOrder.sh Below is an example of how to use the createOrder script to add a 'GoodManuf's' order to book a voyage from Oakland to Shanghai for a fresh product: . / createOrder . sh localhost : 10080 . / orderOacklandToChinaCreate . json There is a lot happening here. The Angular is getting orders using the orders.service.ts service within the BFF component at the address: http://localhost:3010/api/orders . The BFF is calling the Order Query Microservice via a javascript client code: getOrders(manuf) function. . The Order Query microservice URL is defined in environment variable or defaulted in the config file. It is mapped to the deployed Order service. (e.g. http://ordercmd:9080/orders) Selecting one order using the Arrow icon, allow the user to view the order details: As illustrated in the CQRS diagram: the creation of the order goes to the order command microservice which publishes a OrderCreated event to the orders topic and then consumes it to persist the data to its database. See source code here If you plug a 'orders topic' consumer you can see the following trace with the status of the order being pending and the type of event being OrderCreated . { \"payload\" :{ \"orderID\" : \"1fcccdf2-e29d-4b30-8e52-8116dc2a01ff\" , \"productID\" : \"Carrot\" , \"customerID\" : \"GoodManuf\" , \"quantity\" : 10000 , \"pickupAddress\" : \"...\" , \"expectedDeliveryDate\" : \"2019-03-31T13:30Z\" , \"status\" : \"pending\" }, \"type\" : \"OrderCreated\" , \"version\" : \"1\" } Step 2: K Container Shipment Manager looking at Orders From the home page goes to the Shipment Inc tile: Then the home page lists the current order the shipment company received The status of those events will be modified over time while the order is processed down stream by the voyage and container services. The following sequence diagram illustrates the flow: Looking at the traces in the voyage service voyages_1 | emitting { \"timestamp\" : 1548788544290 , \"type\" : \"OrderAssigned\" , \"version\" : \"1\" , \"payload\" : { \"voyageID\" : 100 , \"orderID\" : \"1fcccdf2-e29d-4b30-8e52-8116dc2a01ff\" }} or at the orders topic: { \"timestamp\" : 1548792921679 , \"type\" : \"OrderAssigned\" , \"version\" : \"1\" , \"payload\" :{ \"voyageID\" : 100 , \"orderID\" : \"1fcccdf2-e29d-4b30-8e52-8116dc2a01ff\" }} Step3: Simulate the ship in blue water From the home page goes to the Simulate the bluewater tile, then in the main page select one of the available fleet. Only the North Pacific has data as of now: The fleet panel lists the boats, their location and status and a map: Selecting one boat with the edit button, goes to the boat detail view: You can start the simulation on the ship movement by seleting one of the three pre-defined scenarios: Fire some containers One reefer down Or boat going thru a heat waves The command is sent to the Simulator and the boat will start to move and generate container metrics: The simulation implementation is yet not completed.","title":"Demonstration script"},{"location":"demo/readme/#demo-script","text":"This demo script is using the localhost deployment with a mapping of the host name kcsolution to localhost defined in the /etc/hosts file. For IBM Cloud change the hostname accordingly. Here is how to execute the business process step by step using the demonstration APIs and some scripts.","title":"Demo Script"},{"location":"demo/readme/#pre-requisites","text":"Set the following IP address - hostname mapping: Hostname kcsolution to map to localhost when running on local computer","title":"Pre-requisites"},{"location":"demo/readme/#step-1-manufacturer-create-an-order","text":"Go to the http://kcsolution:3110 URL to access the demonstration home page: This page presents the simple version of the business process and the user interface tiles that can be used to simulate the progression within this business process. The grey shadowed tiles have not implemented logic. From the Initiate Orders - Manufacturer we can have the manufacturer creating a new fresh product order to ship over sea. To represent different manufacturers the first select box is used to support different scenarios in the future. 'GoodManuf' should be used. Once the manufacturer is selected a list of existing orders may be displayed. You can add order with the UI, but you can also use a script in the order command microservice project: https://github.com/ibm-cloud-architecture/refarch-kc-order-ms/blob/master/order-command-ms/scripts/createOrder.sh Below is an example of how to use the createOrder script to add a 'GoodManuf's' order to book a voyage from Oakland to Shanghai for a fresh product: . / createOrder . sh localhost : 10080 . / orderOacklandToChinaCreate . json There is a lot happening here. The Angular is getting orders using the orders.service.ts service within the BFF component at the address: http://localhost:3010/api/orders . The BFF is calling the Order Query Microservice via a javascript client code: getOrders(manuf) function. . The Order Query microservice URL is defined in environment variable or defaulted in the config file. It is mapped to the deployed Order service. (e.g. http://ordercmd:9080/orders) Selecting one order using the Arrow icon, allow the user to view the order details: As illustrated in the CQRS diagram: the creation of the order goes to the order command microservice which publishes a OrderCreated event to the orders topic and then consumes it to persist the data to its database. See source code here If you plug a 'orders topic' consumer you can see the following trace with the status of the order being pending and the type of event being OrderCreated . { \"payload\" :{ \"orderID\" : \"1fcccdf2-e29d-4b30-8e52-8116dc2a01ff\" , \"productID\" : \"Carrot\" , \"customerID\" : \"GoodManuf\" , \"quantity\" : 10000 , \"pickupAddress\" : \"...\" , \"expectedDeliveryDate\" : \"2019-03-31T13:30Z\" , \"status\" : \"pending\" }, \"type\" : \"OrderCreated\" , \"version\" : \"1\" }","title":"Step 1: Manufacturer create an order:"},{"location":"demo/readme/#step-2-k-container-shipment-manager-looking-at-orders","text":"From the home page goes to the Shipment Inc tile: Then the home page lists the current order the shipment company received The status of those events will be modified over time while the order is processed down stream by the voyage and container services. The following sequence diagram illustrates the flow: Looking at the traces in the voyage service voyages_1 | emitting { \"timestamp\" : 1548788544290 , \"type\" : \"OrderAssigned\" , \"version\" : \"1\" , \"payload\" : { \"voyageID\" : 100 , \"orderID\" : \"1fcccdf2-e29d-4b30-8e52-8116dc2a01ff\" }} or at the orders topic: { \"timestamp\" : 1548792921679 , \"type\" : \"OrderAssigned\" , \"version\" : \"1\" , \"payload\" :{ \"voyageID\" : 100 , \"orderID\" : \"1fcccdf2-e29d-4b30-8e52-8116dc2a01ff\" }}","title":"Step 2: K Container Shipment Manager looking at Orders"},{"location":"demo/readme/#step3-simulate-the-ship-in-blue-water","text":"From the home page goes to the Simulate the bluewater tile, then in the main page select one of the available fleet. Only the North Pacific has data as of now: The fleet panel lists the boats, their location and status and a map: Selecting one boat with the edit button, goes to the boat detail view: You can start the simulation on the ship movement by seleting one of the three pre-defined scenarios: Fire some containers One reefer down Or boat going thru a heat waves The command is sent to the Simulator and the boat will start to move and generate container metrics: The simulation implementation is yet not completed.","title":"Step3: Simulate the ship in blue water"},{"location":"deployments/application-components/","text":"Deployment of application microservices for the Event-Driven Architecture Reference Application Environment prerequisites Kafka Topic Creation You can create the topics using the Event Streams console: or if you have manually deployed Event Streams or Kafka, you can use commands similar to the snippet below: # get the name of the Kafka pod $ export NAMESPACE = <target k8s namespace / ocp project> $ export KPOF = $( kubectl get pods -n ${ NAMESPACE } | grep kafka | awk '{print $1;}' ) $ cat ${ KPOF } rolling-streams-ibm-es-kafka-sts-0 rolling-streams-ibm-es-kafka-sts-1 rolling-streams-ibm-es-kafka-sts-2 # Then get the name of the zookeeper service: $ export ZOOKSVC = $( kubectl get svc -n ${ NAMESPACE } | grep zoo | awk '{print $1;}' | head -1 ) rolling-streams-ibm-es-zookeeper-fixed-ip-svc-0 # Then remote exec a shell on one of this broker to configure the topic - for example the \"orders\" topic $ kubectl exec -n ${ NAMESPACE } -ti ${ KPOF } -- bash -c \"/opt/kafka/bin/kafka-topics.sh --create --zookeeper ${ ZOOKSVC } :2181 --replication-factor 1 --partitions 1 --topic orders\" The topics that need to be created are: bluewaterContainer bluewaterShip bluewaterProblem orders orderCommands rejected-orders allocated-orders errors containers containerMetrics reeferTelemetries The command scripts/createTopicsOnK8S.sh creates those topics automatically. Docker registries You will need a Docker image registry to push and pull your images to and from. There are multiple options depending on your use cases and we are only documenting a subset of potential solutions, including but not limited to IBM Cloud Container Registry, Docker Hub, Quay, etc. IBM Cloud Container Registry Install IBM Cloud Container Registry CLI plug-in if needed: ibmcloud plugin install container - registry - r Bluemix Define a private image repository Use the IBM Cloud Container Registry to push your images and then deploy them to any Kubernetes cluster with access to the public internet. When deploying enterprise applications, it is strongly recommended to use private registry to protect your images from being used and changed by unauthorized users. Private registries must be set up by the cluster administrator to ensure that the credentials to access the private registry are available to the cluster users. Create a namespace inside your Container Registry for use here: ibmcloud cr namespace-add ibmcaseeda We will use this namespace when tagging the docker images for our microservices. Here is an example of tagging: docker tag ibmcase/kcontainer-ui us.icr.io/ibmcaseeda/kcontainer-ui:latest To see the images in your private registry you can use the user interface at https://cloud.ibm.com/containers-kubernetes/registry/main/private or the command: ibmcloud cr image-list Private Registry Token Each Helm Chart specifies the name of the Docker image to load the containers & pods. To enable access from Kubernetes Nodes to your private registry, an image pull secret is required and will be stored in a Kubernetes secret. If you are using public Docker Hub image repositories, an image pull secret is not required. Using secret is also mandatory when registry and clusters are not in the same region. Verify current secrets for a given namespace: kubectl describe secrets -n <target namespace> Get a security token: (these can be permanent or renewable ) ibmcloud cr token-add --description \"private registry secret for <target namespace>\" --non-expiring -q To list the available tokens: ibmcloud cr tokens The result: TOKEN ID READONLY EXPIRY DESCRIPTION 2b5ff00e-a.. true 0 token for somebody 3dbf72eb-6.. true 0 private registry secret for browncompute Get the token for a given token identifier: ibmcloud cr token-get cce5a800-... Define the secret to store the Event stream API key token information: kubectl --namespace <target namespace> create secret docker-registry <target namespace>-registry-secret --docker-server = <registry_url> --docker-username = token --docker-password = <token_value> --docker-email = <docker_email> Verify the secret kubectl get secrets -n <target namespace> You will see something like below. NAME TYPE DATA AGE browncompute-registry-secret kubernetes.io/dockerconfigjson 1 2m default-token-ggwl2 kubernetes.io/service-account-token 3 41m eventstreams-apikey Opaque 1 24m Basic Kubernetes IBM Cloud Kubernetes Service To create the cluster follow this tutorial . OpenShift Container Platform 3.11 This needs to be done once per unique deployment of the entire application. If desired, create a non-default Service Account for usage of deploying and running the K Container reference implementation. This will become more important in future iterations, so it's best to start small: Command: oc create serviceaccount -n <target-namespace> kcontainer-runtime Example: oc create serviceaccount -n eda-refarch kcontainer-runtime The target Service Account needs to be allowed to run containers as anyuid for the time being: Command: oc adm policy add-scc-to-user anyuid -z <service-account-name> -n <target-namespace> Example: oc adm policy add-scc-to-user anyuid -z kcontainer-runtime -n eda-refarch NOTE: This requires cluster-admin level privileges. OpenShift Container Platform 4.X TODO OpenShift Container Platform 4.X Prereqs Deploy application microservices Using the master repository You can download the necessary application microservice repsoitories using scripts provided in the master repository: git clone https://github.com/ibm-cloud-architecture/refarch-kc.git cd refarch-kc ./scripts/clone.sh Deploy Order Command microservice Go to the repo cd refarch-kc-order-ms/order-command-ms Build the image docker build -t order-command-ms:latest -f Dockerfile.multistage . Tag the image docker tag order-command-ms <private-registry>/<image-namespace>/order-command-ms:latest Push the image docker login <private-registry> docker push <private-registry>/<image-namespace>/order-command-ms:latest Generate application YAMLs via helm template with the following parameters: --set image.repository=<private-registry>/<image-namespace>/<image-repository> --set image.pullSecret=<private-registry-pullsecret> (only required if pulling from an external private registry) --set kafka.brokersConfigMap=<kafka brokers ConfigMap name> --set eventstreams.enabled=(true/false) ( true when connecting to Event Streams of any kind, false when connecting to Kafka directly) --set eventstreams.apikeyConfigMap=<kafka api key Secret name> --set eventstreams.truststoreRequired=(true/false) ( true when connecting to Event Streams via ICP4I) --set eventstreams.truststoreSecret=<eventstreams jks file secret name> (only used when connecting to Event Streams via ICP4I) --set eventstreams.truststorePassword=<eventstreams jks password> (only used when connecting to Event Streams via ICP4I) --set serviceAccountName=<service-account-name> --namespace <target-namespace> --output-dir <local-template-directory> Example using Event Streams via ICP4I: helm template --set image.repository = rhos-quay.internal-network.local/browncompute/order-command-ms --set kafka.brokersConfigMap = es-kafka-brokers --set eventstreams.enabled = true --set eventstreams.apikeyConfigMap = es-eventstreams-apikey --set serviceAccountName = kcontainer-runtime --set eventstreams.truststoreRequired = true --set eventstreams.truststoreSecret = es-truststore-jks --set eventstreams.truststorePassword = password --output-dir templates --namespace eda-refarch chart/ordercommandms Example using Event Streams hosted on IBM Cloud: helm template --set image.repository = rhos-quay.internal-network.local/browncompute/order-command-ms --set kafka.brokersConfigMap = kafka-brokers --set eventstreams.enabled = true --set eventstreams.apikeyConfigMap = eventstreams-apikey --set serviceAccountName = kcontainer-runtime --output-dir templates --namespace eda-refarch chart/ordercommandms Deploy application using kubectl/oc apply : ( kubectl/oc ) apply -f templates/ordercommandms/templates Verify default service is running correctly: Without any previously tests done, the call below should return an empty array: [] curl http://<cluster endpoints>:31200/orders Deploy Order Query microservice Go to the repo cd refarch-kc-order-ms/order-query-ms Build the image docker build -t order-query-ms:latest -f Dockerfile.multistage . Tag the image docker tag order-query-ms <private-registry>/<image-namespace>/order-query-ms:latest Push the image docker login <private-registry> docker push <private-registry>/<image-namespace>/order-query-ms:latest Generate application YAMLs via helm template with the following parameters: --set image.repository=<private-registry>/<image-namespace>/<image-repository> --set image.pullSecret=<private-registry-pullsecret> (only required if pulling from an external private registry) --set kafka.brokersConfigMap=<kafka brokers ConfigMap name> --set eventstreams.enabled=(true/false) ( true when connecting to Event Streams of any kind, false when connecting to Kafka directly) --set eventstreams.apikeyConfigMap=<kafka api key Secret name> --set eventstreams.truststoreRequired=(true/false) ( true when connecting to Event Streams via ICP4I) --set eventstreams.truststoreSecret=<eventstreams jks file secret name> (only used when connecting to Event Streams via ICP4I) --set eventstreams.truststorePassword=<eventstreams jks password> (only used when connecting to Event Streams via ICP4I) --set serviceAccountName=<service-account-name> --namespace <target-namespace> --output-dir <local-template-directory> Example using Event Streams via ICP4I: helm template --set image.repository = rhos-quay.internal-network.local/browncompute/order-query-ms --set kafka.brokersConfigMap = es-kafka-brokers --set eventstreams.enabled = true --set eventstreams.apikeyConfigMap = es-eventstreams-apikey --set serviceAccountName = kcontainer-runtime --set eventstreams.truststoreRequired = true --set eventstreams.truststoreSecret = es-truststore-jks --set eventstreams.truststorePassword = password --output-dir templates --namespace eda-refarch chart/orderqueryms Example using Event Streams hosted on IBM Cloud: helm template --set image.repository = rhos-quay.internal-network.local/browncompute/order-query-ms --set kafka.brokersConfigMap = kafka-brokers --set eventstreams.enabled = true --set eventstreams.apikeyConfigMap = eventstreams-apikey --set serviceAccountName = kcontainer-runtime --output-dir templates --namespace eda-refarch chart/orderqueryms Deploy application using kubectl/oc apply : ( kubectl/oc ) apply -f templates/orderqueryms/templates Verify default service is running correctly: Without any previously tests done, the call below should return an empty array: [] curl http://<cluster endpoints>:31100/orders Deploy Container microservice TODO Container Microservice requires POSTGRES parameters Go to the repo cd refarch-kc-container-ms/SpringContainerMS Build the image docker build -t kc-spring-container-ms:latest -f Dockerfile . Tag the image docker tag kc-spring-container-ms <private-registry>/<image-namespace>/kc-spring-container-ms:latest Push the image docker login <private-registry> docker push <private-registry>/<image-namespace>/kc-spring-container-ms:latest Generate application YAMLs via helm template with the following parameters: --set image.repository=<private-registry>/<image-namespace>/<image-repository> --set image.pullSecret=<private-registry-pullsecret> (only required if pulling from an external private registry) --set kafka.brokersConfigMap=<kafka brokers ConfigMap name> --set eventstreams.enabled=(true/false) ( true when connecting to Event Streams of any kind, false when connecting to Kafka directly) --set eventstreams.apikeyConfigMap=<kafka api key Secret name> --set eventstreams.caPemFileRequired=(true/false) ( true when connecting to Event Streams via ICP4I) --set eventstreams.caPemSecretName=<eventstreams ca pem file secret name> (only used when connecting to Event Streams via ICP4I) --set postgresql.capemRequired=(true/false) ( true when connecting to Postgresql Services requiring SSL and CA PEM-secured communication) --set postgresql.capemSecret=<postgresql CA pem certificate Secret name> --set postgresql.urlSecret=<postgresql url Secret name> --set postgresql.userSecret=<postgresql user Secret name> --set postgresql.passwordSecret=<postgresql password Secret name> --set serviceAccountName=<service-account-name> --namespace <target-namespace> --output-dir <local-template-directory> Example using Event Streams via ICP4I: helm template --set image.repository = rhos-quay.internal-network.local/browncompute/kc-spring-container-ms --set kafka.brokersConfigMap = es-kafka-brokers --set eventstreams.enabled = true --set eventstreams.apikeyConfigMap = es-eventstreams-apikey --set eventstreams.truststoreRequired = true --set eventstreams.truststoreSecret = es-truststore-jks --set eventstreams.truststorePassword = password --set postgresql.capemRequired = true --set postgresql.capemSecret = postgresql-ca-pem --set postgresql.urlSecret = postgresql-url --set postgresql.userSecret = postgresql-user --set postgresql.passwordSecret = postgresql-pwd --set serviceAccountName = kcontainer-runtime --output-dir templates --namespace eda-refarch chart/springcontainerms Example using Event Streams hosted on IBM Cloud: helm template --set image.repository = rhos-quay.internal-network.local/browncompute/kc-spring-container-ms --set kafka.brokersConfigMap = es-kafka-brokers --set eventstreams.enabled = true --set eventstreams.apikeyConfigMap = es-eventstreams-apikey --set postgresql.capemRequired = true --set postgresql.capemSecret = postgresql-ca-pem --set postgresql.urlSecret = postgresql-url --set postgresql.userSecret = postgresql-user --set postgresql.passwordSecret = postgresql-pwd --set serviceAccountName = kcontainer-runtime --output-dir templates --namespace eda-refarch chart/springcontainerms Deploy application using kubectl/oc apply : ( kubectl/oc ) apply -f templates/springcontainerms/templates Verify default service is running correctly: curl http://cluster-endpoints:31900/containers Deploy Voyages microservice The Voyage microservice is a simple nodejs app to mockup schedule of vessels between two harbors. It is here to illustrate Kafka integration with nodejs app. Go to the repo cd refarch-kc-ms/voyages-ms Build the image docker build -t kc-voyages-ms:latest -f Dockerfile . Tag the image docker tag kc-voyages-ms <private-registry>/<image-namespace>/kc-voyages-ms:latest Push the image docker login <private-registry> docker push <private-registry>/<image-namespace>/kc-voyages-ms:latest Generate application YAMLs via helm template : --set image.repository=<private-registry>/<image-namespace>/<image-repository> --set image.pullSecret=<private-registry-pullsecret> (only required if pulling from an external private registry) --set kafka.brokersConfigMap=<kafka brokers ConfigMap name> --set eventstreams.enabled=(true/false) ( true when connecting to Event Streams of any kind, false when connecting to Kafka directly) --set eventstreams.apikeyConfigMap=<kafka api key Secret name> --set eventstreams.caPemFileRequired=(true/false) ( true when connecting to Event Streams via ICP4I) --set eventstreams.caPemSecretName=<eventstreams ca pem file secret name> (only used when connecting to Event Streams via ICP4I) --set serviceAccountName=<service-account-name> --namespace <target-namespace> --output-dir <local-template-directory> Example using Event Streams via ICP4I: helm template --set image.repository = rhos-quay.internal-network.local/browncompute/kc-voyages-ms --set kafka.brokersConfigMap = es-kafka-brokers --set eventstreams.enabled = true --set eventstreams.apikeyConfigMap = es-eventstreams-apikey --set serviceAccountName = kcontainer-runtime --set eventstreams.caPemFileRequired = true --set eventstreams.caPemSecretName = es-ca-pemfile --output-dir templates --namespace eda-refarch chart/voyagesms Example using Event Streams hosted on IBM Cloud: helm template --set image.repository = rhos-quay.internal-network.local/browncompute/kc-voyages-ms --set kafka.brokersConfigMap = kafka-brokers --set eventstreams.enabled = true --set eventstreams.apikeyConfigMap = eventstreams-apikey --set serviceAccountName = kcontainer-runtime --output-dir templates --namespace eda-refarch chart/voyagesms Deploy application using kubectl/oc apply : ( kubectl/oc ) apply -f templates/voyagesms/templates Verify default service is running correctly: curl http://cluster-endpoint:31000/voyage Deploy the Fleet Simulator microservice The Fleet simulator is to move vessels from one harbors to another, and send container metrics while the containers are on a vessel. It has some predefined simulation to trigger some events. Go to the repo cd cd refarch-kc-ms/fleet-ms Build the image docker build -t kc-fleet-ms:latest -f Dockerfile.multistage . Tag the image docker tag kc-fleet-ms <private-registry>/<image-namespace>/kc-fleet-ms:latest Push the image docker login <private-registry> docker push <private-registry>/<image-namespace>/kc-fleet-ms:latest Generate application YAMLs via helm template : --set image.repository=<private-registry>/<image-namespace>/<image-repository> --set image.pullSecret=<private-registry-pullsecret> (only required if pulling from an external private registry) --set kafka.brokersConfigMap=<kafka brokers ConfigMap name> --set eventstreams.enabled=(true/false) ( true when connecting to Event Streams of any kind, false when connecting to Kafka directly) --set eventstreams.apikeyConfigMap=<kafka api key Secret name> --set eventstreams.truststoreRequired=(true/false) ( true when connecting to Event Streams via ICP4I) --set eventstreams.truststoreSecret=<eventstreams jks file secret name> (only used when connecting to Event Streams via ICP4I) --set eventstreams.truststorePassword=<eventstreams jks password> (only used when connecting to Event Streams via ICP4I) --set serviceAccountName=<service-account-name> --namespace <target-namespace> --output-dir <local-template-directory> Example using Event Streams via ICP4I: helm template --set image.repository = rhos-quay.internal-network.local/browncompute/kc-fleet-ms --set kafka.brokersConfigMap = es-kafka-brokers --set eventstreams.enabled = true --set eventstreams.apikeyConfigMap = es-eventstreams-apikey --set serviceAccountName = kcontainer-runtime --set eventstreams.truststoreRequired = true --set eventstreams.truststoreSecret = es-truststore-jks --set eventstreams.truststorePassword = password --output-dir templates --namespace eda-refarch chart/fleetms Example using Event Streams hosted on IBM Cloud: helm template --set image.repository = rhos-quay.internal-network.local/browncompute/kc-fleet-ms --set kafka.brokersConfigMap = kafka-brokers --set eventstreams.enabled = true --set eventstreams.apikeyConfigMap = eventstreams-apikey --set serviceAccountName = kcontainer-runtime --output-dir templates --namespace eda-refarch chart/fleetms Deploy application using kubectl/oc apply : ( kubectl/oc ) apply -f templates/fleetms/templates Verify default service is running correctly: At the beginning the call below should return an empty array: [] curl http://cluster-endpoint:31300/fleetms/fleets Deploy User Interface microservice TODO User Interface updates for ES ICP Go to the repo cd refarch - kc - ui / Build the image docker build -t kcontainer-ui:latest -f Dockerfile . Tag the image docker tag kc-ui <private-registry>/<image-namespace>/kcontainer-ui:latest Push the image docker login <private-registry> docker push <private-registry>/<image-namespace>/kcontainer-ui:latest Generate application YAMLs via helm template with the following parameters: --set image.repository=<private-registry>/<image-namespace>/<image-repository> --set image.tag=latest --set image.pullSecret=<private-registry-pullsecret> (only required if pulling from an external private registry) --set image.pullPolicy=Always --set eventstreams.env=ICP --set eventstreams.brokersConfigMap=<kafka brokers ConfigMap name> --set serviceAccountName=<service-account-name> --namespace <target-namespace> --output-dir <local-template-directory> # Example parameters helm template --set image.repository = rhos-quay.internal-network.local/browncompute/kc-ui --set image.tag = latest --set image.pullPolicy = Always --set eventstreams.env = ICP --set eventstreams.brokersConfigMap = kafka-brokers --set serviceAccountName = kcontainer-runtime --output-dir templates --namespace eda-refarch chart/kc-ui Deploy application using kubectl/oc apply : ( kubectl/oc ) apply -f templates/kc-ui/templates Verify the installed app Point your web browser to http://cluster-endpoints:31010 and login with username: eddie@email.com and password Eddie. Integration Tests Integration tests are provided in the itg-tests directory and are designed to be run in cluster with the rest of the application components. However, they are regular Python scripts that can be adapted to be runnable anywhere, given the correct Kafka endpoints and configuration information. For simplicity, this quick walkthrough will document how you can build and deploy Docker images that will run the integration tests inside the cluster, with the results visible via kubectl logs and the rest of the application's APIs. Build the base Docker image # From the root of the 'refarch-kc' repository docker build -f docker/docker-python-tools -t osowski/python-tools . docker push osowski/python-tools The above image is a base Python image with our integration tests, defined in the itg-tests directory. It is a long-running Flask process that provides a simple web server, so once deployed, it will remain available to \"exec\" into for additional in-cluster CLI interaction. However, for simplicity, we have defined a few integration scenarios, using a Kubernetes Deployment and multiple Kubernetes Jobs, that will automate some of the integration test scenarios. Update the itg-tests/kustomization.yaml file with the specifics for your python-tools Docker image, changing the newName and newTag fields, as appropriate, along with the namespace field. Then run the following the command to apply the customization and deploy to the platform: # From the root of the 'refarch-kc' repository kubectl apply -k itg-tests/ NOTE: kubectl must be at level 1.14 or higher for the -k flag to be available. This Kubernetes YAML will create one Deployment and one Job. The long-running Deployment will run the OrdersPython/OrderConsumer.py script to watch for order events in the Kafka backend, while the short-lived Job create all the necessary order events via the es-it/ProducerOrderEvents.py script and publish them to Kafka. View the output of the es-it/ProducerOrderEvents.py Job: ( kubectl/oc ) get jobs | grep kcontainer ( kubectl/oc ) logs -f <pod_name> View the output of the OrdersPython/OrderConsumer.py Deployment: ( kubectl/oc ) get pods | grep consumer ( kubectl/oc ) logs -f <pod_name> You should see the same Order ID created by the Job in the output of the Deployment's container. Universal deployment considerations When deploying kafka consumer it is important to assess the horizontal pod autoscaler settings and needs, as adding consumers will not address scalability if the number of partitions in the topic(s) to consume does not match the increase of consumers. So disable HPA by default. If you want to use HPA you also need to ensure that a metrics-server is running, then set the number of partition, and the hpa.maxReplicas to the number of partitions.","title":"Application Components"},{"location":"deployments/application-components/#environment-prerequisites","text":"","title":"Environment prerequisites"},{"location":"deployments/application-components/#kafka-topic-creation","text":"You can create the topics using the Event Streams console: or if you have manually deployed Event Streams or Kafka, you can use commands similar to the snippet below: # get the name of the Kafka pod $ export NAMESPACE = <target k8s namespace / ocp project> $ export KPOF = $( kubectl get pods -n ${ NAMESPACE } | grep kafka | awk '{print $1;}' ) $ cat ${ KPOF } rolling-streams-ibm-es-kafka-sts-0 rolling-streams-ibm-es-kafka-sts-1 rolling-streams-ibm-es-kafka-sts-2 # Then get the name of the zookeeper service: $ export ZOOKSVC = $( kubectl get svc -n ${ NAMESPACE } | grep zoo | awk '{print $1;}' | head -1 ) rolling-streams-ibm-es-zookeeper-fixed-ip-svc-0 # Then remote exec a shell on one of this broker to configure the topic - for example the \"orders\" topic $ kubectl exec -n ${ NAMESPACE } -ti ${ KPOF } -- bash -c \"/opt/kafka/bin/kafka-topics.sh --create --zookeeper ${ ZOOKSVC } :2181 --replication-factor 1 --partitions 1 --topic orders\" The topics that need to be created are: bluewaterContainer bluewaterShip bluewaterProblem orders orderCommands rejected-orders allocated-orders errors containers containerMetrics reeferTelemetries The command scripts/createTopicsOnK8S.sh creates those topics automatically.","title":"Kafka Topic Creation"},{"location":"deployments/application-components/#docker-registries","text":"You will need a Docker image registry to push and pull your images to and from. There are multiple options depending on your use cases and we are only documenting a subset of potential solutions, including but not limited to IBM Cloud Container Registry, Docker Hub, Quay, etc.","title":"Docker registries"},{"location":"deployments/application-components/#ibm-cloud-container-registry","text":"Install IBM Cloud Container Registry CLI plug-in if needed: ibmcloud plugin install container - registry - r Bluemix","title":"IBM Cloud Container Registry"},{"location":"deployments/application-components/#define-a-private-image-repository","text":"Use the IBM Cloud Container Registry to push your images and then deploy them to any Kubernetes cluster with access to the public internet. When deploying enterprise applications, it is strongly recommended to use private registry to protect your images from being used and changed by unauthorized users. Private registries must be set up by the cluster administrator to ensure that the credentials to access the private registry are available to the cluster users. Create a namespace inside your Container Registry for use here: ibmcloud cr namespace-add ibmcaseeda We will use this namespace when tagging the docker images for our microservices. Here is an example of tagging: docker tag ibmcase/kcontainer-ui us.icr.io/ibmcaseeda/kcontainer-ui:latest To see the images in your private registry you can use the user interface at https://cloud.ibm.com/containers-kubernetes/registry/main/private or the command: ibmcloud cr image-list","title":"Define a private image repository"},{"location":"deployments/application-components/#private-registry-token","text":"Each Helm Chart specifies the name of the Docker image to load the containers & pods. To enable access from Kubernetes Nodes to your private registry, an image pull secret is required and will be stored in a Kubernetes secret. If you are using public Docker Hub image repositories, an image pull secret is not required. Using secret is also mandatory when registry and clusters are not in the same region. Verify current secrets for a given namespace: kubectl describe secrets -n <target namespace> Get a security token: (these can be permanent or renewable ) ibmcloud cr token-add --description \"private registry secret for <target namespace>\" --non-expiring -q To list the available tokens: ibmcloud cr tokens The result: TOKEN ID READONLY EXPIRY DESCRIPTION 2b5ff00e-a.. true 0 token for somebody 3dbf72eb-6.. true 0 private registry secret for browncompute Get the token for a given token identifier: ibmcloud cr token-get cce5a800-... Define the secret to store the Event stream API key token information: kubectl --namespace <target namespace> create secret docker-registry <target namespace>-registry-secret --docker-server = <registry_url> --docker-username = token --docker-password = <token_value> --docker-email = <docker_email> Verify the secret kubectl get secrets -n <target namespace> You will see something like below. NAME TYPE DATA AGE browncompute-registry-secret kubernetes.io/dockerconfigjson 1 2m default-token-ggwl2 kubernetes.io/service-account-token 3 41m eventstreams-apikey Opaque 1 24m","title":"Private Registry Token"},{"location":"deployments/application-components/#basic-kubernetes","text":"","title":"Basic Kubernetes"},{"location":"deployments/application-components/#ibm-cloud-kubernetes-service","text":"To create the cluster follow this tutorial .","title":"IBM Cloud Kubernetes Service"},{"location":"deployments/application-components/#openshift-container-platform-311","text":"This needs to be done once per unique deployment of the entire application. If desired, create a non-default Service Account for usage of deploying and running the K Container reference implementation. This will become more important in future iterations, so it's best to start small: Command: oc create serviceaccount -n <target-namespace> kcontainer-runtime Example: oc create serviceaccount -n eda-refarch kcontainer-runtime The target Service Account needs to be allowed to run containers as anyuid for the time being: Command: oc adm policy add-scc-to-user anyuid -z <service-account-name> -n <target-namespace> Example: oc adm policy add-scc-to-user anyuid -z kcontainer-runtime -n eda-refarch NOTE: This requires cluster-admin level privileges.","title":"OpenShift Container Platform 3.11"},{"location":"deployments/application-components/#openshift-container-platform-4x","text":"TODO OpenShift Container Platform 4.X Prereqs","title":"OpenShift Container Platform 4.X"},{"location":"deployments/application-components/#deploy-application-microservices","text":"","title":"Deploy application microservices"},{"location":"deployments/application-components/#using-the-master-repository","text":"You can download the necessary application microservice repsoitories using scripts provided in the master repository: git clone https://github.com/ibm-cloud-architecture/refarch-kc.git cd refarch-kc ./scripts/clone.sh","title":"Using the master repository"},{"location":"deployments/application-components/#deploy-order-command-microservice","text":"Go to the repo cd refarch-kc-order-ms/order-command-ms Build the image docker build -t order-command-ms:latest -f Dockerfile.multistage . Tag the image docker tag order-command-ms <private-registry>/<image-namespace>/order-command-ms:latest Push the image docker login <private-registry> docker push <private-registry>/<image-namespace>/order-command-ms:latest Generate application YAMLs via helm template with the following parameters: --set image.repository=<private-registry>/<image-namespace>/<image-repository> --set image.pullSecret=<private-registry-pullsecret> (only required if pulling from an external private registry) --set kafka.brokersConfigMap=<kafka brokers ConfigMap name> --set eventstreams.enabled=(true/false) ( true when connecting to Event Streams of any kind, false when connecting to Kafka directly) --set eventstreams.apikeyConfigMap=<kafka api key Secret name> --set eventstreams.truststoreRequired=(true/false) ( true when connecting to Event Streams via ICP4I) --set eventstreams.truststoreSecret=<eventstreams jks file secret name> (only used when connecting to Event Streams via ICP4I) --set eventstreams.truststorePassword=<eventstreams jks password> (only used when connecting to Event Streams via ICP4I) --set serviceAccountName=<service-account-name> --namespace <target-namespace> --output-dir <local-template-directory> Example using Event Streams via ICP4I: helm template --set image.repository = rhos-quay.internal-network.local/browncompute/order-command-ms --set kafka.brokersConfigMap = es-kafka-brokers --set eventstreams.enabled = true --set eventstreams.apikeyConfigMap = es-eventstreams-apikey --set serviceAccountName = kcontainer-runtime --set eventstreams.truststoreRequired = true --set eventstreams.truststoreSecret = es-truststore-jks --set eventstreams.truststorePassword = password --output-dir templates --namespace eda-refarch chart/ordercommandms Example using Event Streams hosted on IBM Cloud: helm template --set image.repository = rhos-quay.internal-network.local/browncompute/order-command-ms --set kafka.brokersConfigMap = kafka-brokers --set eventstreams.enabled = true --set eventstreams.apikeyConfigMap = eventstreams-apikey --set serviceAccountName = kcontainer-runtime --output-dir templates --namespace eda-refarch chart/ordercommandms Deploy application using kubectl/oc apply : ( kubectl/oc ) apply -f templates/ordercommandms/templates Verify default service is running correctly: Without any previously tests done, the call below should return an empty array: [] curl http://<cluster endpoints>:31200/orders","title":"Deploy Order Command microservice"},{"location":"deployments/application-components/#deploy-order-query-microservice","text":"Go to the repo cd refarch-kc-order-ms/order-query-ms Build the image docker build -t order-query-ms:latest -f Dockerfile.multistage . Tag the image docker tag order-query-ms <private-registry>/<image-namespace>/order-query-ms:latest Push the image docker login <private-registry> docker push <private-registry>/<image-namespace>/order-query-ms:latest Generate application YAMLs via helm template with the following parameters: --set image.repository=<private-registry>/<image-namespace>/<image-repository> --set image.pullSecret=<private-registry-pullsecret> (only required if pulling from an external private registry) --set kafka.brokersConfigMap=<kafka brokers ConfigMap name> --set eventstreams.enabled=(true/false) ( true when connecting to Event Streams of any kind, false when connecting to Kafka directly) --set eventstreams.apikeyConfigMap=<kafka api key Secret name> --set eventstreams.truststoreRequired=(true/false) ( true when connecting to Event Streams via ICP4I) --set eventstreams.truststoreSecret=<eventstreams jks file secret name> (only used when connecting to Event Streams via ICP4I) --set eventstreams.truststorePassword=<eventstreams jks password> (only used when connecting to Event Streams via ICP4I) --set serviceAccountName=<service-account-name> --namespace <target-namespace> --output-dir <local-template-directory> Example using Event Streams via ICP4I: helm template --set image.repository = rhos-quay.internal-network.local/browncompute/order-query-ms --set kafka.brokersConfigMap = es-kafka-brokers --set eventstreams.enabled = true --set eventstreams.apikeyConfigMap = es-eventstreams-apikey --set serviceAccountName = kcontainer-runtime --set eventstreams.truststoreRequired = true --set eventstreams.truststoreSecret = es-truststore-jks --set eventstreams.truststorePassword = password --output-dir templates --namespace eda-refarch chart/orderqueryms Example using Event Streams hosted on IBM Cloud: helm template --set image.repository = rhos-quay.internal-network.local/browncompute/order-query-ms --set kafka.brokersConfigMap = kafka-brokers --set eventstreams.enabled = true --set eventstreams.apikeyConfigMap = eventstreams-apikey --set serviceAccountName = kcontainer-runtime --output-dir templates --namespace eda-refarch chart/orderqueryms Deploy application using kubectl/oc apply : ( kubectl/oc ) apply -f templates/orderqueryms/templates Verify default service is running correctly: Without any previously tests done, the call below should return an empty array: [] curl http://<cluster endpoints>:31100/orders","title":"Deploy Order Query microservice"},{"location":"deployments/application-components/#deploy-container-microservice","text":"TODO Container Microservice requires POSTGRES parameters Go to the repo cd refarch-kc-container-ms/SpringContainerMS Build the image docker build -t kc-spring-container-ms:latest -f Dockerfile . Tag the image docker tag kc-spring-container-ms <private-registry>/<image-namespace>/kc-spring-container-ms:latest Push the image docker login <private-registry> docker push <private-registry>/<image-namespace>/kc-spring-container-ms:latest Generate application YAMLs via helm template with the following parameters: --set image.repository=<private-registry>/<image-namespace>/<image-repository> --set image.pullSecret=<private-registry-pullsecret> (only required if pulling from an external private registry) --set kafka.brokersConfigMap=<kafka brokers ConfigMap name> --set eventstreams.enabled=(true/false) ( true when connecting to Event Streams of any kind, false when connecting to Kafka directly) --set eventstreams.apikeyConfigMap=<kafka api key Secret name> --set eventstreams.caPemFileRequired=(true/false) ( true when connecting to Event Streams via ICP4I) --set eventstreams.caPemSecretName=<eventstreams ca pem file secret name> (only used when connecting to Event Streams via ICP4I) --set postgresql.capemRequired=(true/false) ( true when connecting to Postgresql Services requiring SSL and CA PEM-secured communication) --set postgresql.capemSecret=<postgresql CA pem certificate Secret name> --set postgresql.urlSecret=<postgresql url Secret name> --set postgresql.userSecret=<postgresql user Secret name> --set postgresql.passwordSecret=<postgresql password Secret name> --set serviceAccountName=<service-account-name> --namespace <target-namespace> --output-dir <local-template-directory> Example using Event Streams via ICP4I: helm template --set image.repository = rhos-quay.internal-network.local/browncompute/kc-spring-container-ms --set kafka.brokersConfigMap = es-kafka-brokers --set eventstreams.enabled = true --set eventstreams.apikeyConfigMap = es-eventstreams-apikey --set eventstreams.truststoreRequired = true --set eventstreams.truststoreSecret = es-truststore-jks --set eventstreams.truststorePassword = password --set postgresql.capemRequired = true --set postgresql.capemSecret = postgresql-ca-pem --set postgresql.urlSecret = postgresql-url --set postgresql.userSecret = postgresql-user --set postgresql.passwordSecret = postgresql-pwd --set serviceAccountName = kcontainer-runtime --output-dir templates --namespace eda-refarch chart/springcontainerms Example using Event Streams hosted on IBM Cloud: helm template --set image.repository = rhos-quay.internal-network.local/browncompute/kc-spring-container-ms --set kafka.brokersConfigMap = es-kafka-brokers --set eventstreams.enabled = true --set eventstreams.apikeyConfigMap = es-eventstreams-apikey --set postgresql.capemRequired = true --set postgresql.capemSecret = postgresql-ca-pem --set postgresql.urlSecret = postgresql-url --set postgresql.userSecret = postgresql-user --set postgresql.passwordSecret = postgresql-pwd --set serviceAccountName = kcontainer-runtime --output-dir templates --namespace eda-refarch chart/springcontainerms Deploy application using kubectl/oc apply : ( kubectl/oc ) apply -f templates/springcontainerms/templates Verify default service is running correctly: curl http://cluster-endpoints:31900/containers","title":"Deploy Container microservice"},{"location":"deployments/application-components/#deploy-voyages-microservice","text":"The Voyage microservice is a simple nodejs app to mockup schedule of vessels between two harbors. It is here to illustrate Kafka integration with nodejs app. Go to the repo cd refarch-kc-ms/voyages-ms Build the image docker build -t kc-voyages-ms:latest -f Dockerfile . Tag the image docker tag kc-voyages-ms <private-registry>/<image-namespace>/kc-voyages-ms:latest Push the image docker login <private-registry> docker push <private-registry>/<image-namespace>/kc-voyages-ms:latest Generate application YAMLs via helm template : --set image.repository=<private-registry>/<image-namespace>/<image-repository> --set image.pullSecret=<private-registry-pullsecret> (only required if pulling from an external private registry) --set kafka.brokersConfigMap=<kafka brokers ConfigMap name> --set eventstreams.enabled=(true/false) ( true when connecting to Event Streams of any kind, false when connecting to Kafka directly) --set eventstreams.apikeyConfigMap=<kafka api key Secret name> --set eventstreams.caPemFileRequired=(true/false) ( true when connecting to Event Streams via ICP4I) --set eventstreams.caPemSecretName=<eventstreams ca pem file secret name> (only used when connecting to Event Streams via ICP4I) --set serviceAccountName=<service-account-name> --namespace <target-namespace> --output-dir <local-template-directory> Example using Event Streams via ICP4I: helm template --set image.repository = rhos-quay.internal-network.local/browncompute/kc-voyages-ms --set kafka.brokersConfigMap = es-kafka-brokers --set eventstreams.enabled = true --set eventstreams.apikeyConfigMap = es-eventstreams-apikey --set serviceAccountName = kcontainer-runtime --set eventstreams.caPemFileRequired = true --set eventstreams.caPemSecretName = es-ca-pemfile --output-dir templates --namespace eda-refarch chart/voyagesms Example using Event Streams hosted on IBM Cloud: helm template --set image.repository = rhos-quay.internal-network.local/browncompute/kc-voyages-ms --set kafka.brokersConfigMap = kafka-brokers --set eventstreams.enabled = true --set eventstreams.apikeyConfigMap = eventstreams-apikey --set serviceAccountName = kcontainer-runtime --output-dir templates --namespace eda-refarch chart/voyagesms Deploy application using kubectl/oc apply : ( kubectl/oc ) apply -f templates/voyagesms/templates Verify default service is running correctly: curl http://cluster-endpoint:31000/voyage","title":"Deploy Voyages microservice"},{"location":"deployments/application-components/#deploy-the-fleet-simulator-microservice","text":"The Fleet simulator is to move vessels from one harbors to another, and send container metrics while the containers are on a vessel. It has some predefined simulation to trigger some events. Go to the repo cd cd refarch-kc-ms/fleet-ms Build the image docker build -t kc-fleet-ms:latest -f Dockerfile.multistage . Tag the image docker tag kc-fleet-ms <private-registry>/<image-namespace>/kc-fleet-ms:latest Push the image docker login <private-registry> docker push <private-registry>/<image-namespace>/kc-fleet-ms:latest Generate application YAMLs via helm template : --set image.repository=<private-registry>/<image-namespace>/<image-repository> --set image.pullSecret=<private-registry-pullsecret> (only required if pulling from an external private registry) --set kafka.brokersConfigMap=<kafka brokers ConfigMap name> --set eventstreams.enabled=(true/false) ( true when connecting to Event Streams of any kind, false when connecting to Kafka directly) --set eventstreams.apikeyConfigMap=<kafka api key Secret name> --set eventstreams.truststoreRequired=(true/false) ( true when connecting to Event Streams via ICP4I) --set eventstreams.truststoreSecret=<eventstreams jks file secret name> (only used when connecting to Event Streams via ICP4I) --set eventstreams.truststorePassword=<eventstreams jks password> (only used when connecting to Event Streams via ICP4I) --set serviceAccountName=<service-account-name> --namespace <target-namespace> --output-dir <local-template-directory> Example using Event Streams via ICP4I: helm template --set image.repository = rhos-quay.internal-network.local/browncompute/kc-fleet-ms --set kafka.brokersConfigMap = es-kafka-brokers --set eventstreams.enabled = true --set eventstreams.apikeyConfigMap = es-eventstreams-apikey --set serviceAccountName = kcontainer-runtime --set eventstreams.truststoreRequired = true --set eventstreams.truststoreSecret = es-truststore-jks --set eventstreams.truststorePassword = password --output-dir templates --namespace eda-refarch chart/fleetms Example using Event Streams hosted on IBM Cloud: helm template --set image.repository = rhos-quay.internal-network.local/browncompute/kc-fleet-ms --set kafka.brokersConfigMap = kafka-brokers --set eventstreams.enabled = true --set eventstreams.apikeyConfigMap = eventstreams-apikey --set serviceAccountName = kcontainer-runtime --output-dir templates --namespace eda-refarch chart/fleetms Deploy application using kubectl/oc apply : ( kubectl/oc ) apply -f templates/fleetms/templates Verify default service is running correctly: At the beginning the call below should return an empty array: [] curl http://cluster-endpoint:31300/fleetms/fleets","title":"Deploy the Fleet Simulator microservice"},{"location":"deployments/application-components/#deploy-user-interface-microservice","text":"TODO User Interface updates for ES ICP Go to the repo cd refarch - kc - ui / Build the image docker build -t kcontainer-ui:latest -f Dockerfile . Tag the image docker tag kc-ui <private-registry>/<image-namespace>/kcontainer-ui:latest Push the image docker login <private-registry> docker push <private-registry>/<image-namespace>/kcontainer-ui:latest Generate application YAMLs via helm template with the following parameters: --set image.repository=<private-registry>/<image-namespace>/<image-repository> --set image.tag=latest --set image.pullSecret=<private-registry-pullsecret> (only required if pulling from an external private registry) --set image.pullPolicy=Always --set eventstreams.env=ICP --set eventstreams.brokersConfigMap=<kafka brokers ConfigMap name> --set serviceAccountName=<service-account-name> --namespace <target-namespace> --output-dir <local-template-directory> # Example parameters helm template --set image.repository = rhos-quay.internal-network.local/browncompute/kc-ui --set image.tag = latest --set image.pullPolicy = Always --set eventstreams.env = ICP --set eventstreams.brokersConfigMap = kafka-brokers --set serviceAccountName = kcontainer-runtime --output-dir templates --namespace eda-refarch chart/kc-ui Deploy application using kubectl/oc apply : ( kubectl/oc ) apply -f templates/kc-ui/templates Verify the installed app Point your web browser to http://cluster-endpoints:31010 and login with username: eddie@email.com and password Eddie.","title":"Deploy User Interface microservice"},{"location":"deployments/application-components/#integration-tests","text":"Integration tests are provided in the itg-tests directory and are designed to be run in cluster with the rest of the application components. However, they are regular Python scripts that can be adapted to be runnable anywhere, given the correct Kafka endpoints and configuration information. For simplicity, this quick walkthrough will document how you can build and deploy Docker images that will run the integration tests inside the cluster, with the results visible via kubectl logs and the rest of the application's APIs. Build the base Docker image # From the root of the 'refarch-kc' repository docker build -f docker/docker-python-tools -t osowski/python-tools . docker push osowski/python-tools The above image is a base Python image with our integration tests, defined in the itg-tests directory. It is a long-running Flask process that provides a simple web server, so once deployed, it will remain available to \"exec\" into for additional in-cluster CLI interaction. However, for simplicity, we have defined a few integration scenarios, using a Kubernetes Deployment and multiple Kubernetes Jobs, that will automate some of the integration test scenarios. Update the itg-tests/kustomization.yaml file with the specifics for your python-tools Docker image, changing the newName and newTag fields, as appropriate, along with the namespace field. Then run the following the command to apply the customization and deploy to the platform: # From the root of the 'refarch-kc' repository kubectl apply -k itg-tests/ NOTE: kubectl must be at level 1.14 or higher for the -k flag to be available. This Kubernetes YAML will create one Deployment and one Job. The long-running Deployment will run the OrdersPython/OrderConsumer.py script to watch for order events in the Kafka backend, while the short-lived Job create all the necessary order events via the es-it/ProducerOrderEvents.py script and publish them to Kafka. View the output of the es-it/ProducerOrderEvents.py Job: ( kubectl/oc ) get jobs | grep kcontainer ( kubectl/oc ) logs -f <pod_name> View the output of the OrdersPython/OrderConsumer.py Deployment: ( kubectl/oc ) get pods | grep consumer ( kubectl/oc ) logs -f <pod_name> You should see the same Order ID created by the Job in the output of the Deployment's container.","title":"Integration Tests"},{"location":"deployments/application-components/#universal-deployment-considerations","text":"When deploying kafka consumer it is important to assess the horizontal pod autoscaler settings and needs, as adding consumers will not address scalability if the number of partitions in the topic(s) to consume does not match the increase of consumers. So disable HPA by default. If you want to use HPA you also need to ensure that a metrics-server is running, then set the number of partition, and the hpa.maxReplicas to the number of partitions.","title":"Universal deployment considerations"},{"location":"deployments/backing-services/","text":"Deployment of backing services for the Event-Driven Architecture Reference Application, which includes Event Streams and Postgresql. As part of the reference implementation, a Kafka instance is used as the main communication channel for all components of the application, while one of the components uses Postgresql for it's backing datastore. You will need these backing services deployed in the cluster or as a public cloud service before the application will run successfully. Kafka & Event Streams Using IBM Event Streams, hosted on IBM Cloud Service Deployment To provision your service, go to the IBM Cloud Catalog and search for Event Streams . It is in the Integration category. Create the service and specify a name, a region, and a resource group. Once the service is created, you are redirected to the Event Stream Standard Dashboard: In the Manage panel add the topics needed for the solution. We need at least the following: In the Service Credentials tab, create new credentials to get the Kafka broker list, the admim URL and the api_key needed to authenticate the consumers or producers. Event Streams Kafka Brokers Regardless of specific deployment targets (OCP, IKS, k8s), the following prerequisite Kubernetes ConfigMap needs to be created to support the deployments of the application's microservices. These artifacts need to be created once per unique deployment of the entire application and can be shared between application components in the same overall application deployment. These values can be acquired from the kafka_brokers_sasl section of the service instance's Service Credentials. kubectl create configmap kafka-brokers --from-literal = brokers = '<replace with comma-separated list of brokers>' -n <target k8s namespace / ocp project> kubectl describe configmap kafka-brokers -n <target k8s namespace / ocp project> Event Streams API Key The Event Streams Broker API Key is needed to connect any deployed consumers or producers to the service in IBM Cloud. To avoid sharing security keys, create a Kubernetes Secret in the target cluster you will deploy the application microservices to. This is available from the Service Credentials information you just created above. kubectl create secret generic eventstreams-apikey --from-literal = binding = '<replace with api key>' -n <target k8s namespace / ocp project> kubectl describe secret eventstreams-apikey -n <target k8s namespace / ocp project> Using IBM Event Streams, deployed on RedHat OpenShift Container Platform Service Deployment The installation is documented in the product documentation and in our own note here. Event Streams Kafka Brokers Regardless of specific deployment targets (OCP, IKS, k8s), the following prerequisite Kubernetes ConfigMap needs to be created to support the deployments of the application's microservices. These artifacts need to be created once per unique deployment of the entire application and can be shared between application components in the same overall application deployment. kubectl create configmap kafka-brokers --from-literal = brokers = '<replace with comma-separated list of brokers>' -n <target k8s namespace / ocp project> kubectl describe configmap kafka-brokers -n <target k8s namespace / ocp project> Event Streams API Key The Event Streams Broker API Key is needed to connect any deployed consumers or producers to the service running in your cluster. To avoid sharing security keys, create a Kubernetes Secret in the target cluster you will deploy the application microservices to. You can specify keys at the topic and consumer group levels or use a unique key for all topics and all consumer groups. kubectl create secret generic eventstreams-apikey --from-literal = binding = '<replace with api key>' -n <target k8s namespace / ocp project> kubectl describe secrets -n <target k8s namespace / ocp project> Event Streams Certificates If you are using Event Streams as your Kafka broker provider and it is deployed via the IBM Cloud Pak for Integration (ICP4I), you will need to create an additional Secret to store the generated Certificates & Truststores to connect securely between your application components and the Kafka brokers. These artifacts need to be created once per unique deployment of the entire application and can be shared between application components in the same overall application deployment. From the Connect to this cluster tab on the landing page of your Event Streams installation, download both the Java truststore and the PEM certificate . Create the Java truststore Secret: Command: oc create secret generic <secret-name> --from-file=/path/to/downloaded/file.jks Example: oc create secret generic es-truststore-jks --from-file=/Users/osowski/Downloads/es-cert.jks Create the PEM certificate Secret: Command: oc create secret generic <secret-name> --from-file=/path/to/downloaded/file.pem Example: oc create secret generic es-ca-pemfile --from-file=/Users/osowski/Downloads/es-cert.pem Using community-based Kafka Helm charts, deployed locally in-cluster If you simply want to deploy Kafka using the open source, community-supported Helm Charts, you can do so with the following commands to deploy the bitnami Kafka Stack Helm Charts . Environment Considerations As of the time of this document being authored, the Bitnami Kafka Stack Helm Chart does not expose the required security configuration elements to run optimally on OpenShift. After generating the Helm templates below, you may need to manually update the YAML files to apply the ServiceAccount you have configured for your environment to run the containers accordingly. Service Deployment Add Bitnami Helm Repository: helm repo add bitnami https://charts.bitnami.com/bitnami Update the Helm repository: helm repo update Create a Kubernetes Namespace or OpenShift Project. kubectl create namespace <target namespace> Deploy Kafka and Zookeeper using the bitnami/kafka Helm Chart: mkdir bitnami mkdir templates helm fetch --untar --untardir bitnami 'bitnami/kafka' helm template kafka --set persistence.enabled = false --set securityContext.enabled = false \\ --set zookeeper.securityContext.enabled = false \\ bitnami/kafka --namespace <target namespace> --output-dir templates ( kubectl/oc ) apply -R -f templates/ It will take a few minutes to get the pods ready. Kafka Brokers Regardless of specific deployment targets (OCP, IKS, k8s), the following prerequisite Kubernetes ConfigMap needs to be created to support the deployments of the application's microservices. These artifacts need to be created once per unique deployment of the entire application and can be shared between application components in the same overall application deployment. kubectl create configmap kafka-brokers --from-literal = brokers = '<replace with comma-separated list of brokers>' -n <target k8s namespace / ocp project> kubectl describe configmap kafka-brokers -n <target k8s namespace / ocp project> Postgresql The Container Manager microservice persists the Reefer Container inventory in a Postgresql database. The deployment of Postgresql is only necessary to support the deployment of the Container Manager microservice. If you are not deploying the Container Manager microservice, you do not need to deploy and configure a Postgresql service and database. Using Postgresql, hosted on IBM Cloud Service Deployment To install the service, follow the product documentation here . Once the service is deployed, you need to create some service credentials and retreive the following values for the different configurations: postgres.username postgres.password postgres.composed , which will need to be mapped to a JDBC URL in the format of jdbc:postgresql://<hostname>:<port>/<database-name>?sslmode=verify-full&sslfactory=org.postgresql.ssl.NonValidatingFactory (this will remove the username and password values from the default composed string) Creating Postgresql credentials as Kubernetes Secrets Applying the same approach as above, copy the Postgresql URL as defined in the Postegresql service credential and execute the following command: kubectl create secret generic postgresql-url --from-literal = binding = '<replace with postgresql-url>' -n <target k8s namespace / ocp project> For the user: kubectl create secret generic postgresql-user --from-literal = binding = 'ibm_cloud_c...' -n <target k8s namespace / ocp project> For the user password: kubectl create secret generic postgresql-pwd --from-literal = binding = '<password from the service credential>.' -n <target k8s namespace / ocp project> When running Postgresql through the IBM Cloud service, additional SSL certificates are required to communicate securely: Install the IBM Cloud Database CLI Plugin: ibmcloud plugin install cloud-databases Get the certificate using the name of the postgresql service: ibmcloud cdb deployment-cacert $IC_POSTGRES_SERV > postgresql.crt Then add it into an environment variable export POSTGRESQL_CA_PEM = \" $( cat ./postgresql.crt ) \" Then define a secret: kubectl create secret generic postgresql-ca-pem --from-literal = binding = \" $POSTGRESQL_CA_PEM \" -n browncompute Using community-based Postgresql Helm charts, deployed locally in-cluster If you simply want to deploy Postgresql using the open source, community-supported Helm Charts, you can do so with the following commands. Environment Considerations Reference Application Components Pre-reqs for details on creating the necessary ServiceAccount with required permissions, prior to deployment. Service Deployment Add Bitnami Helm Repository: helm repo add bitnami https://charts.bitnami.com/bitnami Update the Helm repository: helm repo update Create a Kubernetes Namespace or OpenShift Project (if not already created). kubectl create namespace <target namespace> Deploy Postgresql using the bitnami/postgresql Helm Chart: mkdir bitnami mkdir templates helm fetch --untar --untardir bitnami bitnami/postgresql helm template --name postgre-db --set postgresqlPassword = supersecret \\ --set persistence.enabled = false --set serviceAccount.enabled = true \\ --set serviceAccount.name = <existing service account> bitnami/postgresql \\ --namespace <target namespace> --output-dir templates ( kubectl/oc ) apply -f templates/postgresql/templates It will take a few minutes to get the pods ready. Creating Postgresql credentials as Kubernetes Secrets The postgresql-url needs to point to the in-cluster (non-headless) Kubernetes Service created as part of the deployment and should take the form of the deployment name with the suffix of -postgresql : kubectl get services | grep postgresql | grep -v headless kubectl create secret generic postgresql-url --from-literal = binding = 'jdbc:postgresql://<helm-release-name>-postgresql' -n <target k8s namespace / ocp project> For the user: kubectl create secret generic postgresql-user --from-literal = binding = 'postgres' -n <target k8s namespace / ocp project> For the user password: kubectl create secret generic postgresql-pwd --from-literal = binding = '<password used in the helm template command>.' -n <target k8s namespace / ocp project> Service Debugging & Troubleshooting Access to the in-container password can be made using the following command. This should be the same value you passed in when you deployed the service. export POSTGRES_PASSWORD = $ ( kubectl get secret --namespace <target namespace> postgre-db-postgresql -o jsonpath=\"{.data.postgresql-password}\" | base64 --decode) And then use the psql command line interface to interact with postgresql. For that, we use a Docker image as a client to the Postgresql server: kubectl run postgre - db - postgresql - client --rm --tty -i --restart='Never' --namespace <target namespace> --image bitnami/postgresql:11.3.0-debian-9-r38 --env=\"PGPASSWORD=$POSTGRES_PASSWORD\" --command -- psql --host postgre-db-postgresql -U postgres -p 5432 To connect to your database from outside the cluster execute the following commands: kubectl port - forward --namespace <target namespace> svc/postgre-db-postgresql 5432:5432 &&\\ PGPASSWORD = \"$POSTGRES_PASSWORD\" psql --host 127.0.0.1 -U postgres -p 5432","title":"Backing Services"},{"location":"deployments/backing-services/#kafka-event-streams","text":"","title":"Kafka &amp; Event Streams"},{"location":"deployments/backing-services/#using-ibm-event-streams-hosted-on-ibm-cloud","text":"","title":"Using IBM Event Streams, hosted on IBM Cloud"},{"location":"deployments/backing-services/#service-deployment","text":"To provision your service, go to the IBM Cloud Catalog and search for Event Streams . It is in the Integration category. Create the service and specify a name, a region, and a resource group. Once the service is created, you are redirected to the Event Stream Standard Dashboard: In the Manage panel add the topics needed for the solution. We need at least the following: In the Service Credentials tab, create new credentials to get the Kafka broker list, the admim URL and the api_key needed to authenticate the consumers or producers.","title":"Service Deployment"},{"location":"deployments/backing-services/#event-streams-kafka-brokers","text":"Regardless of specific deployment targets (OCP, IKS, k8s), the following prerequisite Kubernetes ConfigMap needs to be created to support the deployments of the application's microservices. These artifacts need to be created once per unique deployment of the entire application and can be shared between application components in the same overall application deployment. These values can be acquired from the kafka_brokers_sasl section of the service instance's Service Credentials. kubectl create configmap kafka-brokers --from-literal = brokers = '<replace with comma-separated list of brokers>' -n <target k8s namespace / ocp project> kubectl describe configmap kafka-brokers -n <target k8s namespace / ocp project>","title":"Event Streams Kafka Brokers"},{"location":"deployments/backing-services/#event-streams-api-key","text":"The Event Streams Broker API Key is needed to connect any deployed consumers or producers to the service in IBM Cloud. To avoid sharing security keys, create a Kubernetes Secret in the target cluster you will deploy the application microservices to. This is available from the Service Credentials information you just created above. kubectl create secret generic eventstreams-apikey --from-literal = binding = '<replace with api key>' -n <target k8s namespace / ocp project> kubectl describe secret eventstreams-apikey -n <target k8s namespace / ocp project>","title":"Event Streams API Key"},{"location":"deployments/backing-services/#using-ibm-event-streams-deployed-on-redhat-openshift-container-platform","text":"","title":"Using IBM Event Streams, deployed on RedHat OpenShift Container Platform"},{"location":"deployments/backing-services/#service-deployment_1","text":"The installation is documented in the product documentation and in our own note here.","title":"Service Deployment"},{"location":"deployments/backing-services/#event-streams-kafka-brokers_1","text":"Regardless of specific deployment targets (OCP, IKS, k8s), the following prerequisite Kubernetes ConfigMap needs to be created to support the deployments of the application's microservices. These artifacts need to be created once per unique deployment of the entire application and can be shared between application components in the same overall application deployment. kubectl create configmap kafka-brokers --from-literal = brokers = '<replace with comma-separated list of brokers>' -n <target k8s namespace / ocp project> kubectl describe configmap kafka-brokers -n <target k8s namespace / ocp project>","title":"Event Streams Kafka Brokers"},{"location":"deployments/backing-services/#event-streams-api-key_1","text":"The Event Streams Broker API Key is needed to connect any deployed consumers or producers to the service running in your cluster. To avoid sharing security keys, create a Kubernetes Secret in the target cluster you will deploy the application microservices to. You can specify keys at the topic and consumer group levels or use a unique key for all topics and all consumer groups. kubectl create secret generic eventstreams-apikey --from-literal = binding = '<replace with api key>' -n <target k8s namespace / ocp project> kubectl describe secrets -n <target k8s namespace / ocp project>","title":"Event Streams API Key"},{"location":"deployments/backing-services/#event-streams-certificates","text":"If you are using Event Streams as your Kafka broker provider and it is deployed via the IBM Cloud Pak for Integration (ICP4I), you will need to create an additional Secret to store the generated Certificates & Truststores to connect securely between your application components and the Kafka brokers. These artifacts need to be created once per unique deployment of the entire application and can be shared between application components in the same overall application deployment. From the Connect to this cluster tab on the landing page of your Event Streams installation, download both the Java truststore and the PEM certificate . Create the Java truststore Secret: Command: oc create secret generic <secret-name> --from-file=/path/to/downloaded/file.jks Example: oc create secret generic es-truststore-jks --from-file=/Users/osowski/Downloads/es-cert.jks Create the PEM certificate Secret: Command: oc create secret generic <secret-name> --from-file=/path/to/downloaded/file.pem Example: oc create secret generic es-ca-pemfile --from-file=/Users/osowski/Downloads/es-cert.pem","title":"Event Streams Certificates"},{"location":"deployments/backing-services/#using-community-based-kafka-helm-charts-deployed-locally-in-cluster","text":"If you simply want to deploy Kafka using the open source, community-supported Helm Charts, you can do so with the following commands to deploy the bitnami Kafka Stack Helm Charts .","title":"Using community-based Kafka Helm charts, deployed locally in-cluster"},{"location":"deployments/backing-services/#environment-considerations","text":"As of the time of this document being authored, the Bitnami Kafka Stack Helm Chart does not expose the required security configuration elements to run optimally on OpenShift. After generating the Helm templates below, you may need to manually update the YAML files to apply the ServiceAccount you have configured for your environment to run the containers accordingly.","title":"Environment Considerations"},{"location":"deployments/backing-services/#service-deployment_2","text":"Add Bitnami Helm Repository: helm repo add bitnami https://charts.bitnami.com/bitnami Update the Helm repository: helm repo update Create a Kubernetes Namespace or OpenShift Project. kubectl create namespace <target namespace> Deploy Kafka and Zookeeper using the bitnami/kafka Helm Chart: mkdir bitnami mkdir templates helm fetch --untar --untardir bitnami 'bitnami/kafka' helm template kafka --set persistence.enabled = false --set securityContext.enabled = false \\ --set zookeeper.securityContext.enabled = false \\ bitnami/kafka --namespace <target namespace> --output-dir templates ( kubectl/oc ) apply -R -f templates/ It will take a few minutes to get the pods ready.","title":"Service Deployment"},{"location":"deployments/backing-services/#kafka-brokers","text":"Regardless of specific deployment targets (OCP, IKS, k8s), the following prerequisite Kubernetes ConfigMap needs to be created to support the deployments of the application's microservices. These artifacts need to be created once per unique deployment of the entire application and can be shared between application components in the same overall application deployment. kubectl create configmap kafka-brokers --from-literal = brokers = '<replace with comma-separated list of brokers>' -n <target k8s namespace / ocp project> kubectl describe configmap kafka-brokers -n <target k8s namespace / ocp project>","title":"Kafka Brokers"},{"location":"deployments/backing-services/#postgresql","text":"The Container Manager microservice persists the Reefer Container inventory in a Postgresql database. The deployment of Postgresql is only necessary to support the deployment of the Container Manager microservice. If you are not deploying the Container Manager microservice, you do not need to deploy and configure a Postgresql service and database.","title":"Postgresql"},{"location":"deployments/backing-services/#using-postgresql-hosted-on-ibm-cloud","text":"","title":"Using Postgresql, hosted on IBM Cloud"},{"location":"deployments/backing-services/#service-deployment_3","text":"To install the service, follow the product documentation here . Once the service is deployed, you need to create some service credentials and retreive the following values for the different configurations: postgres.username postgres.password postgres.composed , which will need to be mapped to a JDBC URL in the format of jdbc:postgresql://<hostname>:<port>/<database-name>?sslmode=verify-full&sslfactory=org.postgresql.ssl.NonValidatingFactory (this will remove the username and password values from the default composed string)","title":"Service Deployment"},{"location":"deployments/backing-services/#creating-postgresql-credentials-as-kubernetes-secrets","text":"Applying the same approach as above, copy the Postgresql URL as defined in the Postegresql service credential and execute the following command: kubectl create secret generic postgresql-url --from-literal = binding = '<replace with postgresql-url>' -n <target k8s namespace / ocp project> For the user: kubectl create secret generic postgresql-user --from-literal = binding = 'ibm_cloud_c...' -n <target k8s namespace / ocp project> For the user password: kubectl create secret generic postgresql-pwd --from-literal = binding = '<password from the service credential>.' -n <target k8s namespace / ocp project> When running Postgresql through the IBM Cloud service, additional SSL certificates are required to communicate securely: Install the IBM Cloud Database CLI Plugin: ibmcloud plugin install cloud-databases Get the certificate using the name of the postgresql service: ibmcloud cdb deployment-cacert $IC_POSTGRES_SERV > postgresql.crt Then add it into an environment variable export POSTGRESQL_CA_PEM = \" $( cat ./postgresql.crt ) \" Then define a secret: kubectl create secret generic postgresql-ca-pem --from-literal = binding = \" $POSTGRESQL_CA_PEM \" -n browncompute","title":"Creating Postgresql credentials as Kubernetes Secrets"},{"location":"deployments/backing-services/#using-community-based-postgresql-helm-charts-deployed-locally-in-cluster","text":"If you simply want to deploy Postgresql using the open source, community-supported Helm Charts, you can do so with the following commands.","title":"Using community-based Postgresql Helm charts, deployed locally in-cluster"},{"location":"deployments/backing-services/#environment-considerations_1","text":"Reference Application Components Pre-reqs for details on creating the necessary ServiceAccount with required permissions, prior to deployment.","title":"Environment Considerations"},{"location":"deployments/backing-services/#service-deployment_4","text":"Add Bitnami Helm Repository: helm repo add bitnami https://charts.bitnami.com/bitnami Update the Helm repository: helm repo update Create a Kubernetes Namespace or OpenShift Project (if not already created). kubectl create namespace <target namespace> Deploy Postgresql using the bitnami/postgresql Helm Chart: mkdir bitnami mkdir templates helm fetch --untar --untardir bitnami bitnami/postgresql helm template --name postgre-db --set postgresqlPassword = supersecret \\ --set persistence.enabled = false --set serviceAccount.enabled = true \\ --set serviceAccount.name = <existing service account> bitnami/postgresql \\ --namespace <target namespace> --output-dir templates ( kubectl/oc ) apply -f templates/postgresql/templates It will take a few minutes to get the pods ready.","title":"Service Deployment"},{"location":"deployments/backing-services/#creating-postgresql-credentials-as-kubernetes-secrets_1","text":"The postgresql-url needs to point to the in-cluster (non-headless) Kubernetes Service created as part of the deployment and should take the form of the deployment name with the suffix of -postgresql : kubectl get services | grep postgresql | grep -v headless kubectl create secret generic postgresql-url --from-literal = binding = 'jdbc:postgresql://<helm-release-name>-postgresql' -n <target k8s namespace / ocp project> For the user: kubectl create secret generic postgresql-user --from-literal = binding = 'postgres' -n <target k8s namespace / ocp project> For the user password: kubectl create secret generic postgresql-pwd --from-literal = binding = '<password used in the helm template command>.' -n <target k8s namespace / ocp project>","title":"Creating Postgresql credentials as Kubernetes Secrets"},{"location":"deployments/backing-services/#service-debugging-troubleshooting","text":"Access to the in-container password can be made using the following command. This should be the same value you passed in when you deployed the service. export POSTGRES_PASSWORD = $ ( kubectl get secret --namespace <target namespace> postgre-db-postgresql -o jsonpath=\"{.data.postgresql-password}\" | base64 --decode) And then use the psql command line interface to interact with postgresql. For that, we use a Docker image as a client to the Postgresql server: kubectl run postgre - db - postgresql - client --rm --tty -i --restart='Never' --namespace <target namespace> --image bitnami/postgresql:11.3.0-debian-9-r38 --env=\"PGPASSWORD=$POSTGRES_PASSWORD\" --command -- psql --host postgre-db-postgresql -U postgres -p 5432 To connect to your database from outside the cluster execute the following commands: kubectl port - forward --namespace <target namespace> svc/postgre-db-postgresql 5432:5432 &&\\ PGPASSWORD = \"$POSTGRES_PASSWORD\" psql --host 127.0.0.1 -U postgres -p 5432","title":"Service Debugging &amp; Troubleshooting"},{"location":"deployments/docker/","text":"Build and run using docker-compose To build and run the solution locally using docker, please follow the below instructions. Get the app git clone https://github.com/ibm-cloud-architecture/refarch-kc.git cd refarch-kc/ ./scripts/clone.sh Setting up Kafka and Zookeeper Deploying Kafka and Zookeeper on Docker $ cd docker && docker-compose -f backbone-compose.yml up -d > & backend.logs $ ./scripts/createLocalTopics.sh Fleet ms Go to the repo $ cd refarch-kc-ms/fleet-ms Build the image $ ./scripts/buildDocker.sh Deploy on docker $ docker run -it --name fleetms -e KAFKA_BROKERS = \"<your_kafka_brokers>\" -e KAFKA_ENV = \"<LOCAL or IBMCLOUD or ICP>\" -d -p 9080 :9080 -p 9444 :9443 ibmcase/kcontainer-fleet-ms Voyage ms Go to the repo $ cd refarch-kc-ms/voyages-ms Build the image $ ./scripts/buildDocker.sh Deploy on docker $ docker run -it --name voyages -e KAFKA_BROKERS = \"<your_kafka_brokers>\" -e KAFKA_ENV = \"<LOCAL or IBMCLOUD or ICP>\" -e KAFKA_APIKEY = \"<your_kafka_api_key>\" -d -p 3100 :3000 ibmcase/kcontainer-voyages-ms Order command ms Go to the repo $ cd refarch-kc-order-ms/order-command-ms Build the image $ ./scripts/buildDocker.sh Deploy on docker $ docker run -it --name ordercmd -e KAFKA_BROKERS = \"<your_kafka_brokers>\" -e KAFKA_ENV = \"<LOCAL or IBMCLOUD or ICP>\" -e KAFKA_APIKEY = \"<your_kafka_api_key>\" -d -p 10080 :9080 ibmcase/kcontainer-order-command-ms Order query ms Go to the repo $ cd refarch-kc-order-ms/order-query-ms Build the image $ ./scripts/buildDocker.sh Deploy on docker $ docker run -it --name orderquery -e KAFKA_BROKERS = \"<your_kafka_brokers>\" -e KAFKA_ENV = \"<LOCAL or IBMCLOUD or ICP>\" -e KAFKA_APIKEY = \"<your_kafka_api_key>\" -d -p 11080 :9080 ibmcase/kcontainer-order-query-ms Container ms Go to the repo cd refarch - kc - container - ms / SpringContainerMS Build the image $ ./scripts/buildDocker.sh Deploy on docker docker run --name springcontainerms \\ --network docker_default \\ -e KAFKA_ENV= $KAFKA_ENV \\ -e KAFKA_BROKERS= $KAFKA_BROKERS \\ -e KAFKA_APIKEY= $KAFKA_APIKEY \\ -e POSTGRESQL_URL= $POSTGRESQL_URL \\ -e POSTGRESQL_CA_PEM=\" $POSTGRESQL_CA_PEM \" \\ -e POSTGRESQL_USER= $POSTGRESQL_USER \\ -e POSTGRESQL_PWD= $POSTGRESQL_PWD \\ -e TRUSTSTORE_PWD= ${ TRUSTSTORE_PWD } \\ -p 8080:8080 -ti ibmcase/kcontainer-spring-container-ms Web Go to the repo cd refarch - kc - ui / Build the image $ ./scripts/buildDocker.sh Deploy on docker docker run - it --name kcsolution -e KAFKA_BROKERS=\"<your_kafka_brokers>\" -e FLEET_MS_URL=\"<fleetms_url\" ORDER_MS_URL=\"<orderms_url>\" VOYAGE_MS_URL=\"<voyagems_url>\" --link fleetms:fleetms --link voyages:voyages --link ordercmd:ordercmd --link orderquery:orderquery --link springcontainerms:springcontainerms -d -p 3110:3010 ibmcase/kcontainer-ui","title":"Build and run using docker-compose"},{"location":"deployments/docker/#build-and-run-using-docker-compose","text":"To build and run the solution locally using docker, please follow the below instructions.","title":"Build and run using docker-compose"},{"location":"deployments/docker/#get-the-app","text":"git clone https://github.com/ibm-cloud-architecture/refarch-kc.git cd refarch-kc/ ./scripts/clone.sh","title":"Get the app"},{"location":"deployments/docker/#setting-up-kafka-and-zookeeper","text":"Deploying Kafka and Zookeeper on Docker $ cd docker && docker-compose -f backbone-compose.yml up -d > & backend.logs $ ./scripts/createLocalTopics.sh","title":"Setting up Kafka and Zookeeper"},{"location":"deployments/docker/#fleet-ms","text":"Go to the repo $ cd refarch-kc-ms/fleet-ms Build the image $ ./scripts/buildDocker.sh Deploy on docker $ docker run -it --name fleetms -e KAFKA_BROKERS = \"<your_kafka_brokers>\" -e KAFKA_ENV = \"<LOCAL or IBMCLOUD or ICP>\" -d -p 9080 :9080 -p 9444 :9443 ibmcase/kcontainer-fleet-ms","title":"Fleet ms"},{"location":"deployments/docker/#voyage-ms","text":"Go to the repo $ cd refarch-kc-ms/voyages-ms Build the image $ ./scripts/buildDocker.sh Deploy on docker $ docker run -it --name voyages -e KAFKA_BROKERS = \"<your_kafka_brokers>\" -e KAFKA_ENV = \"<LOCAL or IBMCLOUD or ICP>\" -e KAFKA_APIKEY = \"<your_kafka_api_key>\" -d -p 3100 :3000 ibmcase/kcontainer-voyages-ms","title":"Voyage ms"},{"location":"deployments/docker/#order-command-ms","text":"Go to the repo $ cd refarch-kc-order-ms/order-command-ms Build the image $ ./scripts/buildDocker.sh Deploy on docker $ docker run -it --name ordercmd -e KAFKA_BROKERS = \"<your_kafka_brokers>\" -e KAFKA_ENV = \"<LOCAL or IBMCLOUD or ICP>\" -e KAFKA_APIKEY = \"<your_kafka_api_key>\" -d -p 10080 :9080 ibmcase/kcontainer-order-command-ms","title":"Order command ms"},{"location":"deployments/docker/#order-query-ms","text":"Go to the repo $ cd refarch-kc-order-ms/order-query-ms Build the image $ ./scripts/buildDocker.sh Deploy on docker $ docker run -it --name orderquery -e KAFKA_BROKERS = \"<your_kafka_brokers>\" -e KAFKA_ENV = \"<LOCAL or IBMCLOUD or ICP>\" -e KAFKA_APIKEY = \"<your_kafka_api_key>\" -d -p 11080 :9080 ibmcase/kcontainer-order-query-ms","title":"Order query ms"},{"location":"deployments/docker/#container-ms","text":"Go to the repo cd refarch - kc - container - ms / SpringContainerMS Build the image $ ./scripts/buildDocker.sh Deploy on docker docker run --name springcontainerms \\ --network docker_default \\ -e KAFKA_ENV= $KAFKA_ENV \\ -e KAFKA_BROKERS= $KAFKA_BROKERS \\ -e KAFKA_APIKEY= $KAFKA_APIKEY \\ -e POSTGRESQL_URL= $POSTGRESQL_URL \\ -e POSTGRESQL_CA_PEM=\" $POSTGRESQL_CA_PEM \" \\ -e POSTGRESQL_USER= $POSTGRESQL_USER \\ -e POSTGRESQL_PWD= $POSTGRESQL_PWD \\ -e TRUSTSTORE_PWD= ${ TRUSTSTORE_PWD } \\ -p 8080:8080 -ti ibmcase/kcontainer-spring-container-ms","title":"Container ms"},{"location":"deployments/docker/#web","text":"Go to the repo cd refarch - kc - ui / Build the image $ ./scripts/buildDocker.sh Deploy on docker docker run - it --name kcsolution -e KAFKA_BROKERS=\"<your_kafka_brokers>\" -e FLEET_MS_URL=\"<fleetms_url\" ORDER_MS_URL=\"<orderms_url>\" VOYAGE_MS_URL=\"<voyagems_url>\" --link fleetms:fleetms --link voyages:voyages --link ordercmd:ordercmd --link orderquery:orderquery --link springcontainerms:springcontainerms -d -p 3110:3010 ibmcase/kcontainer-ui","title":"Web"},{"location":"deployments/reposlist/","text":"Related repositories This solution supports a set of related repositories which includes user interface, a set of microservices to implement the Event Sourcing and CQRS patterns, and to implement simulators and analytics content. In each repository we are explaining the design and implementation approach, how to build and run them for development purpose. The command ./scripts/clone.sh in this project clones all the dependant repositories as part of the solution. Component GitHub Repository MVP Level* Language(s) Description User Interface refarch-kc-ui 1 Angular 7 User Interface and Backend For Frontend server used for demonstration purpose Order management microservice refarch-kc-order-ms 1 Java with Liberty & MicroProfile Order management using CQRS and event sourcing pattern Shipping container management microservice refarch-kc-container-ms 2 Spring Boot Java, Python Reefer container management microservice in different programming language, and to define python tools to do machine learning on top of event store from Kafka. Fleet microservice refarch-kc-ms/fleet-ms 2 Java Simulation of a fleet of container carrier vessels Voyage microservice refarch-kc-ms/voyages-ms 2 NodeJS Supports the order management and ship voyage assignment, using Nodejs / express and kafka javascript APIs. Real time analytics refarch-kc-streams 3 Python IBM Streaming Analytics to identify problem on containers from real time events. Reefer predictive maintenance refarch-reefer-ml 3 Python Uses Reefer container metrics like power, temperature, CO2, or other sensors to build a machine learning model, deploy it as a service and run it on Event Streams. MQ to Kafka integration with a 'legacy' app refarch-container-inventory 3 Java Bi-directional connections between MQ and Kafka, using a legacy JEE app to manage the inventory for metal and Reefer containers, but only Reefer information is sent to Kafka. Kafka order producer jbcodeforce/order-producer-pythong n/a Python and Flask Quickly test an IBM Cloud Event Streams deployment. * MVP Level denotes minimal level of system capability to demostrate overall solution functionality. MVP Level 1 requires the least amount of components, MVP Level 2 requires more components, and so on.","title":"Related repositories"},{"location":"deployments/reposlist/#related-repositories","text":"This solution supports a set of related repositories which includes user interface, a set of microservices to implement the Event Sourcing and CQRS patterns, and to implement simulators and analytics content. In each repository we are explaining the design and implementation approach, how to build and run them for development purpose. The command ./scripts/clone.sh in this project clones all the dependant repositories as part of the solution. Component GitHub Repository MVP Level* Language(s) Description User Interface refarch-kc-ui 1 Angular 7 User Interface and Backend For Frontend server used for demonstration purpose Order management microservice refarch-kc-order-ms 1 Java with Liberty & MicroProfile Order management using CQRS and event sourcing pattern Shipping container management microservice refarch-kc-container-ms 2 Spring Boot Java, Python Reefer container management microservice in different programming language, and to define python tools to do machine learning on top of event store from Kafka. Fleet microservice refarch-kc-ms/fleet-ms 2 Java Simulation of a fleet of container carrier vessels Voyage microservice refarch-kc-ms/voyages-ms 2 NodeJS Supports the order management and ship voyage assignment, using Nodejs / express and kafka javascript APIs. Real time analytics refarch-kc-streams 3 Python IBM Streaming Analytics to identify problem on containers from real time events. Reefer predictive maintenance refarch-reefer-ml 3 Python Uses Reefer container metrics like power, temperature, CO2, or other sensors to build a machine learning model, deploy it as a service and run it on Event Streams. MQ to Kafka integration with a 'legacy' app refarch-container-inventory 3 Java Bi-directional connections between MQ and Kafka, using a legacy JEE app to manage the inventory for metal and Reefer containers, but only Reefer information is sent to Kafka. Kafka order producer jbcodeforce/order-producer-pythong n/a Python and Flask Quickly test an IBM Cloud Event Streams deployment. * MVP Level denotes minimal level of system capability to demostrate overall solution functionality. MVP Level 1 requires the least amount of components, MVP Level 2 requires more components, and so on.","title":"Related repositories"},{"location":"deployments/troubleshooting/","text":"Work in Progress Troubleshouting Here is a way to assess, within the kafka container, how the message arrived on a topic: $ kubectl exec -ti rolling-streams-ibm-es-kafka-sts-0 -n streams bash nobody@rolling-streams-ibm-es-kafka-sts-0 $ cd /opt/kafka/bin nobody@rolling-streams-ibm-es-kafka-sts-0:/opt/kafka/bin$ ./kafka-console-consumer.sh --bootstrap-server rolling-streams-ibm-es-kafka-broker-svc-0.streams.svc.cluster.local:9092 --topic containers --from-beginning > { \"timestamp\" : 1554338808 , \"type\" : \"ContainerAdded\" , \"version\" : \"1\" , \"containerID\" : \"c_0\" , \"payload\" : { \"containerID\" : \"c_0\" , \"type\" : \"Reefer\" , \"status\" : \"atDock\" , \"city\" : \"Oakland\" , \"brand\" : \"brand-reefer\" , \"capacity\" : 100 }} No resolvable bootstrap urls given in bootstrap.servers It is obvious reason, but when deploying a kafka consumer or producer on Kubernetes it is important to know which name to use. As the communication will be on the overlay network we should use the internal broker name. The name will be linked to the deployed configuration of Kafka or event streams. Here are a set of commands to debug that: # Get the namespace name for the deployed kafka instance $ kubectl get namespaces > ... > streams Active <x>d # Look at the name of the services of the $ kubectl get svc -n streams > rolling-streams-ibm-es-kafka-broker-svc-0 ClusterIP None <none> 9092 /TCP,8093/TCP,9094/TCP,7070/TCP 40d rolling-streams-ibm-es-kafka-broker-svc-1 ClusterIP None <none> 9092 /TCP,8093/TCP,9094/TCP,7070/TCP 40d rolling-streams-ibm-es-kafka-broker-svc-2 ClusterIP None <none> 9092 /TCP,8093/TCP,9094/TCP,7070/TCP # Verify the name lookup with busybox... deploy busybox $ kubectl apply -f https://k8s.io/examples/admin/dns/busybox.yaml $ kubectl exec -ti busybox -n default -- nslookup streams > Server: 10 .0.0.10 Address 1 : 10 .0.0.10 kube-dns.kube-system.svc.cluster.local Name: streams Address 1 : 10 .1.12.101 indexmgr.rolling-streams-ibm-es-indexmgr-svc.streams.svc.cluster.local Address 2 : 10 .1.12.102 rolling-streams-ibm-es-elas-ad8d-0.rolling-streams-ibm-es-elastic-svc.streams.svc.cluster.local Address 3 : 10 .1.31.245 rolling-streams-ibm-es-elas-ad8d-1.rolling-streams-ibm-es-elastic-svc.streams.svc.cluster.local Address 4 : 10 .1.12.97 10 -1-12-97.rolling-streams-ibm-es-kafka-broker-svc-1.streams.svc.cluster.local Address 5 : 10 .1.12.88 10 -1-12-88.rolling-streams-ibm-es-zookeeper-fixed-ip-svc-1.streams.svc.cluster.local Address 6 : 10 .1.193.198 10 -1-193-198.rolling-streams-ibm-es-zookeeper-fixed-ip-svc-2.streams.svc.cluster.local Address 7 : 10 .1.31.249 10 -1-31-249.rolling-streams-ibm-es-zookeeper-fixed-ip-svc-0.streams.svc.cluster.local Address 8 : 10 .0.48.98 rolling-streams-ibm-es-rest-svc.streams.svc.cluster.local Address 9 : 10 .0.104.180 rolling-streams-ibm-es-rest-proxy-svc.streams.svc.cluster.local Address 10 : 10 .0.9.68 rolling-streams-ibm-es-zookeeper-fixed-ip-svc-2.streams.svc.cluster.local Address 11 : 10 .1.193.253 10 -1-193-253.rolling-streams-ibm-es-kafka-broker-svc-2.streams.svc.cluster.local Address 12 : 10 .0.187.135 rolling-streams-ibm-es-zookeeper-fixed-ip-svc-0.streams.svc.cluster.local Address 13 : 10 .1.31.223 10 -1-31-223.rolling-streams-ibm-es-kafka-broker-svc-0.streams.svc.cluster.local Address 14 : 10 .0.33.136 rolling-streams-ibm-es-ui-svc.streams.svc.cluster.local Address 15 : 10 .0.182.152 rolling-streams-ibm-es-zookeeper-fixed-ip-svc-1.streams.svc.cluster.local Address 16 : 10 .0.216.177 rolling-streams-ibm-es-proxy-svc.streams.svc.cluster.local Address 17 : 10 .1.12.87 10 -1-12-87.rolling-streams-ibm-es-access-controller-svc.streams.svc.cluster.local Address 18 : 10 .1.31.251 10 -1-31-251.rolling-streams-ibm-es-access-controller-svc.streams.svc.cluster.local For other DNS troubleshooting see this product note. Failed to verify broker certificate: self signed certificate When connecting a client to a deployed Kafka using the SSL protocol, there will be a SSL handcheck protocol done. The client needs to send security credentials using public keys and root certificates. The '.pem' file can be downloaded from Event stream console as es-cert.pem . The producer or consumer code needs to specify where to get the ssl certificates: Here is will be loaded from the local folder. But this file could be mounted into the docker image running the code to a folder referenced in this ssl.ca.location . 'bootstrap.servers': KAFKA_BROKERS, 'security.protocol': 'SASL_SSL', 'ssl.ca.location': 'es-cert.pem', 'sasl.mechanisms': 'PLAIN', 'sasl.username': 'token', 'sasl.password': KAFKA_APIKEY, LTPA configuration error. Unable to create or read LTPA key file: /opt/ibm/wlp/usr/servers/defaultServer/resources/security/ltpa.keys Our how to on ICP troublshouting: https://github.com/ibm-cloud-architecture/refarch-integration/blob/master/docs/icp/troubleshooting.md","title":"Work in Progress"},{"location":"deployments/troubleshooting/#work-in-progress","text":"","title":"Work in Progress"},{"location":"deployments/troubleshooting/#troubleshouting","text":"Here is a way to assess, within the kafka container, how the message arrived on a topic: $ kubectl exec -ti rolling-streams-ibm-es-kafka-sts-0 -n streams bash nobody@rolling-streams-ibm-es-kafka-sts-0 $ cd /opt/kafka/bin nobody@rolling-streams-ibm-es-kafka-sts-0:/opt/kafka/bin$ ./kafka-console-consumer.sh --bootstrap-server rolling-streams-ibm-es-kafka-broker-svc-0.streams.svc.cluster.local:9092 --topic containers --from-beginning > { \"timestamp\" : 1554338808 , \"type\" : \"ContainerAdded\" , \"version\" : \"1\" , \"containerID\" : \"c_0\" , \"payload\" : { \"containerID\" : \"c_0\" , \"type\" : \"Reefer\" , \"status\" : \"atDock\" , \"city\" : \"Oakland\" , \"brand\" : \"brand-reefer\" , \"capacity\" : 100 }}","title":"Troubleshouting"},{"location":"deployments/troubleshooting/#no-resolvable-bootstrap-urls-given-in-bootstrapservers","text":"It is obvious reason, but when deploying a kafka consumer or producer on Kubernetes it is important to know which name to use. As the communication will be on the overlay network we should use the internal broker name. The name will be linked to the deployed configuration of Kafka or event streams. Here are a set of commands to debug that: # Get the namespace name for the deployed kafka instance $ kubectl get namespaces > ... > streams Active <x>d # Look at the name of the services of the $ kubectl get svc -n streams > rolling-streams-ibm-es-kafka-broker-svc-0 ClusterIP None <none> 9092 /TCP,8093/TCP,9094/TCP,7070/TCP 40d rolling-streams-ibm-es-kafka-broker-svc-1 ClusterIP None <none> 9092 /TCP,8093/TCP,9094/TCP,7070/TCP 40d rolling-streams-ibm-es-kafka-broker-svc-2 ClusterIP None <none> 9092 /TCP,8093/TCP,9094/TCP,7070/TCP # Verify the name lookup with busybox... deploy busybox $ kubectl apply -f https://k8s.io/examples/admin/dns/busybox.yaml $ kubectl exec -ti busybox -n default -- nslookup streams > Server: 10 .0.0.10 Address 1 : 10 .0.0.10 kube-dns.kube-system.svc.cluster.local Name: streams Address 1 : 10 .1.12.101 indexmgr.rolling-streams-ibm-es-indexmgr-svc.streams.svc.cluster.local Address 2 : 10 .1.12.102 rolling-streams-ibm-es-elas-ad8d-0.rolling-streams-ibm-es-elastic-svc.streams.svc.cluster.local Address 3 : 10 .1.31.245 rolling-streams-ibm-es-elas-ad8d-1.rolling-streams-ibm-es-elastic-svc.streams.svc.cluster.local Address 4 : 10 .1.12.97 10 -1-12-97.rolling-streams-ibm-es-kafka-broker-svc-1.streams.svc.cluster.local Address 5 : 10 .1.12.88 10 -1-12-88.rolling-streams-ibm-es-zookeeper-fixed-ip-svc-1.streams.svc.cluster.local Address 6 : 10 .1.193.198 10 -1-193-198.rolling-streams-ibm-es-zookeeper-fixed-ip-svc-2.streams.svc.cluster.local Address 7 : 10 .1.31.249 10 -1-31-249.rolling-streams-ibm-es-zookeeper-fixed-ip-svc-0.streams.svc.cluster.local Address 8 : 10 .0.48.98 rolling-streams-ibm-es-rest-svc.streams.svc.cluster.local Address 9 : 10 .0.104.180 rolling-streams-ibm-es-rest-proxy-svc.streams.svc.cluster.local Address 10 : 10 .0.9.68 rolling-streams-ibm-es-zookeeper-fixed-ip-svc-2.streams.svc.cluster.local Address 11 : 10 .1.193.253 10 -1-193-253.rolling-streams-ibm-es-kafka-broker-svc-2.streams.svc.cluster.local Address 12 : 10 .0.187.135 rolling-streams-ibm-es-zookeeper-fixed-ip-svc-0.streams.svc.cluster.local Address 13 : 10 .1.31.223 10 -1-31-223.rolling-streams-ibm-es-kafka-broker-svc-0.streams.svc.cluster.local Address 14 : 10 .0.33.136 rolling-streams-ibm-es-ui-svc.streams.svc.cluster.local Address 15 : 10 .0.182.152 rolling-streams-ibm-es-zookeeper-fixed-ip-svc-1.streams.svc.cluster.local Address 16 : 10 .0.216.177 rolling-streams-ibm-es-proxy-svc.streams.svc.cluster.local Address 17 : 10 .1.12.87 10 -1-12-87.rolling-streams-ibm-es-access-controller-svc.streams.svc.cluster.local Address 18 : 10 .1.31.251 10 -1-31-251.rolling-streams-ibm-es-access-controller-svc.streams.svc.cluster.local For other DNS troubleshooting see this product note.","title":"No resolvable bootstrap urls given in bootstrap.servers"},{"location":"deployments/troubleshooting/#failed-to-verify-broker-certificate-self-signed-certificate","text":"When connecting a client to a deployed Kafka using the SSL protocol, there will be a SSL handcheck protocol done. The client needs to send security credentials using public keys and root certificates. The '.pem' file can be downloaded from Event stream console as es-cert.pem . The producer or consumer code needs to specify where to get the ssl certificates: Here is will be loaded from the local folder. But this file could be mounted into the docker image running the code to a folder referenced in this ssl.ca.location . 'bootstrap.servers': KAFKA_BROKERS, 'security.protocol': 'SASL_SSL', 'ssl.ca.location': 'es-cert.pem', 'sasl.mechanisms': 'PLAIN', 'sasl.username': 'token', 'sasl.password': KAFKA_APIKEY,","title":"Failed to verify broker certificate: self signed certificate"},{"location":"deployments/troubleshooting/#ltpa-configuration-error-unable-to-create-or-read-ltpa-key-file-optibmwlpusrserversdefaultserverresourcessecurityltpakeys","text":"Our how to on ICP troublshouting: https://github.com/ibm-cloud-architecture/refarch-integration/blob/master/docs/icp/troubleshooting.md","title":"LTPA configuration error. Unable to create or read LTPA key file: /opt/ibm/wlp/usr/servers/defaultServer/resources/security/ltpa.keys"},{"location":"design/architecture/","text":"Architecture System Context When dealing with architecture we want to start by high level and drill down into more detail view. The system context view for the solution looks like the diagram below: Components view Deploying the different components using event-driven and microservice patterns, we may organize them as in the following figure where event backbone ensures pub/sub implementation and supports the event sourcing: Top left represents the user interface to support the demonstration of the K.Container solution, with a set of widgets to present the ships movements, the container tracking / monitoring and the event dashboards. The botton of the UI will have controls to help performaing the step by step demonstration. The event backbone is used to define a set of topics used in the solution and as event sourcing for microservice to microservice data eventual consistency support. Each service supports the top-level process with context boundary defining the microservice scope. Streaming analytics is used to process aggregates and analytics on containers and ships movement data coming in real time. As we develop by iterations the current scope of the Minimum Viable Product is only addressing the following components: Which can also be represented in a deployment view as the figure below: Read more on EDA design pattern... The 12 factors application is also used for order management microservice. Summary of microservice scopes for shipment handling: As presented in the note about event driven microservice patterns , we are using a set of event-driven design patterns to develop this solution. One of them is the sub domain decomposition . From the analysis output we have the aggregates, actors and data that are helping us to extract a set of subdomains. Fleet Service : responsibles to group the ship (container carriers), in fleet, per major ocean. Information model: Fleet has multiple ships, Ship has unique identifier (we will use its name), and a container capacity (represented as a matrix to make it simple), current position, status, voyage identifier for the voyage it is doing. Events: Ship commission, ship position, load container event, unload container event, start itinerary X, arrive at port, docked,... Operations: getFleets, get ships in a fleet, get ship by ID. CRUD Fleet and Ship. Implementation in this project. Voyages Service : define a set of voyage schedules supported by the shipping company Information model: voyageID, shipID, src_Port, planned_departure_date, dest_port, planned_arrival_dates, free_space_this_leg Events: add itinerary route, OrderAssigned Operations: CRUD on itinerary routes, query on capacity availability, assign slot to order, free slot for order. Implementation in this project. Order Service : manage the shipment order Information model: Booking id , customer, pickup location, pickup after date, deliver location, expected deliver date, order status, assigned container Events: order placed, order assigned to voyage( sets VoyageID, ship ID ), container assigned to order ( Sets container ID), Landorder, Transport associated with pickup container, Order status event, Order billing/accounting event Operations: CRUD on order, update order status Implementation in the order ms projects. Container Service : Information model: Container Id, Container temperature, container position, container condition ( maintenance goods), current associated order Events: Operations: CRUD on container Implementation in the container ms project. Customs and Export Service (not yet implemented) Land Transport Service: (not yet implemented)","title":"Architecture"},{"location":"design/architecture/#architecture","text":"","title":"Architecture"},{"location":"design/architecture/#system-context","text":"When dealing with architecture we want to start by high level and drill down into more detail view. The system context view for the solution looks like the diagram below:","title":"System Context"},{"location":"design/architecture/#components-view","text":"Deploying the different components using event-driven and microservice patterns, we may organize them as in the following figure where event backbone ensures pub/sub implementation and supports the event sourcing: Top left represents the user interface to support the demonstration of the K.Container solution, with a set of widgets to present the ships movements, the container tracking / monitoring and the event dashboards. The botton of the UI will have controls to help performaing the step by step demonstration. The event backbone is used to define a set of topics used in the solution and as event sourcing for microservice to microservice data eventual consistency support. Each service supports the top-level process with context boundary defining the microservice scope. Streaming analytics is used to process aggregates and analytics on containers and ships movement data coming in real time. As we develop by iterations the current scope of the Minimum Viable Product is only addressing the following components: Which can also be represented in a deployment view as the figure below: Read more on EDA design pattern... The 12 factors application is also used for order management microservice.","title":"Components view"},{"location":"design/architecture/#summary-of-microservice-scopes-for-shipment-handling","text":"As presented in the note about event driven microservice patterns , we are using a set of event-driven design patterns to develop this solution. One of them is the sub domain decomposition . From the analysis output we have the aggregates, actors and data that are helping us to extract a set of subdomains. Fleet Service : responsibles to group the ship (container carriers), in fleet, per major ocean. Information model: Fleet has multiple ships, Ship has unique identifier (we will use its name), and a container capacity (represented as a matrix to make it simple), current position, status, voyage identifier for the voyage it is doing. Events: Ship commission, ship position, load container event, unload container event, start itinerary X, arrive at port, docked,... Operations: getFleets, get ships in a fleet, get ship by ID. CRUD Fleet and Ship. Implementation in this project. Voyages Service : define a set of voyage schedules supported by the shipping company Information model: voyageID, shipID, src_Port, planned_departure_date, dest_port, planned_arrival_dates, free_space_this_leg Events: add itinerary route, OrderAssigned Operations: CRUD on itinerary routes, query on capacity availability, assign slot to order, free slot for order. Implementation in this project. Order Service : manage the shipment order Information model: Booking id , customer, pickup location, pickup after date, deliver location, expected deliver date, order status, assigned container Events: order placed, order assigned to voyage( sets VoyageID, ship ID ), container assigned to order ( Sets container ID), Landorder, Transport associated with pickup container, Order status event, Order billing/accounting event Operations: CRUD on order, update order status Implementation in the order ms projects. Container Service : Information model: Container Id, Container temperature, container position, container condition ( maintenance goods), current associated order Events: Operations: CRUD on container Implementation in the container ms project. Customs and Export Service (not yet implemented) Land Transport Service: (not yet implemented)","title":"Summary of microservice scopes for shipment handling:"},{"location":"design/readme/","text":"Goals and outline of section This section describes how to apply the domain driven design principles with the event driven approach to design the reefer container shipment solution. For methodology discussion we recommend that you review our summary on combining DDD with event storming end event driven microservice design, in this note. Note Since, we are delivering a demonstration application there will be some simulator / scaffolding / testing services mixed in with the required business processing. This is a common occurrence in agile development and it may be helpful to show how decision to scope and simplify a particular build and test step interacts with decisions relating strictly to microservices design. Applying the DDD and event design approach to Reefer container shipment In this section we discuss how the generic steps introduced in previous section can be applied for the Container shipping example: Context and scope for demonstration build An initial scoping decision is that the demonstration will address shipment orders and shipment progress initiated by the \"manufacturer\" of the fresh goods with the shipment company. In the context of the example there is also discussion of manufacturer and retailer reaching some agreement on the goods to be delivered but this is not part of the demonstrated capabilities. The Event Storming analysis of the shipment problem was end-to-end and involved many aggregates including: Orders, Voyages, Trucking operations both at the source (manufacturer pickup) and at the destination (retailer delivery), Customs and export interactions, Reefer Container loading into ship at source port and unloading from ship at destination port, containers and fleet of ships. To have a simple initial demonstration build showing the role of event-driven architecture and event coupled microservices, as an initial step towards development of a more complete system using agile incremental development and deployment, the initial demonstration build makes the following simplifications and scoping decisions: This build will have no implementation of: Trucking operations, Customs and export, or Dockside Handling aggregates It will show a full lifecyle for a manufacturer user to place an order for shipment, seeing a filled reefer container placed on board ship transported to the destination port and delivered. It will include a simulation service for ship movements - tracking the movement of ships carrying containers It will include simulation Container telemetrics such as temperature, current consumption while loaded. It will provide a query for a user to track an order and the current location and state of the associated shipment It will include a real time analytic solution to assess problems happening within containers 1 - Domain and subdomains During the event storming analysis, we define the domain to be the Reefer container shipping domain. It groups a set of subdomains like orders, invoice, reefer container, shipping, voyage, inventory and external systems as the vessels monitoring, CRM, invoice, and weather data: We have three core subdomains and the rest as support. Order can be delivered by an out of the shelf ordering system, but in our case we are implementing it as one or more microservices to support the demonstration of CQRS and event sourcing. The potential competitive advantage is to have a clear traceability of order -> cargo -> container -> itinerary. The order subdomain interacts with the contract subdomain via the acceptance of the contract conditions from the customer (e.g. manufacturer) and by building a contract from the created shipment order. When the contract is accepted, the order needs to be shipped, but to do so the shipping subdomain needs to interact with the voyage subsystem to get the available vessel itineraries. Voyage is an entity grouping the information about the origin harbor close to the pickup address, to a destination harbor the closest to the shipping address. Finally, the shipping subdomain needs to interact with the container inventory service to get matching Reefer containers, present at the source harbor. 2 - Applications To start addressing clear separation of concerns and assign work to different development team, we can identify some user centric applications: Booking application: customers enters the shipping order information, and review delivery tracking. Tracking query application: The Shipping clark uses the application to address some complex queries related to reefer, orders, and voyages statistics. 3 - Ubiquitous language So we identified the Reefer container, that carries fresh product as Cargo. The order is about getting fresh product from origin to destination at a given target temperature and keep the cold chain. The container will be assigned to a voyage that groups legs the ship will follow. The following figure is based on the work from the chapter 7 of Eric Evans's book with some slight differences to apply to our domain and to illustrate the above concepts: Customer may have the role of emitter or receiver of the cargo. An order can have multiple cargo (but for the current code we will use one to one relationship), and a cargo will be assigned to a Reefer container. We reuse the deliver specification object, for the advantages referenced by Eric's book, like separating the specification from the order, to keep the model easier to present, but also the specification, helps us to keep the assisgnment of Reefer and voyage (see below). The reefer container has refrigerator unit, with sensors and computer to send telemetries on a regular basis (10 to 15 minutes). Those telemetry events will be used for the real time analytics. A Voyage is defined by a set of legs going from one port to another port with expected departure and arrival dates. A voyage instance will have a specific vessel name assign to it. A vessel carries containers, and go from one port to another, following the itinerary defined in the voyage. Carrier Movement represents one particular trip by a particular Carrier (such as a truck or a ship) from one location to another A Reefer container will also have its own voyage as it may change vessels during the end to end travel. Carriers are using port as hub, and vessels doing just round trips between the same ports. Delivery history is an important information, but we will model it as events. 3.1 - Entities and Value Objects What are the object / concepts with an identity? We have the following entities: Customer : we assume the customer id is coming from the external CRM system, and the customer enter order from a web app, the shipping company control. (The UI is part of the demonstration component) Reefer : Our 'company' has already an old container management inventory system, with container identifier. Reefers are in scope of the new application, but they are part of the larger container inventory. Order : order id will be created during the operation of placing an order. Vessel : their identity is defined in external system, we are interested by their travel only. Voyage : defined by unique identity. It lists the planned port to port passages with dates and manifests for each vessel. The value objects: the different addresses the location for the ports the telemetry the cargo 3.2 - Aggregate boundaries In Domain-driven design, an aggregate groups related object as a unique entity. One object is the aggregate root. It ensures the integrity of the whole. Here the root is the Order. The product to ship is part of the aggregate. The Order is what we will persist in one unique transaction. The Reefer and Order and Voyage seem to be an obvious aggregates. Customer and Vessets are also aggregates, but we will not consider them as scope for the solution implementation. 3.3 - Repositories Each of those aggregates has its own repository. But in fact we can also foresee other reference data, like the list of locations served by the shipping company. So the repository list may look like: Customer (not in scope) Order Reefer and Container Vessel movement Voyage 3.4 - Events The event backbone will be configured with a topic for each of the above aggregated. We expect to see multiple event types on each topic, but subscriptions and sequencing of events will be within these high level topics. Command APIs will be provided to: Place a new shipment order. Track an existing order, to confirm its booking state or to resolve the actual location and status of the container in transit. Modify an order request which could not be booked within the requested time window. Create container Assign container to order A more complete and complex build could include an API for a shipping company person to optimally and manually assign orders to voyages, but for this initial demonstration we will automate this process and assign orders automatilly to the first voyage found meeting the requested requirements. Specifically, each order is assigned to the first located voyage: Going from the port nearest to pickup location To the port nearest the delivery location, Within the requested time window for pickup and delivery With available capacity for an additional container on that voyage. Additional APIs will be need: To initiate the overall demonstration To manage and view specific simulation component - container simulation and analytics and ship simulation and analytics. 4- Modules This is where this implementation differ from Eric Evans's proposed list of modules, as we apply a microservices approach, meaning basically that each aggregate with its repository are excellent candidate to be separate microservice. Order microservice For this one, we can see from the business requirements that we need to have complex tracking queries that enforce doing join between data from Voyage, Reefer, and Order, so we propose to implement CQRS for this order microservice, which means we will have at least two standalone, deployable microservices: the orders-command-ms to support the \"write model\" and the orders-query-ms for queries. We are detailing the design in the repository itself . Voyages microservice For Voyages we will need a voyages-command-ms which will maintain a list of all voyages and their current state. In any given run of the demonstration we will work with a fixed set of voyages - effectively the schedule for the container fleet - so there is no need for an API to create additional voyages. The voyage definition will be read from file when the build is initializing. We expect this voyage data to be well formed: each voyage has a vessel in the fleet allocated to make the voyage the voyages assigned to any one vessel are properly \"chained\". For the sequence of voyages assigned to any one container carrier, the destination port of the nth voyage is always the start port of the (n+1)th voyage. The states of a voyage are: SCHEDULED - in this state it can accept order bookings, knows how much free space is available for additional bookings, and knows the orderIDs of each shipment already booked on the voyage IN_PROGRESS - in this state it includes a manifest a list of the orderIDs and containerIDs on board the ship COMPLETED - a voyage in the completed state supports tracing continers, may know which containers in the voyage were spoiled etc It will be helpful for the voyage-command-ms to include a query service to lookup voyages with a particular source port and destination port in a particular time windaw. This will help process booking request event but does not need to be an external API hence there is no strong argument for realizing this as a separate CQRS query service. Reefer Containers microservice For Reefer containers we will use an agent based implementation as the main container inventory is managed by an external system. The agent listen to new reefer container added to the main inventory, and keeps a view of this Reefer inventory internally. It adds different attributes to the basic reefer container and persists metrics. Note The container inventory is in fact coded in a separate project to proof the strangler pattern and data replication using MQ and kafka or change data capture and kafka. This service maintain a list of defined containerIDs and track the state of each container. A fixed set of valid container IDs will be initialized at demonstration start time. As noted previously we will assume this to be enough for all requested orders to be assigned a container without availability issues. Since the collection of containers is fixed the component will not need a command API. The reefer container current state maintained in container-ms is: state = FREE - this container is not in use and is available to be assigned to a new shipment order state = ALLOCATED - this container is allocated to an order orderID and potentially in use for that shipment. We will be modelling and performing streaming analytics on temperature inside a (refrigerated) container. Conceptually, while a container is ALLOCATED to a shipment order with state = CONTAINER_ON_SHIP , its internal temperature, and all its sensors will be generated as event streams and process by an real time analytics component. Fleet/Ships Simulator For Ships we will have a monolithic fleet simulation service providing continuous simulation of ship position for each ship and modelling of ship events. This service will include a UI to enable viewing the positions and states of the ships. It may have a separate UI to control the overall demonstration. There is no requirement for any separate microservice maintining additional information on ship state. TODO rework next sections 6 - Microservice interactions Using the understanding of the event flow from the Event Storming session, the list of microservices and data within each microservices developed in the steps above, we can write out in a complete interaction flow. This flow illustrates how the microservices are linked together via the Event backbone using event interactions for all non API interactions between distinct microservices. Command microservice interactions - order create through voyage start with container on board The diagram below shows all command interactions from initial order creation through voyage start. The grey (shaded) columns of processing blocks are organized to show processing by the different command microservices. Column 1 shows processing by the orders-command-ms Column 2 shows processing by the voyages-command-ms Column 3 shows processing by the containers-command-ms and in a later figure by containers-streaming-ms Column 4 shows processing by the fleet/ships-simulator-ms Comments on steps in the command flow: A new shipment order request is initiated with the synchronous createOrder API at top left The orders-command-ms will create a new order record in its tale of active orders and populate it with order details. A NewOrder event is emitted on the Orders Topic. The generated orderID or the new shipment order is returned to the requester in the createOrder response. This enables the requester to query the status of an order and possibly modify the parameters of an unbooked order. The voyages-command-ms subscribes to all newOrder events on the Orders topic and tries to assign each new order to an available voyage: This operation is simplified by internally maintaining some list of vayages organized by port pair ( Starting port - ending port combination) and by time within port pair. Using such a list each voyage matching the port pair requirement of the new order can be checked or available capacity. If a voyage meeting all requirements for the new order is found, a booking event is emitted; if not, a rejected (No availability) event is emitted. A booked event causes state change in both the voyage - available capacity reduced, new order added to bookings - and to the order. We choose to make both booking and rejected (no Availabiity) events on the Orders topic rather than the Voyages topic. The orders-command-ms subscribes to Orders: booking and to Orders: Rejected (no availability) events and updates the current state of the affected order with the received information. For bookings, the current state of order is updated with the booking information including VoyageID and now specific pickup and delivery expected dates A rejected order has its state updated to rejected. This enables the requester to modify the order, suggesting different required dates or locations and trigerring a new search for a voyage meeting the modified requirements. Booked orders now have a specific schedule from the booked voyage of when they will need a container allocated for their use A Containers: needEmpty event is emitted to get a specific container allocated for use by the booked shipment The containers-command-ms subscribes to Containers: needEmpty events and allocates an available container for each one: This microservice is maintaining a list of all containers and their current states. For each incoming needEmpty event, it assigns a free container to that order and emits an Orders: allocatedContainer event specifying the containerID of the allocated container. It is very natural/necessary for the allocation of a container to be reported as an asynchronous event since this may occur at any time before the container is needed, possibly significanly later that he Containers:needEmpty event occurs We make Orders: allocatedContainer an event on the Orders topic since that is the most significant state change which it drives. The orders-command-ms subscribes to all Orders: allocatedContainer events and updates the order current state with its allocated containerID Once an order is booked on a voyage and has a container allocated for it to use, the actual physical process of shipment canbegin at this point. Since the delivery of empty container, loading it with goods at the pick up site, truck operations to get it to dockside etc are out of scope for this build, we can consider the container ready for its voyage at this point. Hence the Voyages:fullContainerReady event is emitted at this point by the orders-command-ms. This event includes the containerID of the allocated container. The voyages-command-ms subscribes to Voyages: fullContainerReady events and uses these to construct a complete manifest - the list of pairs which will travel on this voyage At this point the voyage-command-ms interacts with the fleet/ships-simulation-ms to simulate start of voyage We have shown this in the figure as a synchronous call to getNextVoyageInfo. This could also be handled with one or more event interactions The ship-simulator-ms will update the state of this ship to show the available containers and orders on board It will start the simulation of the ship moving on its course tocomplete the vogage The ship-simulator-ms willemit a Voyages: ShipStartedVoyage event The Voyages-command-ms receives this event and for each order/container in the manifest emits an Orders: ContainerOnShip event The orders-command-ms will subscribe to Orders: ContainerOnShip events and update the current state of each identified order with this information. Command microservice interaction - container on ship at sea through shipment complete The diagram below shows all command interactions from container on ship in voyage through shipment delivered and order completed. As in the previous interaction diagram, the columns with grey/shaded processing blocks show work by (1) orders-command-ms (2) voyages-command-ms (3) containers-command-ms and containers-streaming-ms (4) fleet/ships-simulator service respectively. This diagram starts with containers on board a ship which is sailing on specific voyage and is at sea. The fleet/ships-simulator-ms repeatedly simulated movement of the ship along its course It emits Ships: GPSposition events recording the position of the ship at different points in simulated time. Similarly, while the ship is at sea, the container-streams-svc is continuously simulating temperature within the container and edge monitoring to adjust controls if necessary and to report a cold chain breach in that container if it occurs. This will result in a repeated stream of Containers: tempAndGpsState events reporting the temperature, GPS coordinates and possibly power consumption of the container There could also be on or more Containers: action events to adjust or reset controls of the refrigeration unit in the container These adjustment event are initiated by predictive real-ime analytics on the container state If the temperature in the container goes out of range and there is a cold chain failure, a Containers: temperature Out of Range event is emitted After some period of simulated time tracked by these ship position and container state repeated events, the ship will be simulated as arriving at the destination port of the voyage. The ship-simulator-ms emits a Voyages: ShipEndedVoyage event The voyages-command-ms subscribes to Voyages: ShipEndedVoyage and for each such event, emits Orders: containerOffShip It can do this because the current state record for each voyage includes the manifest of pairs which travelled on that voyage the current state of the voyage is updated to COMPLETED The orders-command-ms subscribes to Orders: containerOffShip and updates the state of all orders which have completed their shipping leg as a result of completion of their booked voyage Now, since simulation of the dockside unloading, customs processes, trucking operation to support deliver are out of scope for this build, we can consider the shipment delivered at this point orders-command-ms emits Orders: containerDelivered and marks this as current state of container With the shipment delivered, there is no further need for a container to be associated with this order; orders-command-ms emits Containers: containerReleased The containers-command-ms subscribes to Containers: containerReleased and marks the current state of the identified container as FREE and available to be allocated to other shipment orders The order-command-ms considers process of the shipment order complete at this point It emits Orders: orderComplete and marks this as the current state of the order A more complete and realistic build would statr invoicing and billing event at this poitn , but this was decided to be out of scope at this point The fleet/ships-simulator-ms will continue at this point to start the next voyage in its planned itenerary and interact with voyages-command-ms to do this this is a cycled repetition of start of voyage interaction discussed previously Query microservice service - CQRS Order and Shipment tracking microservices The diagram below shows all interactions with the shipment tracking microservice. This microservice subscribes to many events carrying required information and supports one or more query APIs for different flavors of order and shipment tracking There could be multiple flavors of order and shipment tracking query APIs supported: * Order confirmation query could address orders, bookings, rejections, modified orders etc * Shipment state query could cover: container assignment, on board ship, ship position, off ship, delivery, etc * Cold chain certification query could want to augment the above with a full temperature log of the container while in transit and expect reporting on temperature range violations. Since we are using a CQRS approach, and all queries are non state changing, we could combine these multiple query levels into a single microservice or separate them out into separate microservices. If query load is intense there could be multiple instances of each such query microservices with load balancing of user requests. The design of these different flavors query services is essentially the same. The internal state data to respond to queries is obtained by subscribing to the necessary Topics. For cold chain and shipment reporting, this will involve all four topics Orders, Voyages, Containers and Ships. Internally the data will be organized by requester of the order, then by orderID, then current state and possibly summaries of repeated event history. The inteaction diagram 3 above illustrates this organization. For any order and shipment tracking query service there are synchronous APIs offered at one side and subscribed events received at the other to gather required state information from the vent backbone. Topics, event types and the event emit and consumption lists From the interaction diagrams we can compile a list of all event types which will occur in the build and check that they are organized into topics in a way which preserves all essential event sequencing. The diagram below lists the event types and topics, showing emitters ( publishers) and consumers ( subscribers) of each event type.","title":"From Analysis to Microservice Specifications"},{"location":"design/readme/#goals-and-outline-of-section","text":"This section describes how to apply the domain driven design principles with the event driven approach to design the reefer container shipment solution. For methodology discussion we recommend that you review our summary on combining DDD with event storming end event driven microservice design, in this note. Note Since, we are delivering a demonstration application there will be some simulator / scaffolding / testing services mixed in with the required business processing. This is a common occurrence in agile development and it may be helpful to show how decision to scope and simplify a particular build and test step interacts with decisions relating strictly to microservices design.","title":"Goals and outline of section"},{"location":"design/readme/#applying-the-ddd-and-event-design-approach-to-reefer-container-shipment","text":"In this section we discuss how the generic steps introduced in previous section can be applied for the Container shipping example:","title":"Applying the DDD and event design approach to Reefer container shipment"},{"location":"design/readme/#context-and-scope-for-demonstration-build","text":"An initial scoping decision is that the demonstration will address shipment orders and shipment progress initiated by the \"manufacturer\" of the fresh goods with the shipment company. In the context of the example there is also discussion of manufacturer and retailer reaching some agreement on the goods to be delivered but this is not part of the demonstrated capabilities. The Event Storming analysis of the shipment problem was end-to-end and involved many aggregates including: Orders, Voyages, Trucking operations both at the source (manufacturer pickup) and at the destination (retailer delivery), Customs and export interactions, Reefer Container loading into ship at source port and unloading from ship at destination port, containers and fleet of ships. To have a simple initial demonstration build showing the role of event-driven architecture and event coupled microservices, as an initial step towards development of a more complete system using agile incremental development and deployment, the initial demonstration build makes the following simplifications and scoping decisions: This build will have no implementation of: Trucking operations, Customs and export, or Dockside Handling aggregates It will show a full lifecyle for a manufacturer user to place an order for shipment, seeing a filled reefer container placed on board ship transported to the destination port and delivered. It will include a simulation service for ship movements - tracking the movement of ships carrying containers It will include simulation Container telemetrics such as temperature, current consumption while loaded. It will provide a query for a user to track an order and the current location and state of the associated shipment It will include a real time analytic solution to assess problems happening within containers","title":"Context and scope for demonstration build"},{"location":"design/readme/#1-domain-and-subdomains","text":"During the event storming analysis, we define the domain to be the Reefer container shipping domain. It groups a set of subdomains like orders, invoice, reefer container, shipping, voyage, inventory and external systems as the vessels monitoring, CRM, invoice, and weather data: We have three core subdomains and the rest as support. Order can be delivered by an out of the shelf ordering system, but in our case we are implementing it as one or more microservices to support the demonstration of CQRS and event sourcing. The potential competitive advantage is to have a clear traceability of order -> cargo -> container -> itinerary. The order subdomain interacts with the contract subdomain via the acceptance of the contract conditions from the customer (e.g. manufacturer) and by building a contract from the created shipment order. When the contract is accepted, the order needs to be shipped, but to do so the shipping subdomain needs to interact with the voyage subsystem to get the available vessel itineraries. Voyage is an entity grouping the information about the origin harbor close to the pickup address, to a destination harbor the closest to the shipping address. Finally, the shipping subdomain needs to interact with the container inventory service to get matching Reefer containers, present at the source harbor.","title":"1 - Domain and subdomains"},{"location":"design/readme/#2-applications","text":"To start addressing clear separation of concerns and assign work to different development team, we can identify some user centric applications: Booking application: customers enters the shipping order information, and review delivery tracking. Tracking query application: The Shipping clark uses the application to address some complex queries related to reefer, orders, and voyages statistics.","title":"2 -  Applications"},{"location":"design/readme/#3-ubiquitous-language","text":"So we identified the Reefer container, that carries fresh product as Cargo. The order is about getting fresh product from origin to destination at a given target temperature and keep the cold chain. The container will be assigned to a voyage that groups legs the ship will follow. The following figure is based on the work from the chapter 7 of Eric Evans's book with some slight differences to apply to our domain and to illustrate the above concepts: Customer may have the role of emitter or receiver of the cargo. An order can have multiple cargo (but for the current code we will use one to one relationship), and a cargo will be assigned to a Reefer container. We reuse the deliver specification object, for the advantages referenced by Eric's book, like separating the specification from the order, to keep the model easier to present, but also the specification, helps us to keep the assisgnment of Reefer and voyage (see below). The reefer container has refrigerator unit, with sensors and computer to send telemetries on a regular basis (10 to 15 minutes). Those telemetry events will be used for the real time analytics. A Voyage is defined by a set of legs going from one port to another port with expected departure and arrival dates. A voyage instance will have a specific vessel name assign to it. A vessel carries containers, and go from one port to another, following the itinerary defined in the voyage. Carrier Movement represents one particular trip by a particular Carrier (such as a truck or a ship) from one location to another A Reefer container will also have its own voyage as it may change vessels during the end to end travel. Carriers are using port as hub, and vessels doing just round trips between the same ports. Delivery history is an important information, but we will model it as events.","title":"3 - Ubiquitous language"},{"location":"design/readme/#31-entities-and-value-objects","text":"What are the object / concepts with an identity? We have the following entities: Customer : we assume the customer id is coming from the external CRM system, and the customer enter order from a web app, the shipping company control. (The UI is part of the demonstration component) Reefer : Our 'company' has already an old container management inventory system, with container identifier. Reefers are in scope of the new application, but they are part of the larger container inventory. Order : order id will be created during the operation of placing an order. Vessel : their identity is defined in external system, we are interested by their travel only. Voyage : defined by unique identity. It lists the planned port to port passages with dates and manifests for each vessel. The value objects: the different addresses the location for the ports the telemetry the cargo","title":"3.1 - Entities and Value Objects"},{"location":"design/readme/#32-aggregate-boundaries","text":"In Domain-driven design, an aggregate groups related object as a unique entity. One object is the aggregate root. It ensures the integrity of the whole. Here the root is the Order. The product to ship is part of the aggregate. The Order is what we will persist in one unique transaction. The Reefer and Order and Voyage seem to be an obvious aggregates. Customer and Vessets are also aggregates, but we will not consider them as scope for the solution implementation.","title":"3.2 - Aggregate boundaries"},{"location":"design/readme/#33-repositories","text":"Each of those aggregates has its own repository. But in fact we can also foresee other reference data, like the list of locations served by the shipping company. So the repository list may look like: Customer (not in scope) Order Reefer and Container Vessel movement Voyage","title":"3.3 - Repositories"},{"location":"design/readme/#34-events","text":"The event backbone will be configured with a topic for each of the above aggregated. We expect to see multiple event types on each topic, but subscriptions and sequencing of events will be within these high level topics. Command APIs will be provided to: Place a new shipment order. Track an existing order, to confirm its booking state or to resolve the actual location and status of the container in transit. Modify an order request which could not be booked within the requested time window. Create container Assign container to order A more complete and complex build could include an API for a shipping company person to optimally and manually assign orders to voyages, but for this initial demonstration we will automate this process and assign orders automatilly to the first voyage found meeting the requested requirements. Specifically, each order is assigned to the first located voyage: Going from the port nearest to pickup location To the port nearest the delivery location, Within the requested time window for pickup and delivery With available capacity for an additional container on that voyage. Additional APIs will be need: To initiate the overall demonstration To manage and view specific simulation component - container simulation and analytics and ship simulation and analytics.","title":"3.4 - Events"},{"location":"design/readme/#4-modules","text":"This is where this implementation differ from Eric Evans's proposed list of modules, as we apply a microservices approach, meaning basically that each aggregate with its repository are excellent candidate to be separate microservice.","title":"4- Modules"},{"location":"design/readme/#order-microservice","text":"For this one, we can see from the business requirements that we need to have complex tracking queries that enforce doing join between data from Voyage, Reefer, and Order, so we propose to implement CQRS for this order microservice, which means we will have at least two standalone, deployable microservices: the orders-command-ms to support the \"write model\" and the orders-query-ms for queries. We are detailing the design in the repository itself .","title":"Order microservice"},{"location":"design/readme/#voyages-microservice","text":"For Voyages we will need a voyages-command-ms which will maintain a list of all voyages and their current state. In any given run of the demonstration we will work with a fixed set of voyages - effectively the schedule for the container fleet - so there is no need for an API to create additional voyages. The voyage definition will be read from file when the build is initializing. We expect this voyage data to be well formed: each voyage has a vessel in the fleet allocated to make the voyage the voyages assigned to any one vessel are properly \"chained\". For the sequence of voyages assigned to any one container carrier, the destination port of the nth voyage is always the start port of the (n+1)th voyage. The states of a voyage are: SCHEDULED - in this state it can accept order bookings, knows how much free space is available for additional bookings, and knows the orderIDs of each shipment already booked on the voyage IN_PROGRESS - in this state it includes a manifest a list of the orderIDs and containerIDs on board the ship COMPLETED - a voyage in the completed state supports tracing continers, may know which containers in the voyage were spoiled etc It will be helpful for the voyage-command-ms to include a query service to lookup voyages with a particular source port and destination port in a particular time windaw. This will help process booking request event but does not need to be an external API hence there is no strong argument for realizing this as a separate CQRS query service.","title":"Voyages microservice"},{"location":"design/readme/#reefer-containers-microservice","text":"For Reefer containers we will use an agent based implementation as the main container inventory is managed by an external system. The agent listen to new reefer container added to the main inventory, and keeps a view of this Reefer inventory internally. It adds different attributes to the basic reefer container and persists metrics. Note The container inventory is in fact coded in a separate project to proof the strangler pattern and data replication using MQ and kafka or change data capture and kafka. This service maintain a list of defined containerIDs and track the state of each container. A fixed set of valid container IDs will be initialized at demonstration start time. As noted previously we will assume this to be enough for all requested orders to be assigned a container without availability issues. Since the collection of containers is fixed the component will not need a command API. The reefer container current state maintained in container-ms is: state = FREE - this container is not in use and is available to be assigned to a new shipment order state = ALLOCATED - this container is allocated to an order orderID and potentially in use for that shipment. We will be modelling and performing streaming analytics on temperature inside a (refrigerated) container. Conceptually, while a container is ALLOCATED to a shipment order with state = CONTAINER_ON_SHIP , its internal temperature, and all its sensors will be generated as event streams and process by an real time analytics component.","title":"Reefer Containers microservice"},{"location":"design/readme/#fleetships-simulator","text":"For Ships we will have a monolithic fleet simulation service providing continuous simulation of ship position for each ship and modelling of ship events. This service will include a UI to enable viewing the positions and states of the ships. It may have a separate UI to control the overall demonstration. There is no requirement for any separate microservice maintining additional information on ship state. TODO rework next sections","title":"Fleet/Ships Simulator"},{"location":"design/readme/#6-microservice-interactions","text":"Using the understanding of the event flow from the Event Storming session, the list of microservices and data within each microservices developed in the steps above, we can write out in a complete interaction flow. This flow illustrates how the microservices are linked together via the Event backbone using event interactions for all non API interactions between distinct microservices.","title":"6 - Microservice interactions"},{"location":"design/readme/#command-microservice-interactions-order-create-through-voyage-start-with-container-on-board","text":"The diagram below shows all command interactions from initial order creation through voyage start. The grey (shaded) columns of processing blocks are organized to show processing by the different command microservices. Column 1 shows processing by the orders-command-ms Column 2 shows processing by the voyages-command-ms Column 3 shows processing by the containers-command-ms and in a later figure by containers-streaming-ms Column 4 shows processing by the fleet/ships-simulator-ms Comments on steps in the command flow: A new shipment order request is initiated with the synchronous createOrder API at top left The orders-command-ms will create a new order record in its tale of active orders and populate it with order details. A NewOrder event is emitted on the Orders Topic. The generated orderID or the new shipment order is returned to the requester in the createOrder response. This enables the requester to query the status of an order and possibly modify the parameters of an unbooked order. The voyages-command-ms subscribes to all newOrder events on the Orders topic and tries to assign each new order to an available voyage: This operation is simplified by internally maintaining some list of vayages organized by port pair ( Starting port - ending port combination) and by time within port pair. Using such a list each voyage matching the port pair requirement of the new order can be checked or available capacity. If a voyage meeting all requirements for the new order is found, a booking event is emitted; if not, a rejected (No availability) event is emitted. A booked event causes state change in both the voyage - available capacity reduced, new order added to bookings - and to the order. We choose to make both booking and rejected (no Availabiity) events on the Orders topic rather than the Voyages topic. The orders-command-ms subscribes to Orders: booking and to Orders: Rejected (no availability) events and updates the current state of the affected order with the received information. For bookings, the current state of order is updated with the booking information including VoyageID and now specific pickup and delivery expected dates A rejected order has its state updated to rejected. This enables the requester to modify the order, suggesting different required dates or locations and trigerring a new search for a voyage meeting the modified requirements. Booked orders now have a specific schedule from the booked voyage of when they will need a container allocated for their use A Containers: needEmpty event is emitted to get a specific container allocated for use by the booked shipment The containers-command-ms subscribes to Containers: needEmpty events and allocates an available container for each one: This microservice is maintaining a list of all containers and their current states. For each incoming needEmpty event, it assigns a free container to that order and emits an Orders: allocatedContainer event specifying the containerID of the allocated container. It is very natural/necessary for the allocation of a container to be reported as an asynchronous event since this may occur at any time before the container is needed, possibly significanly later that he Containers:needEmpty event occurs We make Orders: allocatedContainer an event on the Orders topic since that is the most significant state change which it drives. The orders-command-ms subscribes to all Orders: allocatedContainer events and updates the order current state with its allocated containerID Once an order is booked on a voyage and has a container allocated for it to use, the actual physical process of shipment canbegin at this point. Since the delivery of empty container, loading it with goods at the pick up site, truck operations to get it to dockside etc are out of scope for this build, we can consider the container ready for its voyage at this point. Hence the Voyages:fullContainerReady event is emitted at this point by the orders-command-ms. This event includes the containerID of the allocated container. The voyages-command-ms subscribes to Voyages: fullContainerReady events and uses these to construct a complete manifest - the list of pairs which will travel on this voyage At this point the voyage-command-ms interacts with the fleet/ships-simulation-ms to simulate start of voyage We have shown this in the figure as a synchronous call to getNextVoyageInfo. This could also be handled with one or more event interactions The ship-simulator-ms will update the state of this ship to show the available containers and orders on board It will start the simulation of the ship moving on its course tocomplete the vogage The ship-simulator-ms willemit a Voyages: ShipStartedVoyage event The Voyages-command-ms receives this event and for each order/container in the manifest emits an Orders: ContainerOnShip event The orders-command-ms will subscribe to Orders: ContainerOnShip events and update the current state of each identified order with this information.","title":"Command microservice interactions - order create through voyage start with container on board"},{"location":"design/readme/#command-microservice-interaction-container-on-ship-at-sea-through-shipment-complete","text":"The diagram below shows all command interactions from container on ship in voyage through shipment delivered and order completed. As in the previous interaction diagram, the columns with grey/shaded processing blocks show work by (1) orders-command-ms (2) voyages-command-ms (3) containers-command-ms and containers-streaming-ms (4) fleet/ships-simulator service respectively. This diagram starts with containers on board a ship which is sailing on specific voyage and is at sea. The fleet/ships-simulator-ms repeatedly simulated movement of the ship along its course It emits Ships: GPSposition events recording the position of the ship at different points in simulated time. Similarly, while the ship is at sea, the container-streams-svc is continuously simulating temperature within the container and edge monitoring to adjust controls if necessary and to report a cold chain breach in that container if it occurs. This will result in a repeated stream of Containers: tempAndGpsState events reporting the temperature, GPS coordinates and possibly power consumption of the container There could also be on or more Containers: action events to adjust or reset controls of the refrigeration unit in the container These adjustment event are initiated by predictive real-ime analytics on the container state If the temperature in the container goes out of range and there is a cold chain failure, a Containers: temperature Out of Range event is emitted After some period of simulated time tracked by these ship position and container state repeated events, the ship will be simulated as arriving at the destination port of the voyage. The ship-simulator-ms emits a Voyages: ShipEndedVoyage event The voyages-command-ms subscribes to Voyages: ShipEndedVoyage and for each such event, emits Orders: containerOffShip It can do this because the current state record for each voyage includes the manifest of pairs which travelled on that voyage the current state of the voyage is updated to COMPLETED The orders-command-ms subscribes to Orders: containerOffShip and updates the state of all orders which have completed their shipping leg as a result of completion of their booked voyage Now, since simulation of the dockside unloading, customs processes, trucking operation to support deliver are out of scope for this build, we can consider the shipment delivered at this point orders-command-ms emits Orders: containerDelivered and marks this as current state of container With the shipment delivered, there is no further need for a container to be associated with this order; orders-command-ms emits Containers: containerReleased The containers-command-ms subscribes to Containers: containerReleased and marks the current state of the identified container as FREE and available to be allocated to other shipment orders The order-command-ms considers process of the shipment order complete at this point It emits Orders: orderComplete and marks this as the current state of the order A more complete and realistic build would statr invoicing and billing event at this poitn , but this was decided to be out of scope at this point The fleet/ships-simulator-ms will continue at this point to start the next voyage in its planned itenerary and interact with voyages-command-ms to do this this is a cycled repetition of start of voyage interaction discussed previously","title":"Command microservice interaction - container on ship at sea through shipment complete"},{"location":"design/readme/#query-microservice-service-cqrs-order-and-shipment-tracking-microservices","text":"The diagram below shows all interactions with the shipment tracking microservice. This microservice subscribes to many events carrying required information and supports one or more query APIs for different flavors of order and shipment tracking There could be multiple flavors of order and shipment tracking query APIs supported: * Order confirmation query could address orders, bookings, rejections, modified orders etc * Shipment state query could cover: container assignment, on board ship, ship position, off ship, delivery, etc * Cold chain certification query could want to augment the above with a full temperature log of the container while in transit and expect reporting on temperature range violations. Since we are using a CQRS approach, and all queries are non state changing, we could combine these multiple query levels into a single microservice or separate them out into separate microservices. If query load is intense there could be multiple instances of each such query microservices with load balancing of user requests. The design of these different flavors query services is essentially the same. The internal state data to respond to queries is obtained by subscribing to the necessary Topics. For cold chain and shipment reporting, this will involve all four topics Orders, Voyages, Containers and Ships. Internally the data will be organized by requester of the order, then by orderID, then current state and possibly summaries of repeated event history. The inteaction diagram 3 above illustrates this organization. For any order and shipment tracking query service there are synchronous APIs offered at one side and subscribed events received at the other to gather required state information from the vent backbone.","title":"Query microservice service  - CQRS Order and  Shipment tracking microservices"},{"location":"design/readme/#topics-event-types-and-the-event-emit-and-consumption-lists","text":"From the interaction diagrams we can compile a list of all event types which will occur in the build and check that they are organized into topics in a way which preserves all essential event sequencing. The diagram below lists the event types and topics, showing emitters ( publishers) and consumers ( subscribers) of each event type.","title":"Topics, event types and the event emit and consumption lists"},{"location":"itg-tests/","text":"Reefer container shipment solution integration tests The itg-tests folder includes a set of tests to validate most of the event-driven microservice patterns like, event sourcing with fail over, CQRS and Saga patterns with recovery and fail over. (See our summary on those patterns here ) These integration tests are done in Python to illustrate how to use Kafka python module of this github and because Python is nice to use for writing integration tests. pre-requisites Building the python environment as docker image To avoid impacting your environment we use a dockerfile to get the basic of python 3.7.x and other needed modules like kafka, http requests, pytest... So build your python image with all the needed libraries, use the following commands: cd docker docker build -t ibmcase/python . Ensure all services are running Note This documentation assumes the solution is running within MINIKUBE or docker compose, but tests will work the same with docker-compose, just replace MINIKUBE with LOCAL as argument of the scripts. Integration tests run also with Event Streams on Cloud and the microservices deployed on kubernetes or openshift. Be sure to be logged into the kubernetes cluster. We are using the greencompute namespace. kubectl get pods - n greencompute NAME READY STATUS RESTARTS AGE fleetms - deployment - f85cb679d - 582 pp 1 / 1 Running 2 14 d kafkabitmani - 0 1 / 1 Running 1 3 d23h kafkabitmani - zookeeper - 0 1 / 1 Running 0 3 d23h kcsolution - kc - ui - 76 b7b4fccf - z85j5 1 / 1 Running 1 8 d ordercommandms - deployment - 857865854 f - 7 mzqs 1 / 1 Running 0 3 d21h orderqueryms - deployment - 778 d79d99c - lrgfs 1 / 1 Running 0 3 d21h postgre - db - postgresql - 0 1 / 1 Running 2 14 d Run the python environment With the image ibmcase/python , you will be able to run the different integration tests. For example, the following commands will start a bash shell with the python environment, mounting the local filesystem into the docker /home folder, and connect to the same network as the Kafka broker and the other solution components are running into: $ pwd itg-tests $ ./startPythonEnv.sh MINIKUBE ( or use LOCAL ) root@fe61560d0cc4:/# From this shell, first specify where python should find the new modules, by setting the environment variable PYTHONPATH : root @fe61560d0cc4 : / # export PYTHONPATH =/ home root @fe61560d0cc4 : / # cd / home As the startPythonEnv is mounting the local itg-tests folder to the /home folder inside the docker container, we can access all the integration tests, and execute them ... How to proof the event sourcing The goal of this test is to illustrate the happy path for event sourcing: all events for an order, are persisted in kafka, in the order of arrival, and a consumer with no offset commit, could run and help to answer to the question: What happened to the orderId 75? We want to validate the order events are sequential over time, and it is possible to replay the loading of events from time origin. Replay all events for a given key First start the python docker container, so we can execute any python code. The script connects to the docker network where kafka runs. cd itg_tests . / startPythonEnv . sh MINIKUBE In the bash session, use the following command to ensure python knows how to get our new defined modules like the kafka consumer and producer: export PYTHONPATH =/ home cd / home / es - it Start python interpreter with the producer events code with orderID set to 75. $ python ProducerOrderEvents . py 75 The arguments are : [ ' ProducerOrderEvents.py ' , ' 75 ' ] Generate events for 75 Create order for 75 1 - CreateOrder :{ \" orderID \" : \" 75 \" , \" timestamp \" : 1555149614 , \" type \" : \" OrderCreated \" , \" payload \" : { \" orderID \" : \" 75 \" , \" productID \" : \" FreshFoodItg \" , \" customerID \" : \" Customer007 \" , \" quantity \" : 180 , \" pickupAddress \" : { \" street \" : \" astreet \" , \" city \" : \" Oakland \" , \" country \" : \" USA \" , \" state \" : \" CA \" , \" zipcode \" : \" 95000 \" }, \" destinationAddress \" : { \" street \" : \" bstreet \" , \" city \" : \" Beijing \" , \" country \" : \" China \" , \" state \" : \" NE \" , \" zipcode \" : \" 09000 \" }, \" pickupDate \" : \" 2019-05-25 \" , \" expectedDeliveryDate \" : \" 2019-06-25 \" }} { ' bootstrap.servers ' : ' kafka1:9092 ' , ' group.id ' : ' OrderProducerPython ' } Message delivered to orders [ 0 ] 2 - Producer accept the offer so now the order is booked :{ \" orderID \" : \" 75 \" , \" timestamp \" : 1555317000 , \" type \" : \" OrderBooked \" , \" payload \" : { \" orderID \" : \" 75 \" , \" productID \" : \" FreshFoodItg \" , \" customerID \" : \" Customer007 \" , \" quantity \" : 180 , \" pickupAddress \" : { \" street \" : \" astreet \" , \" city \" : \" Oakland \" , \" country \" : \" USA \" , \" state \" : \" CA \" , \" zipcode \" : \" 95000 \" }, \" destinationAddress \" : { \" street \" : \" bstreet \" , \" city \" : \" Beijing \" , \" country \" : \" China \" , \" state \" : \" NE \" , \" zipcode \" : \" 09000 \" }, \" pickupDate \" : \" 2019-05-25 \" , \" expectedDeliveryDate \" : \" 2019-06-25 \" }} Message delivered to orders [ 0 ] 3 - Voyage is assigned to order :{ \" orderID \" : \" 75 \" , \" timestamp \" : 1555317300 , \" type \" : \" OrderAssigned \" , \" payload \" : { \" orderID \" : \" 75 \" , \" voyageID \" : \" voyage21 \" }} Message delivered to orders [ 0 ] 4 - Allocate Reefer to order :{ \" orderID \" : \" 75 \" , \" timestamp \" : 1555405800 , \" type \" : \" ContainerAllocated \" , \" payload \" : { \" orderID \" : \" 75 \" , \" containerID \" : \" c13 \" }} Message delivered to orders [ 0 ] 5 - Reefer loaded with goods ready for Voyage :{ \" orderID \" : \" 75 \" , \" timestamp \" : 1557930600 , \" type \" : \" FullContainerVoyageReady \" , \" payload \" : { \" orderID \" : \" 75 \" , \" containerID \" : \" c13 \" }} Message delivered to orders [ 0 ] Now we can start the consumer to see the event coming with different time stamps. In a separate terminal windows run the command: . / runOrderConsumer . sh MINIKUBE 75 You should get the following results @@@ pollNextOrder orders partition : [ 0 ] at offset 8 with key b '75' : value : { \"orderID\" : \"75\" , \"timestamp\" : 1555149614 , \"type\" : \"OrderCreated\" , \"payload\" : { \"orderID\" : \"75\" , \"productID\" : \"FreshFoodItg\" , \"customerID\" : \"Customer007\" , \"quantity\" : 180 , \"pickupAddress\" : { \"street\" : \"astreet\" , \"city\" : \"Oakland\" , \"country\" : \"USA\" , \"state\" : \"CA\" , \"zipcode\" : \"95000\" } , \"destinationAddress\" : { \"street\" : \"bstreet\" , \"city\" : \"Beijing\" , \"country\" : \"China\" , \"state\" : \"NE\" , \"zipcode\" : \"09000\" } , \"pickupDate\" : \"2019-05-25\" , \"expectedDeliveryDate\" : \"2019-06-25\" }} @@@ pollNextOrder orders partition : [ 0 ] at offset 9 with key b '75' : value : { \"orderID\" : \"75\" , \"timestamp\" : 1555317000 , \"type\" : \"OrderBooked\" , \"payload\" : { \"orderID\" : \"75\" , \"productID\" : \"FreshFoodItg\" , \"customerID\" : \"Customer007\" , \"quantity\" : 180 , \"pickupAddress\" : { \"street\" : \"astreet\" , \"city\" : \"Oakland\" , \"country\" : \"USA\" , \"state\" : \"CA\" , \"zipcode\" : \"95000\" } , \"destinationAddress\" : { \"street\" : \"bstreet\" , \"city\" : \"Beijing\" , \"country\" : \"China\" , \"state\" : \"NE\" , \"zipcode\" : \"09000\" } , \"pickupDate\" : \"2019-05-25\" , \"expectedDeliveryDate\" : \"2019-06-25\" }} @@@ pollNextOrder orders partition : [ 0 ] at offset 10 with key b '75' : value : { \"orderID\" : \"75\" , \"timestamp\" : 1555317300 , \"type\" : \"OrderAssigned\" , \"payload\" : { \"orderID\" : \"75\" , \"voyageID\" : \"voyage21\" }} @@@ pollNextOrder orders partition : [ 0 ] at offset 11 with key b '75' : value : { \"orderID\" : \"75\" , \"timestamp\" : 1555405800 , \"type\" : \"ContainerAllocated\" , \"payload\" : { \"orderID\" : \"75\" , \"containerID\" : \"c13\" }} @@@ pollNextOrder orders partition : [ 0 ] at offset 12 with key b '75' : value : { \"orderID\" : \"75\" , \"timestamp\" : 1557930600 , \"type\" : \"FullContainerVoyageReady\" , \"payload\" : { \"orderID\" : \"75\" , \"containerID\" : \"c13\" }} Stopping with Ctrl-C (it can take time for python to get the keyboard interrupt) and then restarting the same consumer will bring you the same content. We can always answer the question at different time, but still get the same answer. The tests are under the itg-tests/es-it folder. The testin es-it/ProducerOrderEvents.py create the order events, and when combined with an order consumer give you the tracing of the process. The diagram below illustrates this simple environment. EventSourcingTests.py uses the event backbone, and the order microservices. To run the tests set the KAFKA_BROKERS and KAFKA_APIKEY environment variables Happy path for the order life cycle: Validating CQRS A classical happy path for a event-driven microservice is to receive a command to do something (could be an API operation (POST a new order)), validate the command, create a new event (OrderCreated) and append it to event log, then update its internal state, and may be run side effect like returning a response to the command initiator, or trigger a call to an external service to initiate a business process or a business task. We assume the python image was built, so you can use the following command to run the first test that creates an order via HTTP POST on the Order Command microservice (the write model), uses Kafka consumer to get the different events and then use HTTP GET to query the Order query microservices (the read model). The OrdersPython/ValidateOrderCreation.py code is documented and should be self-explanatory. The order state diagram is presented here. $ pwd itg-tests $ ./startPythonEnv.sh LOCAL ( or MINIKUBE ) root@fe61560d0cc4:/# cd home/OrdersPython root@fe61560d0cc4:/# python ValidateOrderCreation.py The implementation uses a set of constructs to poll the orders topic for events and timing out if not events are there. Failure on query service One possible bad path, is when the order query microservice fails, and needs to recover from failure. In this case it should reload the events from last commited offset and update its internal states without running any code that could lead to side effect. To test this path, we first manual kill the docker container running the order query microservice, then use the same code as above to create an order. The last step of the call fails as the order created event was not processed by the query microservice, the data was not uploaded to its own projection. Restarting the microservice, and going to the URL http://localhost:11080/orders/byManuf/FishFarm, you will see the order is now part of the data of this service. How to proof the SAGA pattern We want to validate the SAGA pattern to support the long-running, cross microservices, order transactions. The diagram is illustrating the use case we want to proof and tests: What we need to proof for the happy path: Send a new order to the order microservice via API with all the data to ship fresh goods from two countries separated by ocean verify the status of the order is pending The unique cross services key is the order ID verify orderCreated event was published verify voyage was allocated to order verify container was allocated to order verify ship has new containers added to its plan shipping plan verify the status of the order is assigned Business exeception error: no container for this type of load is available in this time frame. So the business response will be to keep the order in pending but trigger a business process for a customer representative to be in contact with the manufacturer for a remediation plan. Verify the response to the container service is an OrderUnfulled event.","title":"Reefer container shipment solution integration tests"},{"location":"itg-tests/#reefer-container-shipment-solution-integration-tests","text":"The itg-tests folder includes a set of tests to validate most of the event-driven microservice patterns like, event sourcing with fail over, CQRS and Saga patterns with recovery and fail over. (See our summary on those patterns here ) These integration tests are done in Python to illustrate how to use Kafka python module of this github and because Python is nice to use for writing integration tests.","title":"Reefer container shipment solution integration tests"},{"location":"itg-tests/#pre-requisites","text":"","title":"pre-requisites"},{"location":"itg-tests/#building-the-python-environment-as-docker-image","text":"To avoid impacting your environment we use a dockerfile to get the basic of python 3.7.x and other needed modules like kafka, http requests, pytest... So build your python image with all the needed libraries, use the following commands: cd docker docker build -t ibmcase/python .","title":"Building the python environment as docker image"},{"location":"itg-tests/#ensure-all-services-are-running","text":"Note This documentation assumes the solution is running within MINIKUBE or docker compose, but tests will work the same with docker-compose, just replace MINIKUBE with LOCAL as argument of the scripts. Integration tests run also with Event Streams on Cloud and the microservices deployed on kubernetes or openshift. Be sure to be logged into the kubernetes cluster. We are using the greencompute namespace. kubectl get pods - n greencompute NAME READY STATUS RESTARTS AGE fleetms - deployment - f85cb679d - 582 pp 1 / 1 Running 2 14 d kafkabitmani - 0 1 / 1 Running 1 3 d23h kafkabitmani - zookeeper - 0 1 / 1 Running 0 3 d23h kcsolution - kc - ui - 76 b7b4fccf - z85j5 1 / 1 Running 1 8 d ordercommandms - deployment - 857865854 f - 7 mzqs 1 / 1 Running 0 3 d21h orderqueryms - deployment - 778 d79d99c - lrgfs 1 / 1 Running 0 3 d21h postgre - db - postgresql - 0 1 / 1 Running 2 14 d","title":"Ensure all services are running"},{"location":"itg-tests/#run-the-python-environment","text":"With the image ibmcase/python , you will be able to run the different integration tests. For example, the following commands will start a bash shell with the python environment, mounting the local filesystem into the docker /home folder, and connect to the same network as the Kafka broker and the other solution components are running into: $ pwd itg-tests $ ./startPythonEnv.sh MINIKUBE ( or use LOCAL ) root@fe61560d0cc4:/# From this shell, first specify where python should find the new modules, by setting the environment variable PYTHONPATH : root @fe61560d0cc4 : / # export PYTHONPATH =/ home root @fe61560d0cc4 : / # cd / home As the startPythonEnv is mounting the local itg-tests folder to the /home folder inside the docker container, we can access all the integration tests, and execute them ...","title":"Run the python environment"},{"location":"itg-tests/#how-to-proof-the-event-sourcing","text":"The goal of this test is to illustrate the happy path for event sourcing: all events for an order, are persisted in kafka, in the order of arrival, and a consumer with no offset commit, could run and help to answer to the question: What happened to the orderId 75? We want to validate the order events are sequential over time, and it is possible to replay the loading of events from time origin.","title":"How to proof the event sourcing"},{"location":"itg-tests/#replay-all-events-for-a-given-key","text":"First start the python docker container, so we can execute any python code. The script connects to the docker network where kafka runs. cd itg_tests . / startPythonEnv . sh MINIKUBE In the bash session, use the following command to ensure python knows how to get our new defined modules like the kafka consumer and producer: export PYTHONPATH =/ home cd / home / es - it Start python interpreter with the producer events code with orderID set to 75. $ python ProducerOrderEvents . py 75 The arguments are : [ ' ProducerOrderEvents.py ' , ' 75 ' ] Generate events for 75 Create order for 75 1 - CreateOrder :{ \" orderID \" : \" 75 \" , \" timestamp \" : 1555149614 , \" type \" : \" OrderCreated \" , \" payload \" : { \" orderID \" : \" 75 \" , \" productID \" : \" FreshFoodItg \" , \" customerID \" : \" Customer007 \" , \" quantity \" : 180 , \" pickupAddress \" : { \" street \" : \" astreet \" , \" city \" : \" Oakland \" , \" country \" : \" USA \" , \" state \" : \" CA \" , \" zipcode \" : \" 95000 \" }, \" destinationAddress \" : { \" street \" : \" bstreet \" , \" city \" : \" Beijing \" , \" country \" : \" China \" , \" state \" : \" NE \" , \" zipcode \" : \" 09000 \" }, \" pickupDate \" : \" 2019-05-25 \" , \" expectedDeliveryDate \" : \" 2019-06-25 \" }} { ' bootstrap.servers ' : ' kafka1:9092 ' , ' group.id ' : ' OrderProducerPython ' } Message delivered to orders [ 0 ] 2 - Producer accept the offer so now the order is booked :{ \" orderID \" : \" 75 \" , \" timestamp \" : 1555317000 , \" type \" : \" OrderBooked \" , \" payload \" : { \" orderID \" : \" 75 \" , \" productID \" : \" FreshFoodItg \" , \" customerID \" : \" Customer007 \" , \" quantity \" : 180 , \" pickupAddress \" : { \" street \" : \" astreet \" , \" city \" : \" Oakland \" , \" country \" : \" USA \" , \" state \" : \" CA \" , \" zipcode \" : \" 95000 \" }, \" destinationAddress \" : { \" street \" : \" bstreet \" , \" city \" : \" Beijing \" , \" country \" : \" China \" , \" state \" : \" NE \" , \" zipcode \" : \" 09000 \" }, \" pickupDate \" : \" 2019-05-25 \" , \" expectedDeliveryDate \" : \" 2019-06-25 \" }} Message delivered to orders [ 0 ] 3 - Voyage is assigned to order :{ \" orderID \" : \" 75 \" , \" timestamp \" : 1555317300 , \" type \" : \" OrderAssigned \" , \" payload \" : { \" orderID \" : \" 75 \" , \" voyageID \" : \" voyage21 \" }} Message delivered to orders [ 0 ] 4 - Allocate Reefer to order :{ \" orderID \" : \" 75 \" , \" timestamp \" : 1555405800 , \" type \" : \" ContainerAllocated \" , \" payload \" : { \" orderID \" : \" 75 \" , \" containerID \" : \" c13 \" }} Message delivered to orders [ 0 ] 5 - Reefer loaded with goods ready for Voyage :{ \" orderID \" : \" 75 \" , \" timestamp \" : 1557930600 , \" type \" : \" FullContainerVoyageReady \" , \" payload \" : { \" orderID \" : \" 75 \" , \" containerID \" : \" c13 \" }} Message delivered to orders [ 0 ] Now we can start the consumer to see the event coming with different time stamps. In a separate terminal windows run the command: . / runOrderConsumer . sh MINIKUBE 75 You should get the following results @@@ pollNextOrder orders partition : [ 0 ] at offset 8 with key b '75' : value : { \"orderID\" : \"75\" , \"timestamp\" : 1555149614 , \"type\" : \"OrderCreated\" , \"payload\" : { \"orderID\" : \"75\" , \"productID\" : \"FreshFoodItg\" , \"customerID\" : \"Customer007\" , \"quantity\" : 180 , \"pickupAddress\" : { \"street\" : \"astreet\" , \"city\" : \"Oakland\" , \"country\" : \"USA\" , \"state\" : \"CA\" , \"zipcode\" : \"95000\" } , \"destinationAddress\" : { \"street\" : \"bstreet\" , \"city\" : \"Beijing\" , \"country\" : \"China\" , \"state\" : \"NE\" , \"zipcode\" : \"09000\" } , \"pickupDate\" : \"2019-05-25\" , \"expectedDeliveryDate\" : \"2019-06-25\" }} @@@ pollNextOrder orders partition : [ 0 ] at offset 9 with key b '75' : value : { \"orderID\" : \"75\" , \"timestamp\" : 1555317000 , \"type\" : \"OrderBooked\" , \"payload\" : { \"orderID\" : \"75\" , \"productID\" : \"FreshFoodItg\" , \"customerID\" : \"Customer007\" , \"quantity\" : 180 , \"pickupAddress\" : { \"street\" : \"astreet\" , \"city\" : \"Oakland\" , \"country\" : \"USA\" , \"state\" : \"CA\" , \"zipcode\" : \"95000\" } , \"destinationAddress\" : { \"street\" : \"bstreet\" , \"city\" : \"Beijing\" , \"country\" : \"China\" , \"state\" : \"NE\" , \"zipcode\" : \"09000\" } , \"pickupDate\" : \"2019-05-25\" , \"expectedDeliveryDate\" : \"2019-06-25\" }} @@@ pollNextOrder orders partition : [ 0 ] at offset 10 with key b '75' : value : { \"orderID\" : \"75\" , \"timestamp\" : 1555317300 , \"type\" : \"OrderAssigned\" , \"payload\" : { \"orderID\" : \"75\" , \"voyageID\" : \"voyage21\" }} @@@ pollNextOrder orders partition : [ 0 ] at offset 11 with key b '75' : value : { \"orderID\" : \"75\" , \"timestamp\" : 1555405800 , \"type\" : \"ContainerAllocated\" , \"payload\" : { \"orderID\" : \"75\" , \"containerID\" : \"c13\" }} @@@ pollNextOrder orders partition : [ 0 ] at offset 12 with key b '75' : value : { \"orderID\" : \"75\" , \"timestamp\" : 1557930600 , \"type\" : \"FullContainerVoyageReady\" , \"payload\" : { \"orderID\" : \"75\" , \"containerID\" : \"c13\" }} Stopping with Ctrl-C (it can take time for python to get the keyboard interrupt) and then restarting the same consumer will bring you the same content. We can always answer the question at different time, but still get the same answer. The tests are under the itg-tests/es-it folder. The testin es-it/ProducerOrderEvents.py create the order events, and when combined with an order consumer give you the tracing of the process. The diagram below illustrates this simple environment. EventSourcingTests.py uses the event backbone, and the order microservices. To run the tests set the KAFKA_BROKERS and KAFKA_APIKEY environment variables","title":"Replay all events for a given key"},{"location":"itg-tests/#happy-path-for-the-order-life-cycle-validating-cqrs","text":"A classical happy path for a event-driven microservice is to receive a command to do something (could be an API operation (POST a new order)), validate the command, create a new event (OrderCreated) and append it to event log, then update its internal state, and may be run side effect like returning a response to the command initiator, or trigger a call to an external service to initiate a business process or a business task. We assume the python image was built, so you can use the following command to run the first test that creates an order via HTTP POST on the Order Command microservice (the write model), uses Kafka consumer to get the different events and then use HTTP GET to query the Order query microservices (the read model). The OrdersPython/ValidateOrderCreation.py code is documented and should be self-explanatory. The order state diagram is presented here. $ pwd itg-tests $ ./startPythonEnv.sh LOCAL ( or MINIKUBE ) root@fe61560d0cc4:/# cd home/OrdersPython root@fe61560d0cc4:/# python ValidateOrderCreation.py The implementation uses a set of constructs to poll the orders topic for events and timing out if not events are there.","title":"Happy path for the order life cycle: Validating CQRS"},{"location":"itg-tests/#failure-on-query-service","text":"One possible bad path, is when the order query microservice fails, and needs to recover from failure. In this case it should reload the events from last commited offset and update its internal states without running any code that could lead to side effect. To test this path, we first manual kill the docker container running the order query microservice, then use the same code as above to create an order. The last step of the call fails as the order created event was not processed by the query microservice, the data was not uploaded to its own projection. Restarting the microservice, and going to the URL http://localhost:11080/orders/byManuf/FishFarm, you will see the order is now part of the data of this service.","title":"Failure on query service"},{"location":"itg-tests/#how-to-proof-the-saga-pattern","text":"We want to validate the SAGA pattern to support the long-running, cross microservices, order transactions. The diagram is illustrating the use case we want to proof and tests: What we need to proof for the happy path: Send a new order to the order microservice via API with all the data to ship fresh goods from two countries separated by ocean verify the status of the order is pending The unique cross services key is the order ID verify orderCreated event was published verify voyage was allocated to order verify container was allocated to order verify ship has new containers added to its plan shipping plan verify the status of the order is assigned Business exeception error: no container for this type of load is available in this time frame. So the business response will be to keep the order in pending but trigger a business process for a customer representative to be in contact with the manufacturer for a remediation plan. Verify the response to the container service is an OrderUnfulled event.","title":"How to proof the SAGA pattern"},{"location":"itg-tests/itgtests/","text":"Reefer container shipment solution integration tests The Reefer container shipment solution comes with a set of integration test cases to ensure the end to end functionality of the application. These test cases are part of our CI/CD process so that we ensure every new pull request that brings new code in does not break or modify the correct functionality of the application. So far we have the following integration test cases: Happy path - End to end happy path test. SAGA pattern - SAGA pattern for new order creation test. Container Anomaly - Container anomaly and maintenance test. New integration test cases will be added in order to test other parts of the application as well as use cases and other Event Driven Patterns. How to run the integration test cases Pre-requisites In order to run the integration tests against the Reefer container shipment solution you first need to have this solution deployed on an Openshift or Kubernetes cluster. The solution is made up of: Backing services such as IBM Event Streams and PostgreSQL - Instructions here . The Reefer container shipment solution components - Instructions here . Once you have the solution deployed into your cluster, apart from an instance of IBM Event Streams and PostgreSQL either on premises or in IBM Cloud, you should have the following components at the very least for the integration tests to run: $ oc get pods NAME READY STATUS RESTARTS AGE pod/ordercommandms-deployment-7cfcf65ffc-ffbxt 1 /1 Running 0 32d pod/orderqueryms-deployment-5ff4fd44d-ghrg6 1 /1 Running 0 32d pod/springcontainerms-deployment-7f78fc9b64-kt2pf 1 /1 Running 0 32d pod/voyagesms-deployment-7775bb8974-h8vj4 1 /1 Running 0 32d The integration test cases have been implemented to be run as a kubernetes job called reefer-itgtests-job . This job consist of a tailored python container where the integration tests, which are written in Python, will get executed in. The yaml file that will create such kubernetes job, called ReeferItgTests.yaml , can be found under the itg-tests/es-it folder in this very same repository. The reason for creating a tailored python container which to execute the integration tests in is because we can then control the execution environment for the integration tests. This way we ensure the appropriate libraries, permissions, etc are as expected. This tailored python container docker image is publicly available in the Docker Hub ( ibmcase/kcontainer-python:itgtests ). Please, make sure you can access the Docker Hub public registries from your OpenShift or Kubernetes cluster. The integration tests also require of some variables being defined beforehand, some of which need to be defined as secrets or configMaps within your kubernetes namespace or OpenShift project, such as KAFKA_APIKEY , KAFKA_BROKERS and the IBM Event Streams PEM certificate (in case you are working with IBM Event Streams on premise), where the Reefer container shipment solution has been deployed into. You should have got these secrets or configMaps already created when deploying your backing services in #1 of this pre-requisites section. Other required variables for the integration tests need to be defined within the kubernetes job yaml file: Orders topic name: This could be specified within the integration tests kubernetes job yaml file under the variable ITGTESTS_ORDERS_TOPIC which defaults to itg-orders . Order Command topic name: This could be specified within the integration tests kubernetes job yaml file under the variable ITGTESTS_ORDER_COMMANDS_TOPIC which defaults to itg-orderCommands . Containers topic name: This could be specified within the integration tests kubernetes job yaml file under the variable ITGTESTS_CONTAINERS_TOPIC which defaults to itg-containers . Kafka Environment: It should be either OCP or IBMCLOUD depending on where your IBM Event Streams instance is deployed onto. If it is deployed on premises in your OpenShift or Kubernetes cluster, then it KAFKA_ENV should be set to OCP . If you are using an IBM Event Streams instance in the IBM Cloud, then KAFKA_ENV should be set to IBMCLOUD . This is important as the IBM Event Streams on-prem instances require a PEM certificate for the Kafka libraries to successfully connect to it. So, if you are using IBM Event Streams on-prem in your OpenShift or Kubernetes cluster, you also have to: Uncomment the bottom part of the integration tests kubernetes job yaml file. Make sure you created the eventstreams-pem-file secret that will hold your IBM Event Streams PEM certificate, in step #1 of this pre-requisites section. IMPORTANT: For the integration tests to work fine, we must mockup the BPM integration by pointing it to a testing post endpoint such as: https://postman-echo.com/post . For doing this, you will need to make sure the bpm-anomaly configMap you created for the Spring Container microservice component of the Reefer container shipment solution holds that testing post endpoint. You can do so by manually editing the configMap: $ oc edit configmap bpm-anomaly -n eda-integration The above will require to restart the Spring Container microservice component. Run In order to run the integration test cases for the Reefer container shipment solution, we need to create the the job that will run these. To create the job, we simply execute: oc apply -f ReeferItgReefer.yaml -n <namespace> You should see the following output: job.batch/reefer-itgtests-job created and if you list the pods in your namespace you should see a new pod which is running the integration tests: $ oc get pods | grep itgtests NAME READY STATUS RESTARTS AGE reefer-itgtests-job-x594k 1 /1 Running 0 2m Once the integration tests have finished, the pod should transition to completed status: $ oc get pods NAME READY STATUS RESTARTS AGE reefer-itgtests-job-x594k 0 /1 Completed 0 3m and the job output should be like: $ oc get jobs NAME DESIRED SUCCESSFUL AGE reefer-itgtests-job 1 1 3m Output If we want to inspect the output of the integration tests, we would need to get the logs for the pod that ran them: $ oc logs e2e-reefer-itgtests-job-x594k The output of the integration test cases is made up of a brief description of the execution environment: ----------------------------------------------------------------- -- Reefer Container Shipment EDA application Integration Tests -- ----------------------------------------------------------------- Executing integrations tests from branch master of https://github.com/ibm-cloud-architecture/refarch-kc.git Kafka Brokers: broker-0-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-3-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093 Kafka API Key: 98YA5dC-G6cODJtRFPJwi4DwNbwZABmsrSFI115jP6k5 Kafka Env: IBMCLOUD Orders topic name: itg-orders Order Command topic name: itg-orderCommands Containers topic name: itg-containers ------------------------------------------------------------------ Then, each of the three test cases outlined in the introduction of this readme file will get executed, each of them beginning with a header like: ****************************************** ****************************************** ********** E2E Happy Path ************ ****************************************** ****************************************** After the header, the different tests within the test case will get executed. Each of these comes with a header and look like: -------------------------------- --- [ TEST ] : Voyage Assigned --- -------------------------------- 1 - Load the expected voyage assigned event on the order topic from its json files The expected voyage assigned event is: { \"payload\" : { \"orderID\" : \"a467070e-797e-40f9-9644-7393e8553f1f\" , \"voyageID\" : \"101\" } , \"timestamp\" : \"\" , \"type\" : \"VoyageAssigned\" , \"version\" : \"1\" } Done 2 - Read voyage assigned from oder topic [ KafkaConsumer ] - This is the configuration for the consumer: [ KafkaConsumer ] - { 'bootstrap.servers' : 'broker-0-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-3-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093' , 'group.id' : 'pythonconsumers' , 'auto.offset.reset' : 'earliest' , 'enable.auto.commit' : True, 'security.protocol' : 'SASL_SSL' , 'sasl.mechanisms' : 'PLAIN' , 'sasl.username' : 'token' , 'sasl.password' : '98YA5dC-G6cODJtRFPJwi4DwNbwZABmsrSFI115jP6k5' } . [ KafkaConsumer ] - @@@ pollNextOrder itg-orders partition: [ 0 ] at offset 2 with key b 'a467070e-797e-40f9-9644-7393e8553f1f' : value: { \"timestamp\" :1576667245430, \"type\" : \"VoyageAssigned\" , \"version\" : \"1\" , \"payload\" : { \"voyageID\" : \"101\" , \"orderID\" : \"a467070e-797e-40f9-9644-7393e8553f1f\" }} This is the event read from the order topic: { \"payload\" : { \"orderID\" : \"a467070e-797e-40f9-9644-7393e8553f1f\" , \"voyageID\" : \"101\" } , \"timestamp\" : \"\" , \"type\" : \"VoyageAssigned\" , \"version\" : \"1\" } Done 3 - Verify voyage assigned event Done A summary of the test case execution is shown at the end and look like: ---------------------------------------------------------------------- Ran 7 tests in 64 .262s OK","title":"Overview"},{"location":"itg-tests/itgtests/#reefer-container-shipment-solution-integration-tests","text":"The Reefer container shipment solution comes with a set of integration test cases to ensure the end to end functionality of the application. These test cases are part of our CI/CD process so that we ensure every new pull request that brings new code in does not break or modify the correct functionality of the application. So far we have the following integration test cases: Happy path - End to end happy path test. SAGA pattern - SAGA pattern for new order creation test. Container Anomaly - Container anomaly and maintenance test. New integration test cases will be added in order to test other parts of the application as well as use cases and other Event Driven Patterns.","title":"Reefer container shipment solution integration tests"},{"location":"itg-tests/itgtests/#how-to-run-the-integration-test-cases","text":"","title":"How to run the integration test cases"},{"location":"itg-tests/itgtests/#pre-requisites","text":"In order to run the integration tests against the Reefer container shipment solution you first need to have this solution deployed on an Openshift or Kubernetes cluster. The solution is made up of: Backing services such as IBM Event Streams and PostgreSQL - Instructions here . The Reefer container shipment solution components - Instructions here . Once you have the solution deployed into your cluster, apart from an instance of IBM Event Streams and PostgreSQL either on premises or in IBM Cloud, you should have the following components at the very least for the integration tests to run: $ oc get pods NAME READY STATUS RESTARTS AGE pod/ordercommandms-deployment-7cfcf65ffc-ffbxt 1 /1 Running 0 32d pod/orderqueryms-deployment-5ff4fd44d-ghrg6 1 /1 Running 0 32d pod/springcontainerms-deployment-7f78fc9b64-kt2pf 1 /1 Running 0 32d pod/voyagesms-deployment-7775bb8974-h8vj4 1 /1 Running 0 32d The integration test cases have been implemented to be run as a kubernetes job called reefer-itgtests-job . This job consist of a tailored python container where the integration tests, which are written in Python, will get executed in. The yaml file that will create such kubernetes job, called ReeferItgTests.yaml , can be found under the itg-tests/es-it folder in this very same repository. The reason for creating a tailored python container which to execute the integration tests in is because we can then control the execution environment for the integration tests. This way we ensure the appropriate libraries, permissions, etc are as expected. This tailored python container docker image is publicly available in the Docker Hub ( ibmcase/kcontainer-python:itgtests ). Please, make sure you can access the Docker Hub public registries from your OpenShift or Kubernetes cluster. The integration tests also require of some variables being defined beforehand, some of which need to be defined as secrets or configMaps within your kubernetes namespace or OpenShift project, such as KAFKA_APIKEY , KAFKA_BROKERS and the IBM Event Streams PEM certificate (in case you are working with IBM Event Streams on premise), where the Reefer container shipment solution has been deployed into. You should have got these secrets or configMaps already created when deploying your backing services in #1 of this pre-requisites section. Other required variables for the integration tests need to be defined within the kubernetes job yaml file: Orders topic name: This could be specified within the integration tests kubernetes job yaml file under the variable ITGTESTS_ORDERS_TOPIC which defaults to itg-orders . Order Command topic name: This could be specified within the integration tests kubernetes job yaml file under the variable ITGTESTS_ORDER_COMMANDS_TOPIC which defaults to itg-orderCommands . Containers topic name: This could be specified within the integration tests kubernetes job yaml file under the variable ITGTESTS_CONTAINERS_TOPIC which defaults to itg-containers . Kafka Environment: It should be either OCP or IBMCLOUD depending on where your IBM Event Streams instance is deployed onto. If it is deployed on premises in your OpenShift or Kubernetes cluster, then it KAFKA_ENV should be set to OCP . If you are using an IBM Event Streams instance in the IBM Cloud, then KAFKA_ENV should be set to IBMCLOUD . This is important as the IBM Event Streams on-prem instances require a PEM certificate for the Kafka libraries to successfully connect to it. So, if you are using IBM Event Streams on-prem in your OpenShift or Kubernetes cluster, you also have to: Uncomment the bottom part of the integration tests kubernetes job yaml file. Make sure you created the eventstreams-pem-file secret that will hold your IBM Event Streams PEM certificate, in step #1 of this pre-requisites section. IMPORTANT: For the integration tests to work fine, we must mockup the BPM integration by pointing it to a testing post endpoint such as: https://postman-echo.com/post . For doing this, you will need to make sure the bpm-anomaly configMap you created for the Spring Container microservice component of the Reefer container shipment solution holds that testing post endpoint. You can do so by manually editing the configMap: $ oc edit configmap bpm-anomaly -n eda-integration The above will require to restart the Spring Container microservice component.","title":"Pre-requisites"},{"location":"itg-tests/itgtests/#run","text":"In order to run the integration test cases for the Reefer container shipment solution, we need to create the the job that will run these. To create the job, we simply execute: oc apply -f ReeferItgReefer.yaml -n <namespace> You should see the following output: job.batch/reefer-itgtests-job created and if you list the pods in your namespace you should see a new pod which is running the integration tests: $ oc get pods | grep itgtests NAME READY STATUS RESTARTS AGE reefer-itgtests-job-x594k 1 /1 Running 0 2m Once the integration tests have finished, the pod should transition to completed status: $ oc get pods NAME READY STATUS RESTARTS AGE reefer-itgtests-job-x594k 0 /1 Completed 0 3m and the job output should be like: $ oc get jobs NAME DESIRED SUCCESSFUL AGE reefer-itgtests-job 1 1 3m","title":"Run"},{"location":"itg-tests/itgtests/#output","text":"If we want to inspect the output of the integration tests, we would need to get the logs for the pod that ran them: $ oc logs e2e-reefer-itgtests-job-x594k The output of the integration test cases is made up of a brief description of the execution environment: ----------------------------------------------------------------- -- Reefer Container Shipment EDA application Integration Tests -- ----------------------------------------------------------------- Executing integrations tests from branch master of https://github.com/ibm-cloud-architecture/refarch-kc.git Kafka Brokers: broker-0-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-3-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093 Kafka API Key: 98YA5dC-G6cODJtRFPJwi4DwNbwZABmsrSFI115jP6k5 Kafka Env: IBMCLOUD Orders topic name: itg-orders Order Command topic name: itg-orderCommands Containers topic name: itg-containers ------------------------------------------------------------------ Then, each of the three test cases outlined in the introduction of this readme file will get executed, each of them beginning with a header like: ****************************************** ****************************************** ********** E2E Happy Path ************ ****************************************** ****************************************** After the header, the different tests within the test case will get executed. Each of these comes with a header and look like: -------------------------------- --- [ TEST ] : Voyage Assigned --- -------------------------------- 1 - Load the expected voyage assigned event on the order topic from its json files The expected voyage assigned event is: { \"payload\" : { \"orderID\" : \"a467070e-797e-40f9-9644-7393e8553f1f\" , \"voyageID\" : \"101\" } , \"timestamp\" : \"\" , \"type\" : \"VoyageAssigned\" , \"version\" : \"1\" } Done 2 - Read voyage assigned from oder topic [ KafkaConsumer ] - This is the configuration for the consumer: [ KafkaConsumer ] - { 'bootstrap.servers' : 'broker-0-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-3-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093' , 'group.id' : 'pythonconsumers' , 'auto.offset.reset' : 'earliest' , 'enable.auto.commit' : True, 'security.protocol' : 'SASL_SSL' , 'sasl.mechanisms' : 'PLAIN' , 'sasl.username' : 'token' , 'sasl.password' : '98YA5dC-G6cODJtRFPJwi4DwNbwZABmsrSFI115jP6k5' } . [ KafkaConsumer ] - @@@ pollNextOrder itg-orders partition: [ 0 ] at offset 2 with key b 'a467070e-797e-40f9-9644-7393e8553f1f' : value: { \"timestamp\" :1576667245430, \"type\" : \"VoyageAssigned\" , \"version\" : \"1\" , \"payload\" : { \"voyageID\" : \"101\" , \"orderID\" : \"a467070e-797e-40f9-9644-7393e8553f1f\" }} This is the event read from the order topic: { \"payload\" : { \"orderID\" : \"a467070e-797e-40f9-9644-7393e8553f1f\" , \"voyageID\" : \"101\" } , \"timestamp\" : \"\" , \"type\" : \"VoyageAssigned\" , \"version\" : \"1\" } Done 3 - Verify voyage assigned event Done A summary of the test case execution is shown at the end and look like: ---------------------------------------------------------------------- Ran 7 tests in 64 .262s OK","title":"Output"},{"location":"itg-tests/containerAnomaly/containerAnomaly/","text":"Container Anomaly test case This test case will test the use whereby a container suffers some anomaly. As a result, ContainerAnomaly events are sent to set the container for maintenance mode. A BPM process will take care of setting a container into inMaintenance mode, assigning a technician to repair the container and finally setting that container back to being empty and available. This test case will also ensure the order the container with the anomaly was carrying is marked spoilt. The following diagram depics roughly this path on a flow base fashion where the blue square rectangles, purple rounded rectangles and orange rectangles represent actions taken by the test case, events produced to the event backbone (either Kafka or IBM Event Streams) and object statuses respectively. Tests The following sequence diagrams represent each of the tests within this container anomaly integration test case. Test 1 - Send Anomaly Events This tests will send the ContainerAnomaly events needed to trigger the bpm call process and order spoilage. Test 2 - Container in MaintenanceNeeded Status This test will make sure the container which ContainerAnomaly events have been sent for gets into MaintenanceNeeded status as a result. Test 3 - Order Spoilt This test will make sure that the order the container which ContainerAnomaly events have been sent for was carrying gets into spoilt status as a result. Test 4 - Container To Maintenance This test will call the toMaintenance API endpoint for setting a container into inMaintenance mode. Test 5 - Container InMaintenance Status This test will make sure that the container that was set into inMaintenance mode in the previous test, is now indeed into inMaintenance mode. Test 6 - Container Off Maintenance This test will set the container that was inMaintenance status off maintenance. Test 7 - Container Empty Status This test will make sure that the container that was set off maintenance mode in the previous test, is now indeed empty and available.","title":"Container Anomaly"},{"location":"itg-tests/containerAnomaly/containerAnomaly/#container-anomaly-test-case","text":"This test case will test the use whereby a container suffers some anomaly. As a result, ContainerAnomaly events are sent to set the container for maintenance mode. A BPM process will take care of setting a container into inMaintenance mode, assigning a technician to repair the container and finally setting that container back to being empty and available. This test case will also ensure the order the container with the anomaly was carrying is marked spoilt. The following diagram depics roughly this path on a flow base fashion where the blue square rectangles, purple rounded rectangles and orange rectangles represent actions taken by the test case, events produced to the event backbone (either Kafka or IBM Event Streams) and object statuses respectively.","title":"Container Anomaly test case"},{"location":"itg-tests/containerAnomaly/containerAnomaly/#tests","text":"The following sequence diagrams represent each of the tests within this container anomaly integration test case.","title":"Tests"},{"location":"itg-tests/containerAnomaly/containerAnomaly/#test-1-send-anomaly-events","text":"This tests will send the ContainerAnomaly events needed to trigger the bpm call process and order spoilage.","title":"Test 1 - Send Anomaly Events"},{"location":"itg-tests/containerAnomaly/containerAnomaly/#test-2-container-in-maintenanceneeded-status","text":"This test will make sure the container which ContainerAnomaly events have been sent for gets into MaintenanceNeeded status as a result.","title":"Test 2 - Container in MaintenanceNeeded Status"},{"location":"itg-tests/containerAnomaly/containerAnomaly/#test-3-order-spoilt","text":"This test will make sure that the order the container which ContainerAnomaly events have been sent for was carrying gets into spoilt status as a result.","title":"Test 3 - Order Spoilt"},{"location":"itg-tests/containerAnomaly/containerAnomaly/#test-4-container-to-maintenance","text":"This test will call the toMaintenance API endpoint for setting a container into inMaintenance mode.","title":"Test 4 - Container To Maintenance"},{"location":"itg-tests/containerAnomaly/containerAnomaly/#test-5-container-inmaintenance-status","text":"This test will make sure that the container that was set into inMaintenance mode in the previous test, is now indeed into inMaintenance mode.","title":"Test 5 - Container InMaintenance Status"},{"location":"itg-tests/containerAnomaly/containerAnomaly/#test-6-container-off-maintenance","text":"This test will set the container that was inMaintenance status off maintenance.","title":"Test 6 - Container Off Maintenance"},{"location":"itg-tests/containerAnomaly/containerAnomaly/#test-7-container-empty-status","text":"This test will make sure that the container that was set off maintenance mode in the previous test, is now indeed empty and available.","title":"Test 7 - Container Empty Status"},{"location":"itg-tests/happy-path/happy_path/","text":"Happy Path integration test case Here you can find the so called \"happy path\" integration test for the Reefer Containers EDA reference implementation. This test case tests the expected happy path for the application where a new order is created, the order gets a container and a voyage assigned and this new order becomes an assigned order. The following diagram depics roughly this path on a flow base fashion where the blue square rectangles, purple rounded rectangles and orange rectangles represent actions taken by the test case, events produced to the event backbone (either Kafka or IBM Event Streams) and object statuses respectively. The idea behind this integration test case is to be used as a validation for any new deployment of the Reefer Containers EDA reference implementation. This test case is intended to be extended along with the creation of other tests cases to verify other scenarios or edge cases. Tests The following sequence diagrams represent each of the tests within this happy path integration test case. Test 1 - Create container This tests will make sure that a container with the appropriate capacity and location for the expected new order to be created exists. Test 2 - Voyages exist This test will make sure the expected voyages exist. Test 3 - Create order This test will make sure that the expected new order event is created along with a new order command event. Test 4 - Container Allocated This test will make sure that the expected container is assigned to the new order. Test 5 - Voyage Assigned This test will make sure that the expected voyage is assigned to the new order. Test 6 - Order Assigned This test will make sure that the resulting order is as expected. That is, it transitions to the assigned status and has a container and voyage assigned to it.","title":"Happy Path"},{"location":"itg-tests/happy-path/happy_path/#happy-path-integration-test-case","text":"Here you can find the so called \"happy path\" integration test for the Reefer Containers EDA reference implementation. This test case tests the expected happy path for the application where a new order is created, the order gets a container and a voyage assigned and this new order becomes an assigned order. The following diagram depics roughly this path on a flow base fashion where the blue square rectangles, purple rounded rectangles and orange rectangles represent actions taken by the test case, events produced to the event backbone (either Kafka or IBM Event Streams) and object statuses respectively. The idea behind this integration test case is to be used as a validation for any new deployment of the Reefer Containers EDA reference implementation. This test case is intended to be extended along with the creation of other tests cases to verify other scenarios or edge cases.","title":"Happy Path integration test case"},{"location":"itg-tests/happy-path/happy_path/#tests","text":"The following sequence diagrams represent each of the tests within this happy path integration test case.","title":"Tests"},{"location":"itg-tests/happy-path/happy_path/#test-1-create-container","text":"This tests will make sure that a container with the appropriate capacity and location for the expected new order to be created exists.","title":"Test 1 - Create container"},{"location":"itg-tests/happy-path/happy_path/#test-2-voyages-exist","text":"This test will make sure the expected voyages exist.","title":"Test 2 - Voyages exist"},{"location":"itg-tests/happy-path/happy_path/#test-3-create-order","text":"This test will make sure that the expected new order event is created along with a new order command event.","title":"Test 3 - Create order"},{"location":"itg-tests/happy-path/happy_path/#test-4-container-allocated","text":"This test will make sure that the expected container is assigned to the new order.","title":"Test 4 - Container Allocated"},{"location":"itg-tests/happy-path/happy_path/#test-5-voyage-assigned","text":"This test will make sure that the expected voyage is assigned to the new order.","title":"Test 5 - Voyage Assigned"},{"location":"itg-tests/happy-path/happy_path/#test-6-order-assigned","text":"This test will make sure that the resulting order is as expected. That is, it transitions to the assigned status and has a container and voyage assigned to it.","title":"Test 6 - Order Assigned"},{"location":"itg-tests/saga/saga/","text":"SAGA pattern integration test case Here you can find two test cases to test the SAGA pattern implemented for our Reefer Container EDA reference application. These test cases verify a new order creation does not get into assigned state unless a container and a voyage are allocated and assigned to it. These test cases are meant to be executed after the happy path test case . Since the case that a container is allocated and a voyage assigned to an newly created order is tested previously by our happy path test case, in these test cases we're actually testing the behavior when either a container or a voyage does not exist for the order. SAGA pattern - no container This test case will ensure that the SAGA pattern for our EDA application works as expected by setting the newly created order to rejected as a result of not finding a suitable container available for the order. The following diagram depics roughly this case on a flow base fashion where the blue square rectangles, purple rounded rectangles and orange rectangles represent actions taken by the test case, events produced to the event backbone (either Kafka or IBM Event Streams) and object statuses respectively. The following sequence diagrams represent each of the tests within this SAGA pattern - no container integration test case. Test 1 - No Container - Create order This test will make sure that the expected CreateOrderCommand event to create an order is delivered into the orderCommands topic after the Create Order API endpoint is called. It will also ensure that an OrderCreated event is also delivered into the orders topic. Test 2 - No Container - Container Not Found This test will make sure that the expected ContainerNotFound event is delivered into the orders topic once the Spring Container microservice could not find a container suitable for the order available. Test 3 - No Container - Order Rejected This test will make sure that the expected OrderRejected event is delivered into the orders topic after the Order Command microservice received the ContainerNotFound event. Test 4 - No Container - Order Rejected REST This test will make sure that the order is set to rejected through the Order Command and Order Query microservices. SAGA pattern - no voyage This test case will ensure that the SAGA pattern for our EDA application works as expected by setting the newly created order to rejected as a result of not finding a suitable voyage available for the order. The following diagram depics roughly this case on a flow base fashion where the blue square rectangles, purple rounded rectangles and orange rectangles represent actions taken by the test case, events produced to the event backbone (either Kafka or IBM Event Streams) and object statuses respectively. The following sequence diagrams represent each of the tests within this SAGA pattern - no voyage integration test case. Test 1 - No Voyage - Create container This test will make sure that the expected ContainerAdded event to create a new container is delivered into the containers topic. It will then make sure the container is properly created through the Spring Container API endpoints. Test 2 - No Voyage - Create order This test will make sure that the expected CreateOrderCommand event to create an order is delivered into the orderCommands topic after the Create Order API endpoint is called. It will also ensure that an OrderCreated event is also delivered into the orders topic. Test 3 - No Voyage - Container Allocated This test will make sure that the expected ContainerAssignedToOrder event is delivered into the containers topic as well as the expected ContainerAllocated event is delivered into the orders topic. It will then verify the container state is as expected through the Spring Container API endpoint. Test 4 - No Voyage - Voyage Not Found This test will make sure that the expected VoyageNotFound event is delivered into the orders topic. Test 5 - No Voyage - Order Rejected This test will make sure that the expected OrderRejected event is delivered into the orders topic after the Order Command microservice received the ContainerNotFound event. Test 6 - No Voyage - Order Rejected REST This test will make sure that the order is set to rejected through the Order Command and Order Query microservices.","title":"Saga Pattern"},{"location":"itg-tests/saga/saga/#saga-pattern-integration-test-case","text":"Here you can find two test cases to test the SAGA pattern implemented for our Reefer Container EDA reference application. These test cases verify a new order creation does not get into assigned state unless a container and a voyage are allocated and assigned to it. These test cases are meant to be executed after the happy path test case . Since the case that a container is allocated and a voyage assigned to an newly created order is tested previously by our happy path test case, in these test cases we're actually testing the behavior when either a container or a voyage does not exist for the order.","title":"SAGA pattern integration test case"},{"location":"itg-tests/saga/saga/#saga-pattern-no-container","text":"This test case will ensure that the SAGA pattern for our EDA application works as expected by setting the newly created order to rejected as a result of not finding a suitable container available for the order. The following diagram depics roughly this case on a flow base fashion where the blue square rectangles, purple rounded rectangles and orange rectangles represent actions taken by the test case, events produced to the event backbone (either Kafka or IBM Event Streams) and object statuses respectively. The following sequence diagrams represent each of the tests within this SAGA pattern - no container integration test case.","title":"SAGA pattern - no container"},{"location":"itg-tests/saga/saga/#test-1-no-container-create-order","text":"This test will make sure that the expected CreateOrderCommand event to create an order is delivered into the orderCommands topic after the Create Order API endpoint is called. It will also ensure that an OrderCreated event is also delivered into the orders topic.","title":"Test 1 - No Container - Create order"},{"location":"itg-tests/saga/saga/#test-2-no-container-container-not-found","text":"This test will make sure that the expected ContainerNotFound event is delivered into the orders topic once the Spring Container microservice could not find a container suitable for the order available.","title":"Test 2 - No Container - Container Not Found"},{"location":"itg-tests/saga/saga/#test-3-no-container-order-rejected","text":"This test will make sure that the expected OrderRejected event is delivered into the orders topic after the Order Command microservice received the ContainerNotFound event.","title":"Test 3 - No Container - Order Rejected"},{"location":"itg-tests/saga/saga/#test-4-no-container-order-rejected-rest","text":"This test will make sure that the order is set to rejected through the Order Command and Order Query microservices.","title":"Test 4 - No Container - Order Rejected REST"},{"location":"itg-tests/saga/saga/#saga-pattern-no-voyage","text":"This test case will ensure that the SAGA pattern for our EDA application works as expected by setting the newly created order to rejected as a result of not finding a suitable voyage available for the order. The following diagram depics roughly this case on a flow base fashion where the blue square rectangles, purple rounded rectangles and orange rectangles represent actions taken by the test case, events produced to the event backbone (either Kafka or IBM Event Streams) and object statuses respectively. The following sequence diagrams represent each of the tests within this SAGA pattern - no voyage integration test case.","title":"SAGA pattern - no voyage"},{"location":"itg-tests/saga/saga/#test-1-no-voyage-create-container","text":"This test will make sure that the expected ContainerAdded event to create a new container is delivered into the containers topic. It will then make sure the container is properly created through the Spring Container API endpoints.","title":"Test 1 - No Voyage - Create container"},{"location":"itg-tests/saga/saga/#test-2-no-voyage-create-order","text":"This test will make sure that the expected CreateOrderCommand event to create an order is delivered into the orderCommands topic after the Create Order API endpoint is called. It will also ensure that an OrderCreated event is also delivered into the orders topic.","title":"Test 2 - No Voyage - Create order"},{"location":"itg-tests/saga/saga/#test-3-no-voyage-container-allocated","text":"This test will make sure that the expected ContainerAssignedToOrder event is delivered into the containers topic as well as the expected ContainerAllocated event is delivered into the orders topic. It will then verify the container state is as expected through the Spring Container API endpoint.","title":"Test 3 - No Voyage - Container Allocated"},{"location":"itg-tests/saga/saga/#test-4-no-voyage-voyage-not-found","text":"This test will make sure that the expected VoyageNotFound event is delivered into the orders topic.","title":"Test 4 - No Voyage - Voyage Not Found"},{"location":"itg-tests/saga/saga/#test-5-no-voyage-order-rejected","text":"This test will make sure that the expected OrderRejected event is delivered into the orders topic after the Order Command microservice received the ContainerNotFound event.","title":"Test 5 - No Voyage - Order Rejected"},{"location":"itg-tests/saga/saga/#test-6-no-voyage-order-rejected-rest","text":"This test will make sure that the order is set to rejected through the Order Command and Order Query microservices.","title":"Test 6 - No Voyage - Order Rejected REST"}]}