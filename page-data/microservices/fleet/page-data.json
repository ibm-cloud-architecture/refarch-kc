{"componentChunkName":"component---src-pages-microservices-fleet-index-mdx","path":"/microservices/fleet/","result":{"pageContext":{"frontmatter":{"title":"Shipping Fleet Management","description":"In-depth description of the Fleet Management microservice component of the Reefer Container Shipment solution reference implementation."},"relativePagePath":"/microservices/fleet/index.mdx","titleType":"append","MdxNode":{"id":"7cecfe92-7e10-5813-aa09-cfd18193085b","children":[],"parent":"7461cb41-cc7e-5f7e-8653-6199cf45922b","internal":{"content":"---\ntitle: Shipping Fleet Management\ndescription: In-depth description of the Fleet Management microservice component of the Reefer Container Shipment solution reference implementation.\n---\n\n<PageDescription>\nThis microservice is responsible to support simulation of fleet of container carrier vessels. It used for demonstration purpose, but it is still using an event-driven microservice implementation approach. It supports the event, actors, and commands discovered during the event storming workshop and illustrated by the following figure for the \"ship actor\".\n</PageDescription>\n\n<AnchorLinks>\n  <AnchorLink>Overview</AnchorLink>\n  <AnchorLink>Build</AnchorLink>\n  <AnchorLink>Run</AnchorLink>\n  <AnchorLink>Usage Details</AnchorLink>\n</AnchorLinks>\n\n## Overview\n\n**Description:** The service exposes simple a REST API to support acquiring ship and fleet information, while also providing simulation control to emulate ship movements and container metrics event generation. When a ship leaves or enters a port, it will also generate the events as listed in the [Event Storming Analysis](/implementation/event-storming-analysis/) and [Domain-Driven Design](/implementation/domain-driven-design/) results.\n\n**Github repository:** [refarch-kc-ms](https://github.com/ibm-cloud-architecture/refarch-kc-ms)\n\n**Folder:** [fleet-ms](https://github.com/ibm-cloud-architecture/refarch-kc-ms/tree/master/fleet-ms)\n\n**Kafka topics consumed from:**\n\n- [Bluewater Container](/microservices/topic-details/#bluewater-container-topic)\n- [Bluewater Ship](/microservices/topic-details/#bluewater-ship-topic)\n- [Bluewater Problem](/microservices/topic-details/#bluewater-problem-topic)\n\n**Kafka topics produced to:**\n\n- [Bluewater Container](/microservices/topic-details/#bluewater-container-topic)\n- [Bluewater Ship](/microservices/topic-details/#bluewater-ship-topic)\n- [Bluewater Problem](/microservices/topic-details/#bluewater-problem-topic)\n\n**Events reacted to:**\n\n- [Container Metric Event](/microservices/event-details/#container-metric-event)\n- [Ship Position Event](/microservices/event-details/#ship-position-event)\n- [Bluewater Problem Event](/microservices/event-details/#bluewater-problem-event)\n\n**Events produced:**\n\n- [Container Metric Event](/microservices/event-details/#container-metric-event)\n- [Ship Position Event](/microservices/event-details/#ship-position-event)\n\n**EDA Patterns implemented:**\n\n- [Consume-transform-produce Loop](/implementation/consume-transform-produce/)\n\n## Build\n\nThis microservice is built using the [Appsody](https://appsody.dev/) development framework. The [Appsody CLI](https://appsody.dev/docs/installing/installing-appsody) is a required prerequisite for building the application locally.\n\nAppsody will build the application by pulling the contents of the Appsody Stack it is based on and then performing the local application build inside the containerized environment:\n\n`appsody build -t <yournamespace>/kcontainer-fleet-ms[:tag] [--push]`\n\n- You can optionally specify a container tag. If left blank, `latest` will be used.\n- You can optionally supply the `--push` flag to automatically push the built image to specified remote repository.\n\nPerforming an Appsody build will update the `app-deploy.yaml` file in the same directory with current information for the application image, labels, and annotations fields.\n\n## Run\n\n### Deployment parameters\n\nThe following deployment parameters are defined in the `app-deploy.yaml` file:\n\n| Name                                     | Required | Description                                                                                                            |\n|------------------------------------------|----------|------------------------------------------------------------------------------------------------------------------------|\n| KAFKA_BROKERS                            | YES      | Comma-separated list of Kafka brokers to connect to                                                                    |\n| KAFKA_APIKEY                             | NO       | API Key used to connect to SASL-secured Kafka brokers. This is required when connecting to IBM Event Streams clusters. |\n| TRUSTSTORE_ENABLED                       | NO       | Required to be set to `true` when connecting to IBM Event Streams on the IBM Cloud Pak for Integration (CP4I).         |\n| TRUSTSTORE_PATH                          | NO       | The local path to the required truststore file when connecting to IBM Event Streams on CP4I. See [**Volume Mounts**](#volume-mounts) below.  |\n| TRUSTSTORE_PWD                           | NO       | The password for the truststore file used for IBM Event Streams server verification.                                   |\n| KAFKA_SHIP_TOPIC_NAME                    | YES      | The topic name used for communication relating to the ship  entity.                                                    |\n| KAFKA_CONTAINER_TOPIC_NAME               | YES      | The topic name used for communication relating to the containers entity.                                               |\n| KAFKA_BW_PROBLEM_TOPIC_NAME              | YES      | The topic name used for communication relating to the bluewater problems domain.                                      |\n\n### Volume Mounts\n\nThe Fleet Management microservice requires up to one file to be injected at runtime for proper operation. As noted in the `TRUSTSTORE_PATH` parameter above, these files are SSL-based certificates which are required to verfiy the identity of the external service when calling it. These files are provided as `--docker-options \"-v host-src:container-dest ...\"` when running the microservice locally and as a Volume Mount when running the microservice on a Kubernetes cluster.\n\nThe `TRUSTSTORE_PATH` parameter is documented in the **Event Streams Certificates** section of the [Prerequisites](/infrastructure/required-services/#ibm-event-streams-on-redhat-openshift-container-platform) page. The Appsody run command should include a parameter similar to `-v /Users/myuser/Downloads/es-cert.jks:/config/resources/security/es-ssl/es-cert.jks` in its `--docker-options` string to run this microservice locally.\n\n**Example:** `appsody run --docker-options \"-v /Users/myuser/Downloads/es-cert.jks:/config/resources/security/es-ssl/es-cert.jks \" ...`\n\n### Running the microservice locally\n\nWhen running the microservice locally, you must specify all the required [deployment parameters](#deployment-parameters) from above as environment variables via the `--docker-options` flag being passed in from the Appsody CLI command.\n\n**Example:** `appsody run --docker-options \"-e KAFKA_BROKERS=remotebroker1:9092,remotebroker2:9092 -e KAFKA_SHIP_TOPIC_NAME=ships -e KAFKA_CONTAINER_TOPIC_NAME=containers -v /Users/myuser/Downloads/es-cert.jks:/config/resources/security/es-ssl/es-cert.jks\" ...`\n\nFor more details on running the microservice locally, consult the [Appsody run documentation](https://appsody.dev/docs/using-appsody/local-development#appsody-local-development) as well as the deployment information contained in the [`app-deploy.yaml`](https://github.com/ibm-cloud-architecture/refarch-kc-ms/blob/master/fleet-ms/app-deploy.yaml) file.\n\n### Running the microservice remotely\n\nThe [Appsody Operator](https://appsody.dev/docs/reference/appsody-operator/) is a required prerequisite for deploying the microservice to a remote Kubernetes or OpenShift cluster.\n\nTo deploy the microservice to a remote cluster:\n\n`appsody deploy <yournamespace>/kcontainer-fleet-ms[:tag] --no-build`\n\n- You can omit the `--no-build` flag to have Appsody perform a build before deploying the application.\n- _**Note:**_ Performing a build at deploy time requires specifying the absolute container reference path, as well as the `--push` flag.\n- The neccesary deployment parameter information will be read from the `app-deploy.yaml` file in the same directory.\n\n## Usage Details\n\n### REST APIs\n\n<InlineNotification kind=\"info\"><strong>TODO</strong> REST APIs documentation via Swagger</InlineNotification>\n\n<!--\n\n## Implementation Details\n\n## The model\n\nA fleet will have one to many ships. Fleet has id and name. Ship has ID, name, status, position, port and type. Ship carries containers. Container has id, and metrics like amp, temperature. Here is an example of JSON document illustrating this model:\n```json\n {\n    \"id\": \"f1\",\n    \"name\": \"KC-NorthAtlantic\",\n    \"ships\": [\n      {\n         \"name\": \"MarieRose\",\n        \"latitude\": \"37.8044\",\n        \"longitude\": \"-122.2711\",\n        \"status\": \"Docked\",\n        \"port\": \"Oakland\",\n        \"type\": \"Carrier\",\n        \"maxRow\": 3,\n        \"maxColumn\": 7,\n         \"numberOfContainers\" : 17,\n         \"containers\": [\n             {\"id\":\"c_2\",\"type\":\"Reefer\",\"temperature\":10,\"amp\":46,\"status\":\"RUN\",\"row\":0,\"column\":2,\"shipId\":\"MarieRose\"}\n         ],\n      }],\n}\n```\n\n## Code\n\nThe base of the project was created using IBM Microclimate using microprofile / Java EE template deployable in WebSphere Liberty. Once, the project template was generated, we applied a Test Driven Development approach to develop the application logic. But first let define some use stories we want to support in this simulator.\n\n### Code organization\n\nThe following package structure is used:\n* `ibm.labs.kc.model` for the domain specific model.\n* `ibm.labs.kc.app.kafka` kafka consumer and producer and config management.\n* `ibm.labs.kc.app.rest` set of REST resources with API definitions\n* `ibm.labs.kc.dao` data access object for ship and fleet. Use mockup no backend DB yet.\n* `ibm.labs.kc.event.model` event definitions for the kafka topic payload\n* `ibm.labs.kc.simulator` simulators for the demo as we do not have real ships... yet.\n\nThe most important properties are defined in the config.properties file under `src/main/resources`.\n\n### Test Driven Development\n\nTest driven development should be used to develop microservice as it helps to develop by contract and think about how each function should work from a client point of view. [This article](https://cloudcontent.mybluemix.net/cloud/garage/content/code/practice_test_driven_development) introduces the practice.\nTo apply TDD we want to describe our approach for this project, by starting by the tests.\n\n#### Start simple\n\nAs an example of TDD applied to this project, we want to test the \"get the list of fleets\" feature. As this code is built by iteration, the first iteration is to get the fleet definition and ships definition from files. The `src/main/resources` folder includes a json file to define the fleets. We do not need an external datasource for this mockup solution.\n\nThe json is an array of fleet definitions, something like:\n```json\n[\n  {\n    \"id\": \"f1\",\n    \"name\": \"KC-NorthAtlantic\",\n    \"ships\": [ ]\n  }\n]\n```\n\nSo starting from the test, we implemented in `src/test/java` the `TestReadingFleet` class to test a FleetService. The service will provide the business interface and it will use a data access object to go to the datasource.\n\nThe first test may look like the basic code below:\n\n```java\npublic void testGetAllFleets() {\n    FleetDAO dao = new FleetDAOMockup(\"Fleet.json\");\n\tFleetService serv = new FleetService(dao);\n    List<Fleet> f = serv.getFleets();\n\tAssert.assertNotNull(f);\n\tAssert.assertTrue(f.size() >= 1);\n}\n```\n\nAfter generating class placeholder and java interface, executing the test fails, and we need to implement the DAO and the service operation `getFleets()`. In the FleetService we simply delegate to the DAO.\n\n```java\npublic List<Fleet> getFleets() {\n\t\treturn new ArrayList<Fleet>(dao.getFleets());\n\t}\n```\n\nIn the future, we may want to filter out the ships or separate fleet from ship in different json files so some logic may be added in this `getFleets()` function. The DAO is defined via an interface, and we add a Factory to build DAO implementation depending on the configuration. The DAO implementation at first is loadding data from file.\n\nTo execute all the tests outside of the Eclipse IDE, we use the maven: `mvn test`.\n\nQuickly we can see that the DAO may be more complex than expected so we add unit tests for the DAO too. After 10, 15 minutes we have a service component and a DAO with Factory and Mockup implementation created and tested.\n\nThe Fleet service needs to be exposed as REST api, so we add the JAXRS annotations inside the service class to the method we want to expose.\n\n```java\n@Path(\"fleets\")\npublic class FleetService {\n\n    @GET\n    @Produces(MediaType.APPLICATION_JSON)\n\tpublic List<Fleet> getFleets() {}\n}\n```\n\nSo now if we want to test at the API level, we need to do integration tests. This is where **IBM Microclimate** is coming handy as it created a nice example with `HealthEndpointIT` test class to get us started. All integration tests are defined in the `it` java package so we can control the maven life cycle and execute the integration tests when the environment is ready. The `pom.xml` defines configuration using the `maven Failsafe Plugin` which is designed to run integration tests. This Maven plugin has four phases for running integration tests:\n\n* **pre-integration-test** for setting up the integration test environment.\n* **integration-test** for running the integration tests.\n* **post-integration-test** for tearing down the integration test environment.\n* **verify** for checking the results of the integration tests.\n\nThe pre-integration-test phase loads IBM Liberty server via another maven plugin: [liberty-maven-app-parent](https://github.com/WASdev/ci.maven/blob/master/docs/parent-pom.md) so that the API can be tested from the app server.\n\nTo execute the integration tests do a `mvn verify`.\n\nBy using the same code approach as `HealthEndpointIT` we created a `TestFleetAPIsIT` Junit test class.\n\nThe environment properties are set in the `pom.xml` file.\n\n```java\n    protected String port = System.getProperty(\"liberty.test.port\");\n\tprotected String warContext = System.getProperty(\"war.context\");\n    protected String baseUrl = \"http://localhost:\" + port + \"/\" + warContext;\n    // .... then get a HTTP client and perform a HTTP GET\n    Client client = ClientBuilder.newClient();\n\tInvocation.Builder invoBuild = client.target(url).request();\n    Response response = invoBuild.get();\n    String fleetsAsString=response.readEntity(String.class);\n    //..\n```\n\nIf you need to debug this test inside Eclipse, you need to start the liberty server as an external process by using `mvn liberty:run-server`.\n\nThe second logic we want to TDD is the simulation.\n\n#### Ship Simulator\n\nThe simulation of the different container events is done in the class `BadEventSimulator`. But this class is used in a Runner, the `ShipRunner`. The approach is to move the ship to the next position as defined in the separate csv file (named by the ship's name), then to send the new ship position, and the container metrics at that position as events. So the simulator uses two Kafka producers, one for the ship position and one for the container metrics.\nThe topic names are defined in the `src/main/resource/config.properties` as well as the Kafka parameters. If you did not configure your kafka server, we have a script to create those topics [here](https://github.com/ibm-cloud-architecture/refarch-kc/tree/master/scripts/createLocalTopics.sh)\n\nFrom a test point of view we want to create a simulation controller instance, call the service simulation operation and verify the impacted container:\n\n```java\n@Test\n\tpublic void validateContainerDown() {\n        serv =  new ShipService();\n\t\tShipSimulationControl ctl = new ShipSimulationControl(\"JimminyCricket\", ShipSimulationControl.REEFER_DOWN);\n\t\tctl.setNumberOfMinutes(1);\n\t\tResponse res = serv.performSimulation(ctl);\n        Ship s = (Ship)res.getEntity();\n        Assert.assertTrue(s.getContainers().get(0).get(3).getStatus().equals(Container.STATUS_DOWN));\n    }\n```\nEvent after adding the ShipSimulationControl Java Bean and the operation performSimulation into the service... we have a problem... How to unit tests without sending message to Kafka?.\n\nThe ShipRunner is a Runnable class and uses the `positionPublisher` and `containerPublisher` which are standard Kafka producers.\nHere is a code snippet for the `run()` method of the `ShipRunner`: The ship positions are loaded from a file in the class loader and then for each container in the boat, send metrics.\n\n```java\ntry  {\n    for (Position p : this.positions) {\n        // ships publish their position to a queue\n        ShipPosition sp = new ShipPosition(this.shipName,p.getLatitude(),p.getLongitude());\n        positionPublisher.publishShipPosition(sp);\n\n        // Then publish the state of their containers\n        for (List<Container> row :  ship.getContainers()) {\n            for (Container c : row) {\n                ContainerMetric cm = BadEventSimulator.buildContainerMetric(this.shipName,c,dateFormat.format(currentWorldTime));\n                containerPublisher.publishContainerMetric(cm);\n            }\n        }\n        currentWorldTime=modifyTime(currentWorldTime);\n        Thread.sleep(Math.round(this.numberOfMinutes*60000/this.positions.size()));\n    }\n} catch (InterruptedException e) {\n```\n\nSo to avoid using kafka for unit tests, we can use mockito to mockup the producers. We encourage to read this [Mockito tutorial](https://javacodehouse.com/blog/mockito-tutorial/) and [this one.](http://www.vogella.com/tutorials/Mockito/article.html#testing-with-mock-objects) to have some basic knowledge on how to use mockito. We added the following dependency in the `pom.xml`.\n\n```xml\n<dependency>\n    <groupId>org.mockito</groupId>\n    <artifactId>mockito-core</artifactId>\n    <version>2.23.4</version>\n    <scope>test</scope>\n</dependency>\n```\n\nAdd a constructor in `ShipRunner` so we can inject the producer. The test can use mockup at the simulator level or at the producer level. Here is an example in the unit test class of injecting for producer:\n```java\n @Mock\n static PositionPublisher positionPublisherMock;\n @Mock\n static ContainerPublisher containerPublisherMock;\n\n @Rule public MockitoRule mockitoRule = MockitoJUnit.rule();\n\n\n @Test\n public void validateContainerFire() {\n     // use dependency injection via constructor.\n    ShipRunner sr = new ShipRunner(positionPublisherMock, containerPublisherMock);\n\tShipSimulator s = new ShipSimulator(sr);\n    serv =  new ShipService(DAOFactory.buildOrGetShipDAOInstance(\"Fleet.json\"),s);\n    // ..\n    Response res = serv.performSimulation(ctl);\n}\n```\nNow the tests succeed and do not send any message to Kafka.\n\n### APIs definition\n\nWe can define the API using yaml file and generates code from there, but we are using a TDD approach we start by the code: so we need to add API annotations to get the Swagger generated for us. The MicroProfile OpenAPI specification provides a set of Java interfaces and programming models that allow Java developers to natively produce OpenAPI v3 documents from their JAX-RS applications. We added annotations to the resource classes to support API documentation. Here is an example of microprofile openapi annotations.\n\n```java\n@Operation(summary = \"Get fleet by fleet name\",description=\" Retrieve a fleet with ships from is unique name\")\n@APIResponses(\n    value = {\n        @APIResponse(\n            responseCode = \"404\",\n            description = \"fleet not found\",\n            content = @Content(mediaType = \"text/plain\")),\n        @APIResponse(\n            responseCode = \"200\",\n            description = \"fleet retrieved\",\n            content = @Content(mediaType = \"application/json\",\n            schema = @Schema(implementation = Fleet.class))) })\n\tpublic Fleet getFleetByName(\n\t\t\t@Parameter(\n\t\t            description = \"The fleetname to get ships data\",\n\t\t            required = true,\n\t\t            example = \"KC-NorthFleet\",\n\t\t            schema = @Schema(type = SchemaType.STRING))\n\t\t\t@PathParam(\"fleetName\") String fleetName) {\n            }\n```\n\nIn the Liberty configuration file: `src/main/liberty/server.xml` we added the following features:\n```\n<feature>jaxrs-2.0</feature>\n<feature>openapi-3.0</feature>\n<feature>restConnector-2.0</feature>\n```\nOnce the server is restarted, we first go to http://localhost:9080/api/explorer to access the API definitions and even we are able to test it:\n\n![](images/fleets-api.png)\n\nA summary of the operations defined for this simulator are:\n\n | API | Description |\n | --- | --- |\n | GET '/fleetms/fleets/' | Get the list of fleet |\n | GET '/fleetms/fleets/:fleetname' | Get the ships of a given fleet |\n | POST '/fleetms/fleets/simulate' | Start to simulate ships movements |\n | POST '/fleetms/ships/simulate' | Start to simulate ship movements and container metrics generation |\n\n![](images/fleetms-apis.png)\n\n## Running integration tests with Kafka\n\nBy adding simulation tests we need to have kafka running now. We have deployed Kafka and Zookeeper to Kubernetes on Docker Edge for Mac and are able to connect to `docker-for-desktop` cluster. We have described this type of deployment [in this note for Kafka](https://github.com/ibm-cloud-architecture/refarch-eda/blob/master/deployments/kafka/README.md) and [this note for zookeeper](https://github.com/ibm-cloud-architecture/refarch-eda/blob/master/deployments/zookeeper/README.md)\n\nAs an alternate you can use the docker image from [confluent.io](https://docs.confluent.io/current/installation/docker/docs/installation/single-node-client.html#single-node-basic) and docker-compose to start zookeeper and kafka single broker.\n\nWe use environment variables to control the configuration:\n\n  | Variable | Role | Values |\n  | --- | --- | --- |\n  | KAFKA_ENV | Define what Kafka to use | We propose 3 values: LOCAL, IBMCLOUD, ICP |\n  | KAFKA_BROKERS | IP addresses and port number of the n brokers configured in your environment | |\n\nThe pom.xml uses those variables to use the local kafka for the integration tests:\n\n```\n<configuration>\n        <environmentVariables>\n            <KAFKA_ENV>LOCAL</KAFKA_ENV>\n            <KAFKA_BROKERS>gc-kafka-0.gc-kafka-hl-svc.greencompute.svc.cluster.local:32224</KAFKA_BROKERS>\n        </environmentVariables>\n```\n\nOne interesting integration test is defined in the class `it.FireContainerSimulationIT.java` as it starts a Thread running a ContainerConsumer (bullet 1 in figure below) which uses Kafka api to get `Container events` (class `ibm.labs.kc.event.model.ContainerMetric`) from the `bluewaterContainer` topic, and then calls the POST HTTP end point (2): `http://localhost:9080/fleetms/ships/simulate` with a simulator control object (`ibm.labs.kc.dto.model.ShipSimulationControl`). The application is producing ship position events and container metrics events at each time slot (3). The consumer is getting multiple events (4) from the topic showing some containers are burning:\n\n```json\n{\"id\":\"c_2\",\"type\":\"Reefer\",\"temperature\":150,\"amp\":46,\"status\":\"FIRE\",\"row\":0,\"column\":2,\"shipId\":\"JimminyCricket\"},\n{\"id\":\"c_3\",\"type\":\"Reefer\",\"temperature\":150,\"amp\":42,\"status\":\"FIRE\",\"row\":0,\"column\":3,\"shipId\":\"JimminyCricket\"}\n```\n\n![](images/it-fire-containers.png)\n\nThe integration tests are executed with maven:\n```\nmvn verify\n```\n\n-->\n","type":"Mdx","contentDigest":"bee9646278988aead5d714a99e07ce72","counter":401,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"Shipping Fleet Management","description":"In-depth description of the Fleet Management microservice component of the Reefer Container Shipment solution reference implementation."},"exports":{},"rawBody":"---\ntitle: Shipping Fleet Management\ndescription: In-depth description of the Fleet Management microservice component of the Reefer Container Shipment solution reference implementation.\n---\n\n<PageDescription>\nThis microservice is responsible to support simulation of fleet of container carrier vessels. It used for demonstration purpose, but it is still using an event-driven microservice implementation approach. It supports the event, actors, and commands discovered during the event storming workshop and illustrated by the following figure for the \"ship actor\".\n</PageDescription>\n\n<AnchorLinks>\n  <AnchorLink>Overview</AnchorLink>\n  <AnchorLink>Build</AnchorLink>\n  <AnchorLink>Run</AnchorLink>\n  <AnchorLink>Usage Details</AnchorLink>\n</AnchorLinks>\n\n## Overview\n\n**Description:** The service exposes simple a REST API to support acquiring ship and fleet information, while also providing simulation control to emulate ship movements and container metrics event generation. When a ship leaves or enters a port, it will also generate the events as listed in the [Event Storming Analysis](/implementation/event-storming-analysis/) and [Domain-Driven Design](/implementation/domain-driven-design/) results.\n\n**Github repository:** [refarch-kc-ms](https://github.com/ibm-cloud-architecture/refarch-kc-ms)\n\n**Folder:** [fleet-ms](https://github.com/ibm-cloud-architecture/refarch-kc-ms/tree/master/fleet-ms)\n\n**Kafka topics consumed from:**\n\n- [Bluewater Container](/microservices/topic-details/#bluewater-container-topic)\n- [Bluewater Ship](/microservices/topic-details/#bluewater-ship-topic)\n- [Bluewater Problem](/microservices/topic-details/#bluewater-problem-topic)\n\n**Kafka topics produced to:**\n\n- [Bluewater Container](/microservices/topic-details/#bluewater-container-topic)\n- [Bluewater Ship](/microservices/topic-details/#bluewater-ship-topic)\n- [Bluewater Problem](/microservices/topic-details/#bluewater-problem-topic)\n\n**Events reacted to:**\n\n- [Container Metric Event](/microservices/event-details/#container-metric-event)\n- [Ship Position Event](/microservices/event-details/#ship-position-event)\n- [Bluewater Problem Event](/microservices/event-details/#bluewater-problem-event)\n\n**Events produced:**\n\n- [Container Metric Event](/microservices/event-details/#container-metric-event)\n- [Ship Position Event](/microservices/event-details/#ship-position-event)\n\n**EDA Patterns implemented:**\n\n- [Consume-transform-produce Loop](/implementation/consume-transform-produce/)\n\n## Build\n\nThis microservice is built using the [Appsody](https://appsody.dev/) development framework. The [Appsody CLI](https://appsody.dev/docs/installing/installing-appsody) is a required prerequisite for building the application locally.\n\nAppsody will build the application by pulling the contents of the Appsody Stack it is based on and then performing the local application build inside the containerized environment:\n\n`appsody build -t <yournamespace>/kcontainer-fleet-ms[:tag] [--push]`\n\n- You can optionally specify a container tag. If left blank, `latest` will be used.\n- You can optionally supply the `--push` flag to automatically push the built image to specified remote repository.\n\nPerforming an Appsody build will update the `app-deploy.yaml` file in the same directory with current information for the application image, labels, and annotations fields.\n\n## Run\n\n### Deployment parameters\n\nThe following deployment parameters are defined in the `app-deploy.yaml` file:\n\n| Name                                     | Required | Description                                                                                                            |\n|------------------------------------------|----------|------------------------------------------------------------------------------------------------------------------------|\n| KAFKA_BROKERS                            | YES      | Comma-separated list of Kafka brokers to connect to                                                                    |\n| KAFKA_APIKEY                             | NO       | API Key used to connect to SASL-secured Kafka brokers. This is required when connecting to IBM Event Streams clusters. |\n| TRUSTSTORE_ENABLED                       | NO       | Required to be set to `true` when connecting to IBM Event Streams on the IBM Cloud Pak for Integration (CP4I).         |\n| TRUSTSTORE_PATH                          | NO       | The local path to the required truststore file when connecting to IBM Event Streams on CP4I. See [**Volume Mounts**](#volume-mounts) below.  |\n| TRUSTSTORE_PWD                           | NO       | The password for the truststore file used for IBM Event Streams server verification.                                   |\n| KAFKA_SHIP_TOPIC_NAME                    | YES      | The topic name used for communication relating to the ship  entity.                                                    |\n| KAFKA_CONTAINER_TOPIC_NAME               | YES      | The topic name used for communication relating to the containers entity.                                               |\n| KAFKA_BW_PROBLEM_TOPIC_NAME              | YES      | The topic name used for communication relating to the bluewater problems domain.                                      |\n\n### Volume Mounts\n\nThe Fleet Management microservice requires up to one file to be injected at runtime for proper operation. As noted in the `TRUSTSTORE_PATH` parameter above, these files are SSL-based certificates which are required to verfiy the identity of the external service when calling it. These files are provided as `--docker-options \"-v host-src:container-dest ...\"` when running the microservice locally and as a Volume Mount when running the microservice on a Kubernetes cluster.\n\nThe `TRUSTSTORE_PATH` parameter is documented in the **Event Streams Certificates** section of the [Prerequisites](/infrastructure/required-services/#ibm-event-streams-on-redhat-openshift-container-platform) page. The Appsody run command should include a parameter similar to `-v /Users/myuser/Downloads/es-cert.jks:/config/resources/security/es-ssl/es-cert.jks` in its `--docker-options` string to run this microservice locally.\n\n**Example:** `appsody run --docker-options \"-v /Users/myuser/Downloads/es-cert.jks:/config/resources/security/es-ssl/es-cert.jks \" ...`\n\n### Running the microservice locally\n\nWhen running the microservice locally, you must specify all the required [deployment parameters](#deployment-parameters) from above as environment variables via the `--docker-options` flag being passed in from the Appsody CLI command.\n\n**Example:** `appsody run --docker-options \"-e KAFKA_BROKERS=remotebroker1:9092,remotebroker2:9092 -e KAFKA_SHIP_TOPIC_NAME=ships -e KAFKA_CONTAINER_TOPIC_NAME=containers -v /Users/myuser/Downloads/es-cert.jks:/config/resources/security/es-ssl/es-cert.jks\" ...`\n\nFor more details on running the microservice locally, consult the [Appsody run documentation](https://appsody.dev/docs/using-appsody/local-development#appsody-local-development) as well as the deployment information contained in the [`app-deploy.yaml`](https://github.com/ibm-cloud-architecture/refarch-kc-ms/blob/master/fleet-ms/app-deploy.yaml) file.\n\n### Running the microservice remotely\n\nThe [Appsody Operator](https://appsody.dev/docs/reference/appsody-operator/) is a required prerequisite for deploying the microservice to a remote Kubernetes or OpenShift cluster.\n\nTo deploy the microservice to a remote cluster:\n\n`appsody deploy <yournamespace>/kcontainer-fleet-ms[:tag] --no-build`\n\n- You can omit the `--no-build` flag to have Appsody perform a build before deploying the application.\n- _**Note:**_ Performing a build at deploy time requires specifying the absolute container reference path, as well as the `--push` flag.\n- The neccesary deployment parameter information will be read from the `app-deploy.yaml` file in the same directory.\n\n## Usage Details\n\n### REST APIs\n\n<InlineNotification kind=\"info\"><strong>TODO</strong> REST APIs documentation via Swagger</InlineNotification>\n\n<!--\n\n## Implementation Details\n\n## The model\n\nA fleet will have one to many ships. Fleet has id and name. Ship has ID, name, status, position, port and type. Ship carries containers. Container has id, and metrics like amp, temperature. Here is an example of JSON document illustrating this model:\n```json\n {\n    \"id\": \"f1\",\n    \"name\": \"KC-NorthAtlantic\",\n    \"ships\": [\n      {\n         \"name\": \"MarieRose\",\n        \"latitude\": \"37.8044\",\n        \"longitude\": \"-122.2711\",\n        \"status\": \"Docked\",\n        \"port\": \"Oakland\",\n        \"type\": \"Carrier\",\n        \"maxRow\": 3,\n        \"maxColumn\": 7,\n         \"numberOfContainers\" : 17,\n         \"containers\": [\n             {\"id\":\"c_2\",\"type\":\"Reefer\",\"temperature\":10,\"amp\":46,\"status\":\"RUN\",\"row\":0,\"column\":2,\"shipId\":\"MarieRose\"}\n         ],\n      }],\n}\n```\n\n## Code\n\nThe base of the project was created using IBM Microclimate using microprofile / Java EE template deployable in WebSphere Liberty. Once, the project template was generated, we applied a Test Driven Development approach to develop the application logic. But first let define some use stories we want to support in this simulator.\n\n### Code organization\n\nThe following package structure is used:\n* `ibm.labs.kc.model` for the domain specific model.\n* `ibm.labs.kc.app.kafka` kafka consumer and producer and config management.\n* `ibm.labs.kc.app.rest` set of REST resources with API definitions\n* `ibm.labs.kc.dao` data access object for ship and fleet. Use mockup no backend DB yet.\n* `ibm.labs.kc.event.model` event definitions for the kafka topic payload\n* `ibm.labs.kc.simulator` simulators for the demo as we do not have real ships... yet.\n\nThe most important properties are defined in the config.properties file under `src/main/resources`.\n\n### Test Driven Development\n\nTest driven development should be used to develop microservice as it helps to develop by contract and think about how each function should work from a client point of view. [This article](https://cloudcontent.mybluemix.net/cloud/garage/content/code/practice_test_driven_development) introduces the practice.\nTo apply TDD we want to describe our approach for this project, by starting by the tests.\n\n#### Start simple\n\nAs an example of TDD applied to this project, we want to test the \"get the list of fleets\" feature. As this code is built by iteration, the first iteration is to get the fleet definition and ships definition from files. The `src/main/resources` folder includes a json file to define the fleets. We do not need an external datasource for this mockup solution.\n\nThe json is an array of fleet definitions, something like:\n```json\n[\n  {\n    \"id\": \"f1\",\n    \"name\": \"KC-NorthAtlantic\",\n    \"ships\": [ ]\n  }\n]\n```\n\nSo starting from the test, we implemented in `src/test/java` the `TestReadingFleet` class to test a FleetService. The service will provide the business interface and it will use a data access object to go to the datasource.\n\nThe first test may look like the basic code below:\n\n```java\npublic void testGetAllFleets() {\n    FleetDAO dao = new FleetDAOMockup(\"Fleet.json\");\n\tFleetService serv = new FleetService(dao);\n    List<Fleet> f = serv.getFleets();\n\tAssert.assertNotNull(f);\n\tAssert.assertTrue(f.size() >= 1);\n}\n```\n\nAfter generating class placeholder and java interface, executing the test fails, and we need to implement the DAO and the service operation `getFleets()`. In the FleetService we simply delegate to the DAO.\n\n```java\npublic List<Fleet> getFleets() {\n\t\treturn new ArrayList<Fleet>(dao.getFleets());\n\t}\n```\n\nIn the future, we may want to filter out the ships or separate fleet from ship in different json files so some logic may be added in this `getFleets()` function. The DAO is defined via an interface, and we add a Factory to build DAO implementation depending on the configuration. The DAO implementation at first is loadding data from file.\n\nTo execute all the tests outside of the Eclipse IDE, we use the maven: `mvn test`.\n\nQuickly we can see that the DAO may be more complex than expected so we add unit tests for the DAO too. After 10, 15 minutes we have a service component and a DAO with Factory and Mockup implementation created and tested.\n\nThe Fleet service needs to be exposed as REST api, so we add the JAXRS annotations inside the service class to the method we want to expose.\n\n```java\n@Path(\"fleets\")\npublic class FleetService {\n\n    @GET\n    @Produces(MediaType.APPLICATION_JSON)\n\tpublic List<Fleet> getFleets() {}\n}\n```\n\nSo now if we want to test at the API level, we need to do integration tests. This is where **IBM Microclimate** is coming handy as it created a nice example with `HealthEndpointIT` test class to get us started. All integration tests are defined in the `it` java package so we can control the maven life cycle and execute the integration tests when the environment is ready. The `pom.xml` defines configuration using the `maven Failsafe Plugin` which is designed to run integration tests. This Maven plugin has four phases for running integration tests:\n\n* **pre-integration-test** for setting up the integration test environment.\n* **integration-test** for running the integration tests.\n* **post-integration-test** for tearing down the integration test environment.\n* **verify** for checking the results of the integration tests.\n\nThe pre-integration-test phase loads IBM Liberty server via another maven plugin: [liberty-maven-app-parent](https://github.com/WASdev/ci.maven/blob/master/docs/parent-pom.md) so that the API can be tested from the app server.\n\nTo execute the integration tests do a `mvn verify`.\n\nBy using the same code approach as `HealthEndpointIT` we created a `TestFleetAPIsIT` Junit test class.\n\nThe environment properties are set in the `pom.xml` file.\n\n```java\n    protected String port = System.getProperty(\"liberty.test.port\");\n\tprotected String warContext = System.getProperty(\"war.context\");\n    protected String baseUrl = \"http://localhost:\" + port + \"/\" + warContext;\n    // .... then get a HTTP client and perform a HTTP GET\n    Client client = ClientBuilder.newClient();\n\tInvocation.Builder invoBuild = client.target(url).request();\n    Response response = invoBuild.get();\n    String fleetsAsString=response.readEntity(String.class);\n    //..\n```\n\nIf you need to debug this test inside Eclipse, you need to start the liberty server as an external process by using `mvn liberty:run-server`.\n\nThe second logic we want to TDD is the simulation.\n\n#### Ship Simulator\n\nThe simulation of the different container events is done in the class `BadEventSimulator`. But this class is used in a Runner, the `ShipRunner`. The approach is to move the ship to the next position as defined in the separate csv file (named by the ship's name), then to send the new ship position, and the container metrics at that position as events. So the simulator uses two Kafka producers, one for the ship position and one for the container metrics.\nThe topic names are defined in the `src/main/resource/config.properties` as well as the Kafka parameters. If you did not configure your kafka server, we have a script to create those topics [here](https://github.com/ibm-cloud-architecture/refarch-kc/tree/master/scripts/createLocalTopics.sh)\n\nFrom a test point of view we want to create a simulation controller instance, call the service simulation operation and verify the impacted container:\n\n```java\n@Test\n\tpublic void validateContainerDown() {\n        serv =  new ShipService();\n\t\tShipSimulationControl ctl = new ShipSimulationControl(\"JimminyCricket\", ShipSimulationControl.REEFER_DOWN);\n\t\tctl.setNumberOfMinutes(1);\n\t\tResponse res = serv.performSimulation(ctl);\n        Ship s = (Ship)res.getEntity();\n        Assert.assertTrue(s.getContainers().get(0).get(3).getStatus().equals(Container.STATUS_DOWN));\n    }\n```\nEvent after adding the ShipSimulationControl Java Bean and the operation performSimulation into the service... we have a problem... How to unit tests without sending message to Kafka?.\n\nThe ShipRunner is a Runnable class and uses the `positionPublisher` and `containerPublisher` which are standard Kafka producers.\nHere is a code snippet for the `run()` method of the `ShipRunner`: The ship positions are loaded from a file in the class loader and then for each container in the boat, send metrics.\n\n```java\ntry  {\n    for (Position p : this.positions) {\n        // ships publish their position to a queue\n        ShipPosition sp = new ShipPosition(this.shipName,p.getLatitude(),p.getLongitude());\n        positionPublisher.publishShipPosition(sp);\n\n        // Then publish the state of their containers\n        for (List<Container> row :  ship.getContainers()) {\n            for (Container c : row) {\n                ContainerMetric cm = BadEventSimulator.buildContainerMetric(this.shipName,c,dateFormat.format(currentWorldTime));\n                containerPublisher.publishContainerMetric(cm);\n            }\n        }\n        currentWorldTime=modifyTime(currentWorldTime);\n        Thread.sleep(Math.round(this.numberOfMinutes*60000/this.positions.size()));\n    }\n} catch (InterruptedException e) {\n```\n\nSo to avoid using kafka for unit tests, we can use mockito to mockup the producers. We encourage to read this [Mockito tutorial](https://javacodehouse.com/blog/mockito-tutorial/) and [this one.](http://www.vogella.com/tutorials/Mockito/article.html#testing-with-mock-objects) to have some basic knowledge on how to use mockito. We added the following dependency in the `pom.xml`.\n\n```xml\n<dependency>\n    <groupId>org.mockito</groupId>\n    <artifactId>mockito-core</artifactId>\n    <version>2.23.4</version>\n    <scope>test</scope>\n</dependency>\n```\n\nAdd a constructor in `ShipRunner` so we can inject the producer. The test can use mockup at the simulator level or at the producer level. Here is an example in the unit test class of injecting for producer:\n```java\n @Mock\n static PositionPublisher positionPublisherMock;\n @Mock\n static ContainerPublisher containerPublisherMock;\n\n @Rule public MockitoRule mockitoRule = MockitoJUnit.rule();\n\n\n @Test\n public void validateContainerFire() {\n     // use dependency injection via constructor.\n    ShipRunner sr = new ShipRunner(positionPublisherMock, containerPublisherMock);\n\tShipSimulator s = new ShipSimulator(sr);\n    serv =  new ShipService(DAOFactory.buildOrGetShipDAOInstance(\"Fleet.json\"),s);\n    // ..\n    Response res = serv.performSimulation(ctl);\n}\n```\nNow the tests succeed and do not send any message to Kafka.\n\n### APIs definition\n\nWe can define the API using yaml file and generates code from there, but we are using a TDD approach we start by the code: so we need to add API annotations to get the Swagger generated for us. The MicroProfile OpenAPI specification provides a set of Java interfaces and programming models that allow Java developers to natively produce OpenAPI v3 documents from their JAX-RS applications. We added annotations to the resource classes to support API documentation. Here is an example of microprofile openapi annotations.\n\n```java\n@Operation(summary = \"Get fleet by fleet name\",description=\" Retrieve a fleet with ships from is unique name\")\n@APIResponses(\n    value = {\n        @APIResponse(\n            responseCode = \"404\",\n            description = \"fleet not found\",\n            content = @Content(mediaType = \"text/plain\")),\n        @APIResponse(\n            responseCode = \"200\",\n            description = \"fleet retrieved\",\n            content = @Content(mediaType = \"application/json\",\n            schema = @Schema(implementation = Fleet.class))) })\n\tpublic Fleet getFleetByName(\n\t\t\t@Parameter(\n\t\t            description = \"The fleetname to get ships data\",\n\t\t            required = true,\n\t\t            example = \"KC-NorthFleet\",\n\t\t            schema = @Schema(type = SchemaType.STRING))\n\t\t\t@PathParam(\"fleetName\") String fleetName) {\n            }\n```\n\nIn the Liberty configuration file: `src/main/liberty/server.xml` we added the following features:\n```\n<feature>jaxrs-2.0</feature>\n<feature>openapi-3.0</feature>\n<feature>restConnector-2.0</feature>\n```\nOnce the server is restarted, we first go to http://localhost:9080/api/explorer to access the API definitions and even we are able to test it:\n\n![](images/fleets-api.png)\n\nA summary of the operations defined for this simulator are:\n\n | API | Description |\n | --- | --- |\n | GET '/fleetms/fleets/' | Get the list of fleet |\n | GET '/fleetms/fleets/:fleetname' | Get the ships of a given fleet |\n | POST '/fleetms/fleets/simulate' | Start to simulate ships movements |\n | POST '/fleetms/ships/simulate' | Start to simulate ship movements and container metrics generation |\n\n![](images/fleetms-apis.png)\n\n## Running integration tests with Kafka\n\nBy adding simulation tests we need to have kafka running now. We have deployed Kafka and Zookeeper to Kubernetes on Docker Edge for Mac and are able to connect to `docker-for-desktop` cluster. We have described this type of deployment [in this note for Kafka](https://github.com/ibm-cloud-architecture/refarch-eda/blob/master/deployments/kafka/README.md) and [this note for zookeeper](https://github.com/ibm-cloud-architecture/refarch-eda/blob/master/deployments/zookeeper/README.md)\n\nAs an alternate you can use the docker image from [confluent.io](https://docs.confluent.io/current/installation/docker/docs/installation/single-node-client.html#single-node-basic) and docker-compose to start zookeeper and kafka single broker.\n\nWe use environment variables to control the configuration:\n\n  | Variable | Role | Values |\n  | --- | --- | --- |\n  | KAFKA_ENV | Define what Kafka to use | We propose 3 values: LOCAL, IBMCLOUD, ICP |\n  | KAFKA_BROKERS | IP addresses and port number of the n brokers configured in your environment | |\n\nThe pom.xml uses those variables to use the local kafka for the integration tests:\n\n```\n<configuration>\n        <environmentVariables>\n            <KAFKA_ENV>LOCAL</KAFKA_ENV>\n            <KAFKA_BROKERS>gc-kafka-0.gc-kafka-hl-svc.greencompute.svc.cluster.local:32224</KAFKA_BROKERS>\n        </environmentVariables>\n```\n\nOne interesting integration test is defined in the class `it.FireContainerSimulationIT.java` as it starts a Thread running a ContainerConsumer (bullet 1 in figure below) which uses Kafka api to get `Container events` (class `ibm.labs.kc.event.model.ContainerMetric`) from the `bluewaterContainer` topic, and then calls the POST HTTP end point (2): `http://localhost:9080/fleetms/ships/simulate` with a simulator control object (`ibm.labs.kc.dto.model.ShipSimulationControl`). The application is producing ship position events and container metrics events at each time slot (3). The consumer is getting multiple events (4) from the topic showing some containers are burning:\n\n```json\n{\"id\":\"c_2\",\"type\":\"Reefer\",\"temperature\":150,\"amp\":46,\"status\":\"FIRE\",\"row\":0,\"column\":2,\"shipId\":\"JimminyCricket\"},\n{\"id\":\"c_3\",\"type\":\"Reefer\",\"temperature\":150,\"amp\":42,\"status\":\"FIRE\",\"row\":0,\"column\":3,\"shipId\":\"JimminyCricket\"}\n```\n\n![](images/it-fire-containers.png)\n\nThe integration tests are executed with maven:\n```\nmvn verify\n```\n\n-->\n","fileAbsolutePath":"/home/runner/work/refarch-kc/refarch-kc/docs/src/pages/microservices/fleet/index.mdx"}}},"staticQueryHashes":["1054721580","1054721580","1364590287","2102389209","2102389209","2456312558","2746626797","3018647132","3037994772","768070550"]}