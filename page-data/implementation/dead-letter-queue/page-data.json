{"componentChunkName":"component---src-pages-implementation-dead-letter-queue-index-mdx","path":"/implementation/dead-letter-queue/","result":{"pageContext":{"frontmatter":{"title":"Dead Letter Queue Pattern","description":"Dead Letter Queue Pattern"},"relativePagePath":"/implementation/dead-letter-queue/index.mdx","titleType":"append","MdxNode":{"id":"1b7fca2b-f275-50d5-8018-1e61c70eb7af","children":[],"parent":"0353fdc0-dda3-5245-94bc-576f7032bf0a","internal":{"content":"---\ntitle: Dead Letter Queue Pattern\ndescription: Dead Letter Queue Pattern\n---\n\nIn distributed systems, it is common to find mechanisms for retrying calls to other, potentially external, services and fail gracefully if that service is unavailable for any reason. Here we are going to talk about using non-blocking request reprocessing and dead letter queues (DLQ) to achieve decoupled, observable error-handling without disrupting real-time traffic in the context of the [Container Anomaly Use Case](/implementation/container-anomaly/) of our Reefer Container Reference Application.\n\nA brief description about the problem and how non-blocking reprocessing and dead letter queues can help us can be found [here](https://ibm-cloud-architecture.github.io/refarch-eda/patterns/dlq/).\n\n## Implementation\n\nAs explained in the [Container Anomaly Use Case](/implementation/container-anomaly) of our Reefer Container Reference Application, the Spring Containers component calls a BPM process in order to get a field engineer assigned to checking and fixing a potentially bad container based on the telemetry events being sent by it.\n\nAs said in the introduction above, this call to the BPM process might fail so we need to implement a mechanism whereby we use non-blocking request reprocessing and dead letter queues (DLQ) to achieve decoupled, observable error-handling without disrupting real-time traffic.\n\nWe have done so by creating two new topics the Spring Container component will subscribe and publish to: **container-anomaly-retry** and **container-anomaly-dead**.\n\nWhen the Spring Container receives a **ContainerAnomaly** event (actually three of these to decrease the load on the BPM side), it will call the BPM service. This call is divided into two actions:\n\n1. **Authenticate** against BPM which will give you a token to\n2. **Call the BPM process** that will trigger the field engineer assignment process.\n\nAnother important thing to have in mind when implementing retry and graceful failure mechanisms is that it only makes sense to retry a call to a, potentially external, service when this service is temporarily unavailable (because it is temporarily overloaded, temporarily down, etc) and not when the call failed (due to bad data sent, bad url used, etc). This way we avoid extra load in the dependent service by not retrying calls we know that will fail again anyway beforehand.\n\nTo illustrate this, we have decided that any failure in authenticating with BPM is an error and that it does not make sense to retry since it will fail again due to bad Spring Container component configuration (wrong BPM credentials provided).\n\n### Service Unavailable\n\n![retry](./images/Slide1.png)\n\nAs said above, there might be some times when the service we depend on and call (BPM in our case) is temporarily not available. Either because it is temporarily down or overloaded. If this happens, the Spring Container component will send the **ContainerAnomaly** event into the **container-anomaly-retry** topic.\n\nThe Spring Container component will also subscribe to that topic and will retry the call to BPM for each **ContainerAnomalyRetry** event it reads from the **container-anomaly-retry** topic. However, it will only make the call to BPM after certain delay, so that we don't collapse the service with retries, which will increase based on the retry attempt number:\n\n```java\n@Override\npublic void onMessage(ConsumerRecord<Integer, String> message) {\n    if (message.value().contains(ContainerEvent.CONTAINER_ANOMALY_RETRY)) {\n        // --------------------------------------------\n        // ContainerAnomalyEventRetry events\n        //---------------------------------------------\n        LOG.info(\"Received new ContainerAnomalyEventRetry event: \" + message.value());\n        // Get the ContainerAnomalyRetryEvent objet from the event received\n        ContainerAnomalyEventRetry caer = parser.fromJson(message.value(), ContainerAnomalyEventRetry.class);\n        int time_to_wait = caer.getRetries() * 10;\n        LOG.info(\"This is a BPM call retry. Applying a delay of \" + time_to_wait + \" seconds\");\n        try {\n            Thread.sleep(time_to_wait * 1000);\n        } catch (InterruptedException e) {\n            e.printStackTrace();\n        }\n        bpmAgent.callBPM(caer,false);\n    }\n}\n```\n\nThe ContainerAnomalyEventRetry events sent to the container-anomaly-retry topic look like this:\n\n```json\n{\n  \"containerID\": \"1111\",\n  \"payload\": {\n    \"ambiant_temperature\": 19.8447,\n    \"carbon_dioxide_level\": 4.42579,\n    \"content_type\": 2,\n    \"defrost_cycle\": 6,\n    \"humidity_level\": 60.3148,\n    \"kilowatts\": 3.44686,\n    \"latitude\": 31.4,\n    \"longitude\": 121.5,\n    \"nitrogen_level\": 79.4046,\n    \"oxygen_level\": 20.4543,\n    \"target_temperature\": 6,\n    \"temperature\": 5.49647,\n    \"time_door_open\": 0.822024,\n    \"vent_1\": true,\n    \"vent_2\": true,\n    \"vent_3\": true\n  },\n  \"retries\": 1,\n  \"timestamp\": 1583752031,\n  \"type\": \"ContainerAnomalyRetry\"\n}\n```\n\nwhere you can see a new attribute called **retries** so that we keep the count of how many times we have called the BPM service. If the BPM service is still unreachable/unavailable after three attempts, the Spring Container component will eventually send a **ContainerAnomalyDead** event into the **container-anomaly-dead** topic, which should be somehow monitored to take the appropriate action based on the type of events that get published:\n\n```java\nprivate void toRetryTopic(ContainerAnomalyEvent cae){\n    ContainerAnomalyEventRetry caer;\n\n    // Add one retry to ContainerAnomalyEventRetry or create a new one\n    if (cae instanceof ContainerAnomalyEventRetry){\n        caer = (ContainerAnomalyEventRetry)cae;\n        caer.setRetries(caer.getRetries()+1);\n    }\n    else caer = new ContainerAnomalyEventRetry(cae,1);\n\n    if (caer.getRetries() > 3){\n        // send the event to the container anomaly dead queue\n        toDeadTopic(cae,\"No more BPM process retries left\");\n    }\n    else {\n        // Send the event to the container anomaly retry queue\n        LOG.info(\"Sending ContainerAnomalyEventRetry event for containerID: \" + cae.getContainerID() + \" to the container anomaly retry topic\");\n        containerAnomalyRetryProducer.emit(caer);\n    }\n}\n```\n\nand the ContainerAnomalyDead event looks like:\n\n```json\n{\n  \"containerID\": \"1111\",\n  \"payload\": {\n    \"ambiant_temperature\": 19.8447,\n    \"carbon_dioxide_level\": 4.42579,\n    \"content_type\": 2,\n    \"defrost_cycle\": 6,\n    \"humidity_level\": 60.3148,\n    \"kilowatts\": 3.44686,\n    \"latitude\": 31.4,\n    \"longitude\": 121.5,\n    \"nitrogen_level\": 79.4046,\n    \"oxygen_level\": 20.4543,\n    \"target_temperature\": 6,\n    \"temperature\": 5.49647,\n    \"time_door_open\": 0.822024,\n    \"vent_1\": true,\n    \"vent_2\": true,\n    \"vent_3\": true\n  },\n  \"reason\": \"No more BPM process retries left\",\n  \"timestamp\": 1583752031,\n  \"type\": \"ContainerAnomalyDead\"\n}\n```\n\nwhere you can see we have added the **reason** so that if any operator or automated system monitors this queue, they know what might be the problem.\n\n### Errors\n\n![error](./images/Slide2.png)\n\nAs said earlier, when implementing request retry and graceful failure mechanisms for a, potentially external, service, we want to retry only when the service is unreachable/unavailable and not when there is an error in the data we are calling the service with, an error with the service url or something on those lines so that we don't keep retrying calls we know they are bound to fail beforehand.\n\nAs a result, when this happens, we simply send the the event to the dead letter queue with a meaningful error message/code so that monitoring systems (either automated ones or operators monitoring these queues) can act accordingly.\n\nIn our case, we are considering an error any failed attempt to get authenticated against BPM, either due to bad credentials or login url, and consider that retrying those requests will only put more load onto the system. As a result, we simply send a **ContainerAnomalyDead** event into the **container-anomaly-dead** topic instead of retrying.\n\n```java\n// Manage BPM token\nif (bpm_token == \"\" || expired) {\n    // Logging\n    if (bpm_token == \"\") LOG.info(\"No BPM token - Calling the BPM authentication service\");\n    if (expired) LOG.info(\"BPM token expired - Calling the BPM authentication service\");\n    // Get the BPM token\n    try {\n        bpm_token = getBPMToken();\n    } catch (Exception ex) {\n        /*\n        * DEAD\n        * We Consider not being able to authenticate with BPM a severe problem as it does not\n        * make sense to retry calling a service which we can't authenticate against.\n        */\n        toDeadTopic(ContainerAnomalyEvent, ex.getMessage());\n    }\n}\n```\n\nIn this case, the ContainerAnomalyDead event looks exactly the same as in the service unavailable section but with a different reason, potentially indicating another problem this time.\n\n```json\n{\n  \"containerID\": \"1111\",\n  \"payload\": {\n    \"ambiant_temperature\": 19.8447,\n    \"carbon_dioxide_level\": 4.42579,\n    \"content_type\": 2,\n    \"defrost_cycle\": 6,\n    \"humidity_level\": 60.3148,\n    \"kilowatts\": 3.44686,\n    \"latitude\": 31.4,\n    \"longitude\": 121.5,\n    \"nitrogen_level\": 79.4046,\n    \"oxygen_level\": 20.4543,\n    \"target_temperature\": 6,\n    \"temperature\": 5.49647,\n    \"time_door_open\": 0.822024,\n    \"vent_1\": true,\n    \"vent_2\": true,\n    \"vent_3\": true\n  },\n  \"reason\": \"BPM authentication exception\",\n  \"timestamp\": 1583751647,\n  \"type\": \"ContainerAnomalyDead\"\n}\n```\n\n### What to do next\n\nAs already said, the idea of implementing the Request Retry and Dead Letter Queue Pattern is not only to alleviate the load an, external or not, system we depend on might have but also as a sink for potential problems. Whether ContainerAnomaly messages end up in the container-anomaly-dead topic because the BPM service was (temporarily or not) unavailable or there was an, expected or not, error in the system, this dead letter queue will ideally have some automated/manual monitoring system/person so that the appropriate action can be taken as a result in an attempt to minimize the incorrect functioning of your system/service. This is, for instance, one good reason to wrap up your messages being sent to the dead letter queue with some code/reason that can be quickly understood by whatever monitor system. Not only understood but possibly aggregated and queried to better understand your system performance over time.\n","type":"Mdx","contentDigest":"8b26577c90f122ae5526e811078118f4","counter":339,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"Dead Letter Queue Pattern","description":"Dead Letter Queue Pattern"},"exports":{},"rawBody":"---\ntitle: Dead Letter Queue Pattern\ndescription: Dead Letter Queue Pattern\n---\n\nIn distributed systems, it is common to find mechanisms for retrying calls to other, potentially external, services and fail gracefully if that service is unavailable for any reason. Here we are going to talk about using non-blocking request reprocessing and dead letter queues (DLQ) to achieve decoupled, observable error-handling without disrupting real-time traffic in the context of the [Container Anomaly Use Case](/implementation/container-anomaly/) of our Reefer Container Reference Application.\n\nA brief description about the problem and how non-blocking reprocessing and dead letter queues can help us can be found [here](https://ibm-cloud-architecture.github.io/refarch-eda/patterns/dlq/).\n\n## Implementation\n\nAs explained in the [Container Anomaly Use Case](/implementation/container-anomaly) of our Reefer Container Reference Application, the Spring Containers component calls a BPM process in order to get a field engineer assigned to checking and fixing a potentially bad container based on the telemetry events being sent by it.\n\nAs said in the introduction above, this call to the BPM process might fail so we need to implement a mechanism whereby we use non-blocking request reprocessing and dead letter queues (DLQ) to achieve decoupled, observable error-handling without disrupting real-time traffic.\n\nWe have done so by creating two new topics the Spring Container component will subscribe and publish to: **container-anomaly-retry** and **container-anomaly-dead**.\n\nWhen the Spring Container receives a **ContainerAnomaly** event (actually three of these to decrease the load on the BPM side), it will call the BPM service. This call is divided into two actions:\n\n1. **Authenticate** against BPM which will give you a token to\n2. **Call the BPM process** that will trigger the field engineer assignment process.\n\nAnother important thing to have in mind when implementing retry and graceful failure mechanisms is that it only makes sense to retry a call to a, potentially external, service when this service is temporarily unavailable (because it is temporarily overloaded, temporarily down, etc) and not when the call failed (due to bad data sent, bad url used, etc). This way we avoid extra load in the dependent service by not retrying calls we know that will fail again anyway beforehand.\n\nTo illustrate this, we have decided that any failure in authenticating with BPM is an error and that it does not make sense to retry since it will fail again due to bad Spring Container component configuration (wrong BPM credentials provided).\n\n### Service Unavailable\n\n![retry](./images/Slide1.png)\n\nAs said above, there might be some times when the service we depend on and call (BPM in our case) is temporarily not available. Either because it is temporarily down or overloaded. If this happens, the Spring Container component will send the **ContainerAnomaly** event into the **container-anomaly-retry** topic.\n\nThe Spring Container component will also subscribe to that topic and will retry the call to BPM for each **ContainerAnomalyRetry** event it reads from the **container-anomaly-retry** topic. However, it will only make the call to BPM after certain delay, so that we don't collapse the service with retries, which will increase based on the retry attempt number:\n\n```java\n@Override\npublic void onMessage(ConsumerRecord<Integer, String> message) {\n    if (message.value().contains(ContainerEvent.CONTAINER_ANOMALY_RETRY)) {\n        // --------------------------------------------\n        // ContainerAnomalyEventRetry events\n        //---------------------------------------------\n        LOG.info(\"Received new ContainerAnomalyEventRetry event: \" + message.value());\n        // Get the ContainerAnomalyRetryEvent objet from the event received\n        ContainerAnomalyEventRetry caer = parser.fromJson(message.value(), ContainerAnomalyEventRetry.class);\n        int time_to_wait = caer.getRetries() * 10;\n        LOG.info(\"This is a BPM call retry. Applying a delay of \" + time_to_wait + \" seconds\");\n        try {\n            Thread.sleep(time_to_wait * 1000);\n        } catch (InterruptedException e) {\n            e.printStackTrace();\n        }\n        bpmAgent.callBPM(caer,false);\n    }\n}\n```\n\nThe ContainerAnomalyEventRetry events sent to the container-anomaly-retry topic look like this:\n\n```json\n{\n  \"containerID\": \"1111\",\n  \"payload\": {\n    \"ambiant_temperature\": 19.8447,\n    \"carbon_dioxide_level\": 4.42579,\n    \"content_type\": 2,\n    \"defrost_cycle\": 6,\n    \"humidity_level\": 60.3148,\n    \"kilowatts\": 3.44686,\n    \"latitude\": 31.4,\n    \"longitude\": 121.5,\n    \"nitrogen_level\": 79.4046,\n    \"oxygen_level\": 20.4543,\n    \"target_temperature\": 6,\n    \"temperature\": 5.49647,\n    \"time_door_open\": 0.822024,\n    \"vent_1\": true,\n    \"vent_2\": true,\n    \"vent_3\": true\n  },\n  \"retries\": 1,\n  \"timestamp\": 1583752031,\n  \"type\": \"ContainerAnomalyRetry\"\n}\n```\n\nwhere you can see a new attribute called **retries** so that we keep the count of how many times we have called the BPM service. If the BPM service is still unreachable/unavailable after three attempts, the Spring Container component will eventually send a **ContainerAnomalyDead** event into the **container-anomaly-dead** topic, which should be somehow monitored to take the appropriate action based on the type of events that get published:\n\n```java\nprivate void toRetryTopic(ContainerAnomalyEvent cae){\n    ContainerAnomalyEventRetry caer;\n\n    // Add one retry to ContainerAnomalyEventRetry or create a new one\n    if (cae instanceof ContainerAnomalyEventRetry){\n        caer = (ContainerAnomalyEventRetry)cae;\n        caer.setRetries(caer.getRetries()+1);\n    }\n    else caer = new ContainerAnomalyEventRetry(cae,1);\n\n    if (caer.getRetries() > 3){\n        // send the event to the container anomaly dead queue\n        toDeadTopic(cae,\"No more BPM process retries left\");\n    }\n    else {\n        // Send the event to the container anomaly retry queue\n        LOG.info(\"Sending ContainerAnomalyEventRetry event for containerID: \" + cae.getContainerID() + \" to the container anomaly retry topic\");\n        containerAnomalyRetryProducer.emit(caer);\n    }\n}\n```\n\nand the ContainerAnomalyDead event looks like:\n\n```json\n{\n  \"containerID\": \"1111\",\n  \"payload\": {\n    \"ambiant_temperature\": 19.8447,\n    \"carbon_dioxide_level\": 4.42579,\n    \"content_type\": 2,\n    \"defrost_cycle\": 6,\n    \"humidity_level\": 60.3148,\n    \"kilowatts\": 3.44686,\n    \"latitude\": 31.4,\n    \"longitude\": 121.5,\n    \"nitrogen_level\": 79.4046,\n    \"oxygen_level\": 20.4543,\n    \"target_temperature\": 6,\n    \"temperature\": 5.49647,\n    \"time_door_open\": 0.822024,\n    \"vent_1\": true,\n    \"vent_2\": true,\n    \"vent_3\": true\n  },\n  \"reason\": \"No more BPM process retries left\",\n  \"timestamp\": 1583752031,\n  \"type\": \"ContainerAnomalyDead\"\n}\n```\n\nwhere you can see we have added the **reason** so that if any operator or automated system monitors this queue, they know what might be the problem.\n\n### Errors\n\n![error](./images/Slide2.png)\n\nAs said earlier, when implementing request retry and graceful failure mechanisms for a, potentially external, service, we want to retry only when the service is unreachable/unavailable and not when there is an error in the data we are calling the service with, an error with the service url or something on those lines so that we don't keep retrying calls we know they are bound to fail beforehand.\n\nAs a result, when this happens, we simply send the the event to the dead letter queue with a meaningful error message/code so that monitoring systems (either automated ones or operators monitoring these queues) can act accordingly.\n\nIn our case, we are considering an error any failed attempt to get authenticated against BPM, either due to bad credentials or login url, and consider that retrying those requests will only put more load onto the system. As a result, we simply send a **ContainerAnomalyDead** event into the **container-anomaly-dead** topic instead of retrying.\n\n```java\n// Manage BPM token\nif (bpm_token == \"\" || expired) {\n    // Logging\n    if (bpm_token == \"\") LOG.info(\"No BPM token - Calling the BPM authentication service\");\n    if (expired) LOG.info(\"BPM token expired - Calling the BPM authentication service\");\n    // Get the BPM token\n    try {\n        bpm_token = getBPMToken();\n    } catch (Exception ex) {\n        /*\n        * DEAD\n        * We Consider not being able to authenticate with BPM a severe problem as it does not\n        * make sense to retry calling a service which we can't authenticate against.\n        */\n        toDeadTopic(ContainerAnomalyEvent, ex.getMessage());\n    }\n}\n```\n\nIn this case, the ContainerAnomalyDead event looks exactly the same as in the service unavailable section but with a different reason, potentially indicating another problem this time.\n\n```json\n{\n  \"containerID\": \"1111\",\n  \"payload\": {\n    \"ambiant_temperature\": 19.8447,\n    \"carbon_dioxide_level\": 4.42579,\n    \"content_type\": 2,\n    \"defrost_cycle\": 6,\n    \"humidity_level\": 60.3148,\n    \"kilowatts\": 3.44686,\n    \"latitude\": 31.4,\n    \"longitude\": 121.5,\n    \"nitrogen_level\": 79.4046,\n    \"oxygen_level\": 20.4543,\n    \"target_temperature\": 6,\n    \"temperature\": 5.49647,\n    \"time_door_open\": 0.822024,\n    \"vent_1\": true,\n    \"vent_2\": true,\n    \"vent_3\": true\n  },\n  \"reason\": \"BPM authentication exception\",\n  \"timestamp\": 1583751647,\n  \"type\": \"ContainerAnomalyDead\"\n}\n```\n\n### What to do next\n\nAs already said, the idea of implementing the Request Retry and Dead Letter Queue Pattern is not only to alleviate the load an, external or not, system we depend on might have but also as a sink for potential problems. Whether ContainerAnomaly messages end up in the container-anomaly-dead topic because the BPM service was (temporarily or not) unavailable or there was an, expected or not, error in the system, this dead letter queue will ideally have some automated/manual monitoring system/person so that the appropriate action can be taken as a result in an attempt to minimize the incorrect functioning of your system/service. This is, for instance, one good reason to wrap up your messages being sent to the dead letter queue with some code/reason that can be quickly understood by whatever monitor system. Not only understood but possibly aggregated and queried to better understand your system performance over time.\n","fileAbsolutePath":"/home/runner/work/refarch-kc/refarch-kc/docs/src/pages/implementation/dead-letter-queue/index.mdx"}}}}