{"componentChunkName":"component---src-pages-aaa-orphans-deployments-troubleshooting-mdx","path":"/aaa_ORPHANS/deployments/troubleshooting/","result":{"pageContext":{"frontmatter":{"title":"Work in Progress","description":"Work in Progress"},"relativePagePath":"/aaa_ORPHANS/deployments/troubleshooting.mdx","titleType":"append","MdxNode":{"id":"1d9e9509-bdb6-59d2-bf7d-6e711061aa5a","children":[],"parent":"3eebaa75-d04f-55ef-b375-287700c08002","internal":{"content":"---\ntitle: Work in Progress\ndescription: Work in Progress\n---\n\n### Troubleshouting\n\nHere is a way to assess, within the kafka container, how the message arrived on a topic:\n\n```shell\n$ kubectl exec -ti rolling-streams-ibm-es-kafka-sts-0 -n streams bash\nnobody@rolling-streams-ibm-es-kafka-sts-0 $ cd /opt/kafka/bin\nnobody@rolling-streams-ibm-es-kafka-sts-0:/opt/kafka/bin$ ./kafka-console-consumer.sh --bootstrap-server rolling-streams-ibm-es-kafka-broker-svc-0.streams.svc.cluster.local:9092 --topic containers --from-beginning\n\n> {\"timestamp\": 1554338808, \"type\": \"ContainerAdded\", \"version\": \"1\", \"containerID\": \"c_0\", \"payload\": {\"containerID\": \"c_0\", \"type\": \"Reefer\", \"status\": \"atDock\", \"city\": \"Oakland\", \"brand\": \"brand-reefer\", \"capacity\": 100}}\n```\n\n\n#### No resolvable bootstrap urls given in bootstrap.servers\n\nIt is obvious reason, but when deploying a kafka consumer or producer on Kubernetes it is important to know which name to use. As the communication will be on the overlay network we should use the internal broker name. The name will be linked to the deployed configuration of Kafka or event streams. Here are a set of commands to debug that:\n\n```shell\n# Get the namespace name for the deployed kafka instance\n$ kubectl get namespaces\n> ...\n> streams Active <x>d\n# Look at the name of the services of the\n$ kubectl get svc -n streams\n> rolling-streams-ibm-es-kafka-broker-svc-0         ClusterIP   None           <none>        9092/TCP,8093/TCP,9094/TCP,7070/TCP                               40d\nrolling-streams-ibm-es-kafka-broker-svc-1         ClusterIP   None           <none>        9092/TCP,8093/TCP,9094/TCP,7070/TCP                               40d\nrolling-streams-ibm-es-kafka-broker-svc-2         ClusterIP   None           <none>        9092/TCP,8093/TCP,9094/TCP,7070/TCP\n# Verify the name lookup with busybox... deploy busybox\n$  kubectl apply -f https://k8s.io/examples/admin/dns/busybox.yaml\n$  kubectl exec -ti busybox -n default -- nslookup streams\n> Server:    10.0.0.10\nAddress 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local\n\nName:      streams\nAddress 1: 10.1.12.101 indexmgr.rolling-streams-ibm-es-indexmgr-svc.streams.svc.cluster.local\nAddress 2: 10.1.12.102 rolling-streams-ibm-es-elas-ad8d-0.rolling-streams-ibm-es-elastic-svc.streams.svc.cluster.local\nAddress 3: 10.1.31.245 rolling-streams-ibm-es-elas-ad8d-1.rolling-streams-ibm-es-elastic-svc.streams.svc.cluster.local\nAddress 4: 10.1.12.97 10-1-12-97.rolling-streams-ibm-es-kafka-broker-svc-1.streams.svc.cluster.local\nAddress 5: 10.1.12.88 10-1-12-88.rolling-streams-ibm-es-zookeeper-fixed-ip-svc-1.streams.svc.cluster.local\nAddress 6: 10.1.193.198 10-1-193-198.rolling-streams-ibm-es-zookeeper-fixed-ip-svc-2.streams.svc.cluster.local\nAddress 7: 10.1.31.249 10-1-31-249.rolling-streams-ibm-es-zookeeper-fixed-ip-svc-0.streams.svc.cluster.local\nAddress 8: 10.0.48.98 rolling-streams-ibm-es-rest-svc.streams.svc.cluster.local\nAddress 9: 10.0.104.180 rolling-streams-ibm-es-rest-proxy-svc.streams.svc.cluster.local\nAddress 10: 10.0.9.68 rolling-streams-ibm-es-zookeeper-fixed-ip-svc-2.streams.svc.cluster.local\nAddress 11: 10.1.193.253 10-1-193-253.rolling-streams-ibm-es-kafka-broker-svc-2.streams.svc.cluster.local\nAddress 12: 10.0.187.135 rolling-streams-ibm-es-zookeeper-fixed-ip-svc-0.streams.svc.cluster.local\nAddress 13: 10.1.31.223 10-1-31-223.rolling-streams-ibm-es-kafka-broker-svc-0.streams.svc.cluster.local\nAddress 14: 10.0.33.136 rolling-streams-ibm-es-ui-svc.streams.svc.cluster.local\nAddress 15: 10.0.182.152 rolling-streams-ibm-es-zookeeper-fixed-ip-svc-1.streams.svc.cluster.local\nAddress 16: 10.0.216.177 rolling-streams-ibm-es-proxy-svc.streams.svc.cluster.local\nAddress 17: 10.1.12.87 10-1-12-87.rolling-streams-ibm-es-access-controller-svc.streams.svc.cluster.local\nAddress 18: 10.1.31.251 10-1-31-251.rolling-streams-ibm-es-access-controller-svc.streams.svc.cluster.local\n```\nFor other DNS troubleshooting see [this product note.](https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/)\n\n#### Failed to verify broker certificate: self signed certificate\n\nWhen connecting a client to a deployed Kafka using the SSL protocol, there will be a SSL handcheck protocol done. The client needs to send security credentials using public keys and root certificates. The '.pem' file can be downloaded from Event stream console as `es-cert.pem`.\n\n![](es-connection.png)\n\nThe producer or consumer code needs to specify where to get the ssl certificates: Here is will be loaded from the local folder. But this file could be mounted into the docker image running the code to a folder referenced in this `ssl.ca.location`.\n\n```json\n    'bootstrap.servers':  KAFKA_BROKERS,\n    'security.protocol': 'SASL_SSL',\n    'ssl.ca.location': 'es-cert.pem',\n    'sasl.mechanisms': 'PLAIN',\n    'sasl.username': 'token',\n    'sasl.password': KAFKA_APIKEY,\n```\n\n#### LTPA configuration error. Unable to create or read LTPA key file: /opt/ibm/wlp/usr/servers/defaultServer/resources/security/ltpa.keys\n\nOur how to on ICP troublshouting:https://github.com/ibm-cloud-architecture/refarch-integration/blob/master/docs/icp/troubleshooting.md\n","type":"Mdx","contentDigest":"18578b8d3c03428cc1501ea6718bf345","counter":309,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"Work in Progress","description":"Work in Progress"},"exports":{},"rawBody":"---\ntitle: Work in Progress\ndescription: Work in Progress\n---\n\n### Troubleshouting\n\nHere is a way to assess, within the kafka container, how the message arrived on a topic:\n\n```shell\n$ kubectl exec -ti rolling-streams-ibm-es-kafka-sts-0 -n streams bash\nnobody@rolling-streams-ibm-es-kafka-sts-0 $ cd /opt/kafka/bin\nnobody@rolling-streams-ibm-es-kafka-sts-0:/opt/kafka/bin$ ./kafka-console-consumer.sh --bootstrap-server rolling-streams-ibm-es-kafka-broker-svc-0.streams.svc.cluster.local:9092 --topic containers --from-beginning\n\n> {\"timestamp\": 1554338808, \"type\": \"ContainerAdded\", \"version\": \"1\", \"containerID\": \"c_0\", \"payload\": {\"containerID\": \"c_0\", \"type\": \"Reefer\", \"status\": \"atDock\", \"city\": \"Oakland\", \"brand\": \"brand-reefer\", \"capacity\": 100}}\n```\n\n\n#### No resolvable bootstrap urls given in bootstrap.servers\n\nIt is obvious reason, but when deploying a kafka consumer or producer on Kubernetes it is important to know which name to use. As the communication will be on the overlay network we should use the internal broker name. The name will be linked to the deployed configuration of Kafka or event streams. Here are a set of commands to debug that:\n\n```shell\n# Get the namespace name for the deployed kafka instance\n$ kubectl get namespaces\n> ...\n> streams Active <x>d\n# Look at the name of the services of the\n$ kubectl get svc -n streams\n> rolling-streams-ibm-es-kafka-broker-svc-0         ClusterIP   None           <none>        9092/TCP,8093/TCP,9094/TCP,7070/TCP                               40d\nrolling-streams-ibm-es-kafka-broker-svc-1         ClusterIP   None           <none>        9092/TCP,8093/TCP,9094/TCP,7070/TCP                               40d\nrolling-streams-ibm-es-kafka-broker-svc-2         ClusterIP   None           <none>        9092/TCP,8093/TCP,9094/TCP,7070/TCP\n# Verify the name lookup with busybox... deploy busybox\n$  kubectl apply -f https://k8s.io/examples/admin/dns/busybox.yaml\n$  kubectl exec -ti busybox -n default -- nslookup streams\n> Server:    10.0.0.10\nAddress 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local\n\nName:      streams\nAddress 1: 10.1.12.101 indexmgr.rolling-streams-ibm-es-indexmgr-svc.streams.svc.cluster.local\nAddress 2: 10.1.12.102 rolling-streams-ibm-es-elas-ad8d-0.rolling-streams-ibm-es-elastic-svc.streams.svc.cluster.local\nAddress 3: 10.1.31.245 rolling-streams-ibm-es-elas-ad8d-1.rolling-streams-ibm-es-elastic-svc.streams.svc.cluster.local\nAddress 4: 10.1.12.97 10-1-12-97.rolling-streams-ibm-es-kafka-broker-svc-1.streams.svc.cluster.local\nAddress 5: 10.1.12.88 10-1-12-88.rolling-streams-ibm-es-zookeeper-fixed-ip-svc-1.streams.svc.cluster.local\nAddress 6: 10.1.193.198 10-1-193-198.rolling-streams-ibm-es-zookeeper-fixed-ip-svc-2.streams.svc.cluster.local\nAddress 7: 10.1.31.249 10-1-31-249.rolling-streams-ibm-es-zookeeper-fixed-ip-svc-0.streams.svc.cluster.local\nAddress 8: 10.0.48.98 rolling-streams-ibm-es-rest-svc.streams.svc.cluster.local\nAddress 9: 10.0.104.180 rolling-streams-ibm-es-rest-proxy-svc.streams.svc.cluster.local\nAddress 10: 10.0.9.68 rolling-streams-ibm-es-zookeeper-fixed-ip-svc-2.streams.svc.cluster.local\nAddress 11: 10.1.193.253 10-1-193-253.rolling-streams-ibm-es-kafka-broker-svc-2.streams.svc.cluster.local\nAddress 12: 10.0.187.135 rolling-streams-ibm-es-zookeeper-fixed-ip-svc-0.streams.svc.cluster.local\nAddress 13: 10.1.31.223 10-1-31-223.rolling-streams-ibm-es-kafka-broker-svc-0.streams.svc.cluster.local\nAddress 14: 10.0.33.136 rolling-streams-ibm-es-ui-svc.streams.svc.cluster.local\nAddress 15: 10.0.182.152 rolling-streams-ibm-es-zookeeper-fixed-ip-svc-1.streams.svc.cluster.local\nAddress 16: 10.0.216.177 rolling-streams-ibm-es-proxy-svc.streams.svc.cluster.local\nAddress 17: 10.1.12.87 10-1-12-87.rolling-streams-ibm-es-access-controller-svc.streams.svc.cluster.local\nAddress 18: 10.1.31.251 10-1-31-251.rolling-streams-ibm-es-access-controller-svc.streams.svc.cluster.local\n```\nFor other DNS troubleshooting see [this product note.](https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/)\n\n#### Failed to verify broker certificate: self signed certificate\n\nWhen connecting a client to a deployed Kafka using the SSL protocol, there will be a SSL handcheck protocol done. The client needs to send security credentials using public keys and root certificates. The '.pem' file can be downloaded from Event stream console as `es-cert.pem`.\n\n![](es-connection.png)\n\nThe producer or consumer code needs to specify where to get the ssl certificates: Here is will be loaded from the local folder. But this file could be mounted into the docker image running the code to a folder referenced in this `ssl.ca.location`.\n\n```json\n    'bootstrap.servers':  KAFKA_BROKERS,\n    'security.protocol': 'SASL_SSL',\n    'ssl.ca.location': 'es-cert.pem',\n    'sasl.mechanisms': 'PLAIN',\n    'sasl.username': 'token',\n    'sasl.password': KAFKA_APIKEY,\n```\n\n#### LTPA configuration error. Unable to create or read LTPA key file: /opt/ibm/wlp/usr/servers/defaultServer/resources/security/ltpa.keys\n\nOur how to on ICP troublshouting:https://github.com/ibm-cloud-architecture/refarch-integration/blob/master/docs/icp/troubleshooting.md\n","fileAbsolutePath":"/home/runner/work/refarch-kc/refarch-kc/docs/src/pages/aaa_ORPHANS/deployments/troubleshooting.mdx"}}}}