{"componentChunkName":"component---src-pages-infrastructure-required-services-index-mdx","path":"/infrastructure/required-services/","result":{"pageContext":{"frontmatter":{"title":"Required Services","description":"Deployment details for dependent services required by the Reefer Container Shipment solution reference implementation."},"relativePagePath":"/infrastructure/required-services/index.mdx","titleType":"append","MdxNode":{"id":"d6932633-d5bf-5741-858d-43ed2b76c141","children":[],"parent":"a59519d9-acd3-56b9-8108-bfe8143402b2","internal":{"content":"---\ntitle: Required Services\ndescription: Deployment details for dependent services required by the Reefer Container Shipment solution reference implementation.\n---\n\n<InlineNotification kind=\"warning\"><strong>TODO</strong> - UNDER CONSTRUCTION</InlineNotification>\n\n<AnchorLinks>\n  <AnchorLink>Development Tools</AnchorLink>\n  <AnchorLink>Apache Kafka</AnchorLink>\n  <AnchorLink>Postgresql</AnchorLink>\n  <AnchorLink>BPM</AnchorLink>\n</AnchorLinks>\n\n## Development Tools\n\nYou will require the following tools locally on your development system:\n- [Git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)\n- [Maven](https://maven.apache.org/index.html)\n- [Docker](https://www.docker.com/products/docker-desktop)\n- [Appsody](https://appsody.dev/)\n- Programming Languages & Compilers:\n    - [Java JDK](https://openjdk.java.net/) - version 1.8 or greater\n    - [NodeJS](https://nodejs.org/en/) - version 12.18 or greater\n    - [Python](https://www.python.org/downloads/) - version 3.7 or greater\n\n### Clone all the repositories\n\nStart by cloning the root repository using the command:\n\n```\ngit clone https://github.com/ibm-cloud-architecture/refarch-kc/\n```\n\nThen go to the `refarch-kc` folder and use the command:\n\n```\n./script/clone.sh\n```\n\nto get all the solution repositories. You should have at least the following repositories:\n\n```\nrefarch-kc-container-ms\nrefarch-kc-order-ms\nrefarch-kc-ui\nrefarch-kc\nrefarch-kc-ms\nrefarch-kc-streams\n```\n\nThen modify the environment variables according to your environment you are using. This file is used by a lot of scripts in the solution to set the target deployment environment: LOCAL, IBMCLOUD, ICP, MINIKUBE.\n\n## Apache Kafka\n\nThere are multiple options to deploy an Apache Kafka-based cluster to support this reference implementation:\n\n<AnchorLinks small>\n  <AnchorLink>IBM Event Streams on IBM Cloud</AnchorLink>\n  <AnchorLink>IBM Event Streams on RedHat OpenShift Container Platform</AnchorLink>\n  <AnchorLink>Apache Kafka via Strimzi Operator</AnchorLink>\n</AnchorLinks>\n\n### IBM Event Streams on IBM Cloud\n\n#### Service Deployment\n\nWe recommend to follow [our most recent lab](https://ibm-cloud-architecture.github.io/refarch-eda/technology/event-streams/es-cloud/) on how to provision an Event Streams intance on cloud.\n\n* In the *Manage* panel add the topics needed for the solution. We need at least the following:\n\n ![](images/IES-IC-topics.png)\n\n* In the Service Credentials tab, create new credentials to get the Kafka broker list, the admim URL and the api_key needed to authenticate the consumers or producers.\n\n ![](images/IES-IC-credentials.png)\n\n#### Kafka Brokers\n\nRegardless of specific deployment targets (OCP, IKS, k8s), the following prerequisite Kubernetes ConfigMap needs to be created to support the deployments of the application's microservices.  These artifacts need to be created once per unique deployment of the entire application and can be shared between application components in the same overall application deployment.  These values can be acquired from the `kafka_brokers_sasl` section of the service instance's Service Credentials.\n\n```shell\nkubectl create configmap kafka-brokers --from-literal=brokers='<replace with comma-separated list of brokers>' -n <target k8s namespace / ocp project>\nkubectl describe configmap kafka-brokers -n <target k8s namespace / ocp project>\n```\n\n#### Event Streams User Credentials\n\nThe Event Streams User Credentials are needed in order for any deployed consumers or producers to work with the IBM Event Streams service in IBM Cloud. To avoid sharing security keys, create a Kubernetes Secret in the target cluster you will deploy the application microservices to. This is available from the Service Credentials information you just created above.\n\n```shell\nkubectl create secret generic eventstreams-cred --from-literal=username='token' --from-literal=password='<replace with api key>' -n <target k8s namespace / ocp project>\nkubectl describe secret eventstreams-cred -n <target k8s namespace / ocp project>\n```\n\n### IBM Event Streams on RedHat OpenShift Container Platform\n\n#### Service Deployment\n\nThe installation is documented in the [product documentation](https://ibm.github.io/event-streams/installing/installing-openshift/) and in our [own note here.](https://ibm-cloud-architecture.github.io/refarch-eda/deployments/eventstreams/)\n\n#### Kafka Brokers\n\nRegardless of specific deployment targets (OCP, IKS, k8s), the following prerequisite Kubernetes ConfigMap needs to be created to support the deployments of the application's microservices.  These artifacts need to be created once per unique deployment of the entire application and can be shared between application components in the same overall application deployment.\n\n```shell\nkubectl create configmap kafka-brokers --from-literal=brokers='<replace with comma-separated list of brokers>' -n <target k8s namespace / ocp project>\nkubectl describe configmap kafka-brokers -n <target k8s namespace / ocp project>\n```\n\n#### Event Streams User Credentials\n\nThe Event Streams Scram User Credentials are needed in order for any deployed consumers or producers to work with the IBM Event Streams instance running in your cluster. These SCRAM credentials are associated to the KafkaUser object that is being created behind the scenes. In order to create that KafkaUser object and obtain the SCRAM credentials for it, follow the instructions at <https://ibm.github.io/event-streams/security/managing-access/#creating-a-kafkauser-in-the-ibm-event-streams-ui>\n\nTo avoid sharing security keys, create a Kubernetes Secret in the target cluster you will deploy the application microservices to:\n\n```shell\nkubectl create secret generic eventstreams-cred --from-literal=username='<replace with scram username>' --from-literal=password='<replace with scram password>' -n <target k8s namespace / ocp project>\nkubectl describe secrets -n <target k8s namespace / ocp project>\n```\n\n**IMPORTANT:** Our reference application uses idempotent producers which our KafkaUser needs a special set of permissions for. In, order to add these, edit the KafkaUser created when you generatted the SCRAM credentials following the link above in this section and add the following set of permissions in the `acls` list:\n\n```yaml\n- host: '*'\n  operation: IdempotentWrite\n  resource:\n    name: '*'\n    patternType: literal\n    type: cluster\n```\n\n#### Event Streams Certificates\n\nIf you are using Event Streams as your Kafka broker provider and it is deployed via the IBM Cloud Pak for Integration (ICP4I), you will need to create an additional Secret to store the generated Certificates & Truststores to connect securely between your application components and the Kafka brokers.  These artifacts need to be created once per unique deployment of the entire application and can be shared between application components in the same overall application deployment.\n\nFrom the **Connect to this cluster** tab on the landing page of your Event Streams installation, download both the **PKCS12** and the **PEM** certificates. After you have downloaded these, we are going to make them available to our producer and consumers by storing them in Kubernetes Secrets:\n\n```shell\noc create secret generic eventstreams-truststore --from-file=<path to downloaded es-cert.p12>\noc create secret generic eventstreams-cert-pem --from-file=<path to downloaded es-cert.pem>\n```\n\n### Apache Kafka via Strimzi Operator\n\n<InlineNotification kind=\"warning\"><strong>TODO</strong> - Update Strimzi documentation</InlineNotification>\n\nIf you simply want to deploy Kafka using the open source, community-supported Helm Charts, you can do so with the following commands.\n\n## Postgresql\n\nThe [Container Manager microservice](https://github.com/ibm-cloud-architecture/refarch-kc-container-ms/) persists the Reefer Container inventory in a Postgresql database.  The deployment of Postgresql is only necessary to support the deployment of the Container Manager microservice.  If you are not deploying the Container Manager microservice, you do not need to deploy and configure a Postgresql service and database.\n\nThe options to support the Container Manager microservice with a Postgresql database are:\n<AnchorLinks small>\n  <AnchorLink>Postgresql on IBM Cloud</AnchorLink>\n  <AnchorLink>Community-based Postgresql Helm charts</AnchorLink>\n</AnchorLinks>\n\n### Postgresql on IBM Cloud\n\n#### Service Deployment\n\n To install the service, follow the [product documentation here](https://cloud.ibm.com/catalog/services/databases-for-postgresql).\n\n Once the service is deployed, you need to create some service credentials and retreive the following values for the different configurations:\n\n * `postgres.username`\n * `postgres.password`\n * `postgres.composed`, which will need to be mapped to a JDBC URL in the format of `jdbc:postgresql://<hostname>:<port>/<database-name>?sslmode=verify-full&sslfactory=org.postgresql.ssl.NonValidatingFactory` _(this will remove the `username` and `password` values from the default `composed` string)_\n\n ![](images/postgres-credentials.png)\n\n#### Creating Postgresql credentials as Kubernetes Secrets\n\n1. Applying the same approach as above, copy the Postgresql URL as defined in the Postegresql service credential and execute the following command:\n```shell\nkubectl create secret generic postgresql-url --from-literal=binding='<replace with postgresql-url>' -n <target k8s namespace / ocp project>\n```\n\n2. For the user:\n```shell\nkubectl create secret generic postgresql-user --from-literal=binding='ibm_cloud_c...' -n <target k8s namespace / ocp project>\n```\n\n3. For the user password:\n```shell\nkubectl create secret generic postgresql-pwd --from-literal=binding='<password from the service credential>.' -n <target k8s namespace / ocp project>\n```\n\n4. When running Postgresql through the IBM Cloud service, additional SSL certificates are required to communicate securely:\n    1. Install the IBM Cloud Database CLI Plugin:\n   ```shell\n   ibmcloud plugin install cloud-databases\n   ```\n    2. Get the certificate using the name of the postgresql service:\n  ```shell\n  ibmcloud cdb deployment-cacert $IC_POSTGRES_SERV > postgresql.crt\n  ```\n    3. Then add it into an environment variable\n  ```shell\n  export POSTGRESQL_CA_PEM=\"$(cat ./postgresql.crt)\"\n  ```\n    4. Then define a secret:\n  ```shell\n  kubectl create secret generic postgresql-ca-pem --from-literal=binding=\"$POSTGRESQL_CA_PEM\" -n browncompute\n  ```\n\n### Community-based Postgresql Helm charts\n\nIf you simply want to deploy Postgresql using the open source, community-supported Helm Charts, you can do so with the following commands.\n\n#### Environment Considerations\n\nReference [Application Components Pre-reqs](application-components.md#openshift-container-platform-3-11) for details on creating the necessary ServiceAccount with required permissions, prior to deployment.\n\n#### Service Deployment\n\n1. Add Bitnami Helm Repository:\n```shell\nhelm repo add bitnami https://charts.bitnami.com/bitnami\n```\n\n2. Update the Helm repository:\n```shell\nhelm repo update\n```\n\n3. Create a Kubernetes Namespace or OpenShift Project (if not already created).\n```shell\nkubectl create namespace <target namespace>\n```\n4. Deploy Postgresql using the `bitnami/postgresql` Helm Chart:\n```shell\nmkdir bitnami\nmkdir templates\nhelm fetch --untar --untardir bitnami bitnami/postgresql\nhelm template --name postgre-db --set postgresqlPassword=supersecret \\\n  --set persistence.enabled=false --set serviceAccount.enabled=true \\\n  --set serviceAccount.name=<existing service account> bitnami/postgresql \\\n  --namespace <target namespace> --output-dir templates\nkubectl apply -f templates/postgresql/templates\n```\n  It will take a few minutes to get the pods ready.\n\n#### Creating Postgresql credentials as Kubernetes Secrets\n\n* The `postgresql-url` needs to point to the in-cluster (non-headless) Kubernetes Service created as part of the deployment and should take the form of the deployment name with the suffix of `-postgresql`:\n\n ```shell\n kubectl get services | grep postgresql | grep -v headless\n kubectl create secret generic postgresql-url --from-literal=binding='jdbc:postgresql://<helm-release-name>-postgresql' -n <target k8s namespace / ocp project>\n ```\n\n* For the user:\n\n ```shell\n kubectl create secret generic postgresql-user --from-literal=binding='postgres' -n <target k8s namespace / ocp project>\n ```\n\n* For the user password:\n\n ```shell\n kubectl create secret generic postgresql-pwd --from-literal=binding='<password used in the helm template command>.' -n <target k8s namespace / ocp project>\n ```\n\n#### Service Debugging & Troubleshooting\n\nAccess to the in-container password can be made using the following command.  This should be the same value you passed in when you deployed the service.\n\n```shell\nexport POSTGRES_PASSWORD=$(kubectl get secret --namespace  <target namespace> postgre-db-postgresql -o jsonpath=\"{.data.postgresql-password}\" | base64 --decode)\n```\n\nAnd then use the `psql` command line interface to interact with postgresql. For that, we use a Docker image as a client to the Postgresql server:\n\n```shell\nkubectl run postgre-db-postgresql-client --rm --tty -i --restart='Never' --namespace <target namespace> --image bitnami/postgresql:11.3.0-debian-9-r38 --env=\"PGPASSWORD=$POSTGRES_PASSWORD\" --command -- psql --host postgre-db-postgresql -U postgres -p 5432\n```\n\nTo connect to your database from outside the cluster execute the following commands:\n\n```shell\nkubectl port-forward --namespace <target namespace> svc/postgre-db-postgresql 5432:5432 &&\\\n  PGPASSWORD=\"$POSTGRES_PASSWORD\" psql --host 127.0.0.1 -U postgres -p 5432\n```\n\n## BPM\n\nThe containers microservice component of this Reefer Container EDA reference application can be integrated with a BPM process for the the maintenance of the containers. This BPM process will dispatch a field engineer so that the engineer can go to the reefer container to fix it. The process of scheduling an engineer and then completing the work can best be facilitated through a process based, structured workflow. We will be using IBM BPM on Cloud or Cloud Pak for Automation to best demonstrate the workflow. This workflow can be explored in detail [here](https://github.com/ibm-cloud-architecture/refarch-reefer-ml/tree/master/docs/bpm).\n\n<AnchorLinks small>\n  <AnchorLink>IBM BPM on IBM Cloud</AnchorLink>\n  <AnchorLink>IBM BPM on RedHat OpenShift Container Platform</AnchorLink>\n</AnchorLinks>\n\nIn order for the containers microservice to fire the BPM workflow, we need to provide the following information through Kubernetes configMaps and secrets:\n\n1. Provide in a configMap:\n   * the **BPM authentication login endpoint**\n   * the **BPM workflow endpoint**\n   * the **BPM anomaly event threshold**\n   * the **BPM authentication token time expiration**\n\n   ```shell\n   cat <<EOF | kubectl apply -f -\n   apiVersion: v1\n   kind: ConfigMap\n   metadata:\n     name: bpm-anomaly\n   data:\n     url: <replace with your BPM workflow endpoint>\n     login: <replace with your BPM authentication endpoint>\n     expiration: <replace with the number of second for the auth token to expire after>\n     anomalyThreshold: <replace with the number of anomaly events to receive before calling BPM>\n   EOF\n   ```\n\n2. Provide your BPM instance's **credentials** in a secret:\n\n   ```shell\n   kubectl create secret generic bpm-anomaly --from-literal=user='<replace with your BPM user>' --from-literal=password='<replace with your BPM password>' -n <target k8s namespace / ocp project>\n   kubectl describe secrets -n <target k8s namespace / ocp project>\n   ```\n\n**IMPORTANT:** The names for both the secret and configMap (`bpm-anomaly`) is the default the container microservice uses in its [helm chart](https://github.com/ibm-cloud-architecture/refarch-kc-container-ms/tree/master/SpringContainerMS/chart/springcontainerms). Make sure the name for the configMap and secret you create **match** the names you used in the containers microservice's helm chart.\n\nIf you do not have access to any BPM instance with this field engineer dispatching workflow, you can bypass the call to BPM by disabling such call in the container microservice component. For doing so, you can use the following container microservice's API endpoints:\n\n1. Enable BPM: [`http://<container_microservice_endpoint>/bpm/enable`](#bpm)\n2. Disable BPM: [`http://<container_microservice_endpoint>/bpm/disable`](#bpm)\n3. BPM status: [`http://<container_microservice_endpoint>/bpm/status`](#bpm)\n\nwhere `<container_microservice_endpoint>` is the route, ingress or nodeport service you associated to your container microservice component at deployment time.\n\n### IBM BPM on IBM Cloud\n\n_To be completed_\n\n**Reference:** [https://www.bpm.ibmcloud.com/](https://www.bpm.ibmcloud.com/)\n\n### IBM BPM on RedHat OpenShift Container Platform\n\n_To be completed_\n\n**Reference:** [https://www.ibm.com/cloud/cloud-pak-for-automation/](https://www.ibm.com/cloud/cloud-pak-for-automation/)\n","type":"Mdx","contentDigest":"58414b4c74218abcad7d4e355a8fb677","counter":365,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"Required Services","description":"Deployment details for dependent services required by the Reefer Container Shipment solution reference implementation."},"exports":{},"rawBody":"---\ntitle: Required Services\ndescription: Deployment details for dependent services required by the Reefer Container Shipment solution reference implementation.\n---\n\n<InlineNotification kind=\"warning\"><strong>TODO</strong> - UNDER CONSTRUCTION</InlineNotification>\n\n<AnchorLinks>\n  <AnchorLink>Development Tools</AnchorLink>\n  <AnchorLink>Apache Kafka</AnchorLink>\n  <AnchorLink>Postgresql</AnchorLink>\n  <AnchorLink>BPM</AnchorLink>\n</AnchorLinks>\n\n## Development Tools\n\nYou will require the following tools locally on your development system:\n- [Git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)\n- [Maven](https://maven.apache.org/index.html)\n- [Docker](https://www.docker.com/products/docker-desktop)\n- [Appsody](https://appsody.dev/)\n- Programming Languages & Compilers:\n    - [Java JDK](https://openjdk.java.net/) - version 1.8 or greater\n    - [NodeJS](https://nodejs.org/en/) - version 12.18 or greater\n    - [Python](https://www.python.org/downloads/) - version 3.7 or greater\n\n### Clone all the repositories\n\nStart by cloning the root repository using the command:\n\n```\ngit clone https://github.com/ibm-cloud-architecture/refarch-kc/\n```\n\nThen go to the `refarch-kc` folder and use the command:\n\n```\n./script/clone.sh\n```\n\nto get all the solution repositories. You should have at least the following repositories:\n\n```\nrefarch-kc-container-ms\nrefarch-kc-order-ms\nrefarch-kc-ui\nrefarch-kc\nrefarch-kc-ms\nrefarch-kc-streams\n```\n\nThen modify the environment variables according to your environment you are using. This file is used by a lot of scripts in the solution to set the target deployment environment: LOCAL, IBMCLOUD, ICP, MINIKUBE.\n\n## Apache Kafka\n\nThere are multiple options to deploy an Apache Kafka-based cluster to support this reference implementation:\n\n<AnchorLinks small>\n  <AnchorLink>IBM Event Streams on IBM Cloud</AnchorLink>\n  <AnchorLink>IBM Event Streams on RedHat OpenShift Container Platform</AnchorLink>\n  <AnchorLink>Apache Kafka via Strimzi Operator</AnchorLink>\n</AnchorLinks>\n\n### IBM Event Streams on IBM Cloud\n\n#### Service Deployment\n\nWe recommend to follow [our most recent lab](https://ibm-cloud-architecture.github.io/refarch-eda/technology/event-streams/es-cloud/) on how to provision an Event Streams intance on cloud.\n\n* In the *Manage* panel add the topics needed for the solution. We need at least the following:\n\n ![](images/IES-IC-topics.png)\n\n* In the Service Credentials tab, create new credentials to get the Kafka broker list, the admim URL and the api_key needed to authenticate the consumers or producers.\n\n ![](images/IES-IC-credentials.png)\n\n#### Kafka Brokers\n\nRegardless of specific deployment targets (OCP, IKS, k8s), the following prerequisite Kubernetes ConfigMap needs to be created to support the deployments of the application's microservices.  These artifacts need to be created once per unique deployment of the entire application and can be shared between application components in the same overall application deployment.  These values can be acquired from the `kafka_brokers_sasl` section of the service instance's Service Credentials.\n\n```shell\nkubectl create configmap kafka-brokers --from-literal=brokers='<replace with comma-separated list of brokers>' -n <target k8s namespace / ocp project>\nkubectl describe configmap kafka-brokers -n <target k8s namespace / ocp project>\n```\n\n#### Event Streams User Credentials\n\nThe Event Streams User Credentials are needed in order for any deployed consumers or producers to work with the IBM Event Streams service in IBM Cloud. To avoid sharing security keys, create a Kubernetes Secret in the target cluster you will deploy the application microservices to. This is available from the Service Credentials information you just created above.\n\n```shell\nkubectl create secret generic eventstreams-cred --from-literal=username='token' --from-literal=password='<replace with api key>' -n <target k8s namespace / ocp project>\nkubectl describe secret eventstreams-cred -n <target k8s namespace / ocp project>\n```\n\n### IBM Event Streams on RedHat OpenShift Container Platform\n\n#### Service Deployment\n\nThe installation is documented in the [product documentation](https://ibm.github.io/event-streams/installing/installing-openshift/) and in our [own note here.](https://ibm-cloud-architecture.github.io/refarch-eda/deployments/eventstreams/)\n\n#### Kafka Brokers\n\nRegardless of specific deployment targets (OCP, IKS, k8s), the following prerequisite Kubernetes ConfigMap needs to be created to support the deployments of the application's microservices.  These artifacts need to be created once per unique deployment of the entire application and can be shared between application components in the same overall application deployment.\n\n```shell\nkubectl create configmap kafka-brokers --from-literal=brokers='<replace with comma-separated list of brokers>' -n <target k8s namespace / ocp project>\nkubectl describe configmap kafka-brokers -n <target k8s namespace / ocp project>\n```\n\n#### Event Streams User Credentials\n\nThe Event Streams Scram User Credentials are needed in order for any deployed consumers or producers to work with the IBM Event Streams instance running in your cluster. These SCRAM credentials are associated to the KafkaUser object that is being created behind the scenes. In order to create that KafkaUser object and obtain the SCRAM credentials for it, follow the instructions at <https://ibm.github.io/event-streams/security/managing-access/#creating-a-kafkauser-in-the-ibm-event-streams-ui>\n\nTo avoid sharing security keys, create a Kubernetes Secret in the target cluster you will deploy the application microservices to:\n\n```shell\nkubectl create secret generic eventstreams-cred --from-literal=username='<replace with scram username>' --from-literal=password='<replace with scram password>' -n <target k8s namespace / ocp project>\nkubectl describe secrets -n <target k8s namespace / ocp project>\n```\n\n**IMPORTANT:** Our reference application uses idempotent producers which our KafkaUser needs a special set of permissions for. In, order to add these, edit the KafkaUser created when you generatted the SCRAM credentials following the link above in this section and add the following set of permissions in the `acls` list:\n\n```yaml\n- host: '*'\n  operation: IdempotentWrite\n  resource:\n    name: '*'\n    patternType: literal\n    type: cluster\n```\n\n#### Event Streams Certificates\n\nIf you are using Event Streams as your Kafka broker provider and it is deployed via the IBM Cloud Pak for Integration (ICP4I), you will need to create an additional Secret to store the generated Certificates & Truststores to connect securely between your application components and the Kafka brokers.  These artifacts need to be created once per unique deployment of the entire application and can be shared between application components in the same overall application deployment.\n\nFrom the **Connect to this cluster** tab on the landing page of your Event Streams installation, download both the **PKCS12** and the **PEM** certificates. After you have downloaded these, we are going to make them available to our producer and consumers by storing them in Kubernetes Secrets:\n\n```shell\noc create secret generic eventstreams-truststore --from-file=<path to downloaded es-cert.p12>\noc create secret generic eventstreams-cert-pem --from-file=<path to downloaded es-cert.pem>\n```\n\n### Apache Kafka via Strimzi Operator\n\n<InlineNotification kind=\"warning\"><strong>TODO</strong> - Update Strimzi documentation</InlineNotification>\n\nIf you simply want to deploy Kafka using the open source, community-supported Helm Charts, you can do so with the following commands.\n\n## Postgresql\n\nThe [Container Manager microservice](https://github.com/ibm-cloud-architecture/refarch-kc-container-ms/) persists the Reefer Container inventory in a Postgresql database.  The deployment of Postgresql is only necessary to support the deployment of the Container Manager microservice.  If you are not deploying the Container Manager microservice, you do not need to deploy and configure a Postgresql service and database.\n\nThe options to support the Container Manager microservice with a Postgresql database are:\n<AnchorLinks small>\n  <AnchorLink>Postgresql on IBM Cloud</AnchorLink>\n  <AnchorLink>Community-based Postgresql Helm charts</AnchorLink>\n</AnchorLinks>\n\n### Postgresql on IBM Cloud\n\n#### Service Deployment\n\n To install the service, follow the [product documentation here](https://cloud.ibm.com/catalog/services/databases-for-postgresql).\n\n Once the service is deployed, you need to create some service credentials and retreive the following values for the different configurations:\n\n * `postgres.username`\n * `postgres.password`\n * `postgres.composed`, which will need to be mapped to a JDBC URL in the format of `jdbc:postgresql://<hostname>:<port>/<database-name>?sslmode=verify-full&sslfactory=org.postgresql.ssl.NonValidatingFactory` _(this will remove the `username` and `password` values from the default `composed` string)_\n\n ![](images/postgres-credentials.png)\n\n#### Creating Postgresql credentials as Kubernetes Secrets\n\n1. Applying the same approach as above, copy the Postgresql URL as defined in the Postegresql service credential and execute the following command:\n```shell\nkubectl create secret generic postgresql-url --from-literal=binding='<replace with postgresql-url>' -n <target k8s namespace / ocp project>\n```\n\n2. For the user:\n```shell\nkubectl create secret generic postgresql-user --from-literal=binding='ibm_cloud_c...' -n <target k8s namespace / ocp project>\n```\n\n3. For the user password:\n```shell\nkubectl create secret generic postgresql-pwd --from-literal=binding='<password from the service credential>.' -n <target k8s namespace / ocp project>\n```\n\n4. When running Postgresql through the IBM Cloud service, additional SSL certificates are required to communicate securely:\n    1. Install the IBM Cloud Database CLI Plugin:\n   ```shell\n   ibmcloud plugin install cloud-databases\n   ```\n    2. Get the certificate using the name of the postgresql service:\n  ```shell\n  ibmcloud cdb deployment-cacert $IC_POSTGRES_SERV > postgresql.crt\n  ```\n    3. Then add it into an environment variable\n  ```shell\n  export POSTGRESQL_CA_PEM=\"$(cat ./postgresql.crt)\"\n  ```\n    4. Then define a secret:\n  ```shell\n  kubectl create secret generic postgresql-ca-pem --from-literal=binding=\"$POSTGRESQL_CA_PEM\" -n browncompute\n  ```\n\n### Community-based Postgresql Helm charts\n\nIf you simply want to deploy Postgresql using the open source, community-supported Helm Charts, you can do so with the following commands.\n\n#### Environment Considerations\n\nReference [Application Components Pre-reqs](application-components.md#openshift-container-platform-3-11) for details on creating the necessary ServiceAccount with required permissions, prior to deployment.\n\n#### Service Deployment\n\n1. Add Bitnami Helm Repository:\n```shell\nhelm repo add bitnami https://charts.bitnami.com/bitnami\n```\n\n2. Update the Helm repository:\n```shell\nhelm repo update\n```\n\n3. Create a Kubernetes Namespace or OpenShift Project (if not already created).\n```shell\nkubectl create namespace <target namespace>\n```\n4. Deploy Postgresql using the `bitnami/postgresql` Helm Chart:\n```shell\nmkdir bitnami\nmkdir templates\nhelm fetch --untar --untardir bitnami bitnami/postgresql\nhelm template --name postgre-db --set postgresqlPassword=supersecret \\\n  --set persistence.enabled=false --set serviceAccount.enabled=true \\\n  --set serviceAccount.name=<existing service account> bitnami/postgresql \\\n  --namespace <target namespace> --output-dir templates\nkubectl apply -f templates/postgresql/templates\n```\n  It will take a few minutes to get the pods ready.\n\n#### Creating Postgresql credentials as Kubernetes Secrets\n\n* The `postgresql-url` needs to point to the in-cluster (non-headless) Kubernetes Service created as part of the deployment and should take the form of the deployment name with the suffix of `-postgresql`:\n\n ```shell\n kubectl get services | grep postgresql | grep -v headless\n kubectl create secret generic postgresql-url --from-literal=binding='jdbc:postgresql://<helm-release-name>-postgresql' -n <target k8s namespace / ocp project>\n ```\n\n* For the user:\n\n ```shell\n kubectl create secret generic postgresql-user --from-literal=binding='postgres' -n <target k8s namespace / ocp project>\n ```\n\n* For the user password:\n\n ```shell\n kubectl create secret generic postgresql-pwd --from-literal=binding='<password used in the helm template command>.' -n <target k8s namespace / ocp project>\n ```\n\n#### Service Debugging & Troubleshooting\n\nAccess to the in-container password can be made using the following command.  This should be the same value you passed in when you deployed the service.\n\n```shell\nexport POSTGRES_PASSWORD=$(kubectl get secret --namespace  <target namespace> postgre-db-postgresql -o jsonpath=\"{.data.postgresql-password}\" | base64 --decode)\n```\n\nAnd then use the `psql` command line interface to interact with postgresql. For that, we use a Docker image as a client to the Postgresql server:\n\n```shell\nkubectl run postgre-db-postgresql-client --rm --tty -i --restart='Never' --namespace <target namespace> --image bitnami/postgresql:11.3.0-debian-9-r38 --env=\"PGPASSWORD=$POSTGRES_PASSWORD\" --command -- psql --host postgre-db-postgresql -U postgres -p 5432\n```\n\nTo connect to your database from outside the cluster execute the following commands:\n\n```shell\nkubectl port-forward --namespace <target namespace> svc/postgre-db-postgresql 5432:5432 &&\\\n  PGPASSWORD=\"$POSTGRES_PASSWORD\" psql --host 127.0.0.1 -U postgres -p 5432\n```\n\n## BPM\n\nThe containers microservice component of this Reefer Container EDA reference application can be integrated with a BPM process for the the maintenance of the containers. This BPM process will dispatch a field engineer so that the engineer can go to the reefer container to fix it. The process of scheduling an engineer and then completing the work can best be facilitated through a process based, structured workflow. We will be using IBM BPM on Cloud or Cloud Pak for Automation to best demonstrate the workflow. This workflow can be explored in detail [here](https://github.com/ibm-cloud-architecture/refarch-reefer-ml/tree/master/docs/bpm).\n\n<AnchorLinks small>\n  <AnchorLink>IBM BPM on IBM Cloud</AnchorLink>\n  <AnchorLink>IBM BPM on RedHat OpenShift Container Platform</AnchorLink>\n</AnchorLinks>\n\nIn order for the containers microservice to fire the BPM workflow, we need to provide the following information through Kubernetes configMaps and secrets:\n\n1. Provide in a configMap:\n   * the **BPM authentication login endpoint**\n   * the **BPM workflow endpoint**\n   * the **BPM anomaly event threshold**\n   * the **BPM authentication token time expiration**\n\n   ```shell\n   cat <<EOF | kubectl apply -f -\n   apiVersion: v1\n   kind: ConfigMap\n   metadata:\n     name: bpm-anomaly\n   data:\n     url: <replace with your BPM workflow endpoint>\n     login: <replace with your BPM authentication endpoint>\n     expiration: <replace with the number of second for the auth token to expire after>\n     anomalyThreshold: <replace with the number of anomaly events to receive before calling BPM>\n   EOF\n   ```\n\n2. Provide your BPM instance's **credentials** in a secret:\n\n   ```shell\n   kubectl create secret generic bpm-anomaly --from-literal=user='<replace with your BPM user>' --from-literal=password='<replace with your BPM password>' -n <target k8s namespace / ocp project>\n   kubectl describe secrets -n <target k8s namespace / ocp project>\n   ```\n\n**IMPORTANT:** The names for both the secret and configMap (`bpm-anomaly`) is the default the container microservice uses in its [helm chart](https://github.com/ibm-cloud-architecture/refarch-kc-container-ms/tree/master/SpringContainerMS/chart/springcontainerms). Make sure the name for the configMap and secret you create **match** the names you used in the containers microservice's helm chart.\n\nIf you do not have access to any BPM instance with this field engineer dispatching workflow, you can bypass the call to BPM by disabling such call in the container microservice component. For doing so, you can use the following container microservice's API endpoints:\n\n1. Enable BPM: [`http://<container_microservice_endpoint>/bpm/enable`](#bpm)\n2. Disable BPM: [`http://<container_microservice_endpoint>/bpm/disable`](#bpm)\n3. BPM status: [`http://<container_microservice_endpoint>/bpm/status`](#bpm)\n\nwhere `<container_microservice_endpoint>` is the route, ingress or nodeport service you associated to your container microservice component at deployment time.\n\n### IBM BPM on IBM Cloud\n\n_To be completed_\n\n**Reference:** [https://www.bpm.ibmcloud.com/](https://www.bpm.ibmcloud.com/)\n\n### IBM BPM on RedHat OpenShift Container Platform\n\n_To be completed_\n\n**Reference:** [https://www.ibm.com/cloud/cloud-pak-for-automation/](https://www.ibm.com/cloud/cloud-pak-for-automation/)\n","fileAbsolutePath":"/home/runner/work/refarch-kc/refarch-kc/docs/src/pages/infrastructure/required-services/index.mdx"}}},"staticQueryHashes":["1054721580","1054721580","1364590287","2102389209","2102389209","2456312558","2746626797","3018647132","3037994772","768070550"]}