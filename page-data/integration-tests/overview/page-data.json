{"componentChunkName":"component---src-pages-integration-tests-overview-index-mdx","path":"/integration-tests/overview/","result":{"pageContext":{"frontmatter":{"title":"Reefer container shipment solution integration tests","description":"Reefer container shipment solution integration tests"},"relativePagePath":"/integration-tests/overview/index.mdx","titleType":"append","MdxNode":{"id":"0b2b6258-2b1b-5ef0-8a94-46af0d71daa8","children":[],"parent":"a7dce1f9-6bac-50c7-a933-d5ab88f9ed6a","internal":{"content":"---\ntitle: Reefer container shipment solution integration tests\ndescription: Reefer container shipment solution integration tests\n---\n\nThe Reefer container shipment solution comes with a set of integration test cases to ensure the end to end functionality of the application. These test cases are part of our CI/CD process so that we ensure every new pull request that brings new code in does not break or modify the correct functionality of the application.\n\nSo far we have the following integration test cases:\n\n- [Happy path](/integration-tests/happy-path/) - End to end happy path test.\n- [SAGA pattern](/integration-tests/saga-pattern/) - SAGA pattern for new order creation test.\n- [Order Cancellation](/integration-tests/order-cancellation/) - Order Cancellation test.\n- [Container Anomaly](/integration-tests/container-anomaly/) - Container anomaly and maintenance test.\n- [Dead Letter Queue](/integration-tests/dead-letter-queue/) - Container Anomaly Dead Letter Queue Pattern test.\n\nNew integration test cases will be added in order to test other parts of the application as well as use cases and other Event Driven Patterns.\n\n## How to run the integration test cases\n\n### Pre-requisites\n\nIn order to run the integration tests against the Reefer container shipment solution you first need to have this solution deployed on an Openshift or Kubernetes cluster. The solution is made up of:\n\n1. Backing services such as IBM Event Streams and PostgreSQL - Instructions [here](/infrastructure/required-services/).\n2. The Reefer container shipment solution components - Instructions [here](/business-scenario/quickstart-tutorial/).\n\nOnce you have the solution deployed into your cluster, apart from an instance of IBM Event Streams and PostgreSQL either on premises or in IBM Cloud, you should have the following components at the very least for the integration tests to run:\n\n```bash\n$ oc get pods\nNAME                                                READY     STATUS    RESTARTS   AGE\npod/ordercommandms-deployment-7cfcf65ffc-ffbxt      1/1       Running   0          32d\npod/orderqueryms-deployment-5ff4fd44d-ghrg6         1/1       Running   0          32d\npod/springcontainerms-deployment-7f78fc9b64-kt2pf   1/1       Running   0          32d\npod/voyagesms-deployment-7775bb8974-h8vj4           1/1       Running   0          32d\n```\n\nThe integration test cases have been implemented to be run as a kubernetes job called **reefer-itgtests-job**. This job consist of a tailored python container where the integration tests, which are written in Python, will get executed in. The yaml file that will create such kubernetes job, called **ReeferItgTests.yaml**, can be found under the `itg-tests/es-it` folder in this very same repository. The reason for creating a tailored python container which to execute the integration tests in is because we can then control the execution environment for the integration tests. This way we ensure the appropriate libraries, permissions, etc are as expected. This tailored python container docker image is publicly available in the Docker Hub (`ibmcase/kcontainer-python:itgtests`). Please, make sure you can access the Docker Hub public registries from your OpenShift or Kubernetes cluster.\n\nThe integration tests also require of some variables being defined beforehand, some of which need to be defined as **secrets or configMaps** within your kubernetes namespace or OpenShift project, such as `KAFKA_APIKEY`, `KAFKA_BROKERS` and the IBM Event Streams PEM certificate (in case you are working with IBM Event Streams on premise), where the Reefer container shipment solution has been deployed into. You should have got these secrets or configMaps already created when deploying your backing services in #1 of this pre-requisites section.\n\nOther required variables for the integration tests need to be defined within the kubernetes job yaml file:\n\n- Orders topic name: This could be specified within the integration tests kubernetes job yaml file under the variable **ITGTESTS_ORDERS_TOPIC** which defaults to `itg-orders`.\n\n- Order Command topic name: This could be specified within the integration tests kubernetes job yaml file under the variable **ITGTESTS_ORDER_COMMANDS_TOPIC** which defaults to `itg-order-commands`.\n\n- Containers topic name: This could be specified within the integration tests kubernetes job yaml file under the variable **ITGTESTS_CONTAINERS_TOPIC** which defaults to `itg-containers`.\n\n- Container anomaly retry topic name: This could be specified within the integration tests kubernetes job yaml file under the variable **ITGTESTS_CONTAINER_ANOMALY_RETRY_TOPIC** which defaults to `itg-container-anomaly-retry`.\n\n- Container anomaly dead topic name: This could be specified within the integration tests kubernetes job yaml file under the variable **ITGTESTS_CONTAINER_ANOMALY_DEAD_TOPIC** which defaults to `itg-container-anomaly-dead`.\n\n- Kafka Environment: It should be either **OCP** or **IBMCLOUD** depending on where your IBM Event Streams instance is deployed onto. If it is deployed on premises in your OpenShift or Kubernetes cluster, then it `KAFKA_ENV` should be set to `OCP`. If you are using an IBM Event Streams instance in the IBM Cloud, then `KAFKA_ENV` should be set to `IBMCLOUD`.\n\n  This is important as the **IBM Event Streams on-prem instances require a PEM certificate** for the Kafka libraries to successfully connect to it. So, if you are using IBM Event Streams on-prem in your OpenShift or Kubernetes cluster, you also have to:\n\n  1. Uncomment the bottom part of the integration tests kubernetes job yaml file.\n  2. Make sure you created the **eventstreams-pem-file** secret that will hold your IBM Event Streams PEM certificate, in step #1 of this pre-requisites section.\n\n**IMPORTANT:** For the integration test suite to work fine, more precisely to get the test case for testing [the Dead Letter Queue pattern](/implementation/dead-letter-queue/)) to succeed, we **must** mockup the BPM integration which we have developed some internal endpoints for. To use the BPM mockup endpoints, you will need to make sure the **bpm-anomaly** configMap you created for the Spring Container microservice component of the Reefer container shipment solution holds the following values for the url and login attributes:\n\n```bash\nlogin: 'http://localhost:8080/bpm_mockup/login'\nurl: 'http://localhost:8080/bpm_mockup/bpm_process_404'\n```\n\nYou can do so by manually editing the configMap:\n\n```bash\n$ oc edit configmap bpm-anomaly -n eda-integration\n```\n\nThe above will require to restart the Spring Container microservice component, although we **strongly suggest** the integration tests are run on a separate testing environment where the **bpm-anomaly** (and any other configuration item) holds testing values as well as the **recreation** of the kafka topics involved in the integration tests suite.\n\n### Run\n\nIn order to run the integration test cases for the Reefer container shipment solution, we need to create the the job that will run these. To create the job, we simply execute:\n\n```bash\noc apply -f ReeferItgReefer.yaml -n <namespace>\n```\n\nYou should see the following output:\n\n``` bash\njob.batch/reefer-itgtests-job created\n```\n\nand if you list the pods in your namespace you should see a new pod which is running the integration tests:\n\n```bash\n$ oc get pods | grep itgtests\nNAME                                           READY     STATUS        RESTARTS   AGE\nreefer-itgtests-job-x594k                      1/1       Running       0          2m\n```\n\nOnce the integration tests have finished, the pod should transition to completed status:\n\n```bash\n$ oc get pods\nNAME                                           READY     STATUS      RESTARTS   AGE\nreefer-itgtests-job-x594k                      0/1       Completed   0          3m\n```\n\nand the job output should be like:\n\n```bash\n$ oc get jobs\nNAME                      DESIRED   SUCCESSFUL   AGE\nreefer-itgtests-job       1         1            3m\n```\n\n## Output\n\nIf we want to inspect the output of the integration tests, we would need to get the logs for the pod that ran them:\n\n```bash\n$ oc logs e2e-reefer-itgtests-job-x594k\n```\n\nThe output of the integration test cases is made up of a brief description of the execution environment:\n\n```bash\n-----------------------------------------------------------------\n-- Reefer Container Shipment EDA application Integration Tests --\n-----------------------------------------------------------------\n\nExecuting integrations tests from branch master of https://github.com/ibm-cloud-architecture/refarch-kc.git\nKafka Brokers: broker-0-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-3-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093\nKafka API Key: XXXXXXX\nKafka Env: IBMCLOUD\nOrders topic name: itg-orders\nOrder Command topic name: itg-order-commands\nContainers topic name: itg-containers\n------------------------------------------------------------------\n```\n\nThen, each of the three test cases outlined in the introduction of this readme file will get executed, each of them beginning with a header like:\n\n```bash\n******************************************\n******************************************\n**********   E2E Happy Path   ************\n******************************************\n******************************************\n```\n\nAfter the header, the different tests within the test case will get executed. Each of these comes with a header and look like:\n\n```bash\n--------------------------------\n--- [TEST] : Voyage Assigned ---\n--------------------------------\n\n1 - Load the expected voyage assigned event on the order topic from its json files\nThe expected voyage assigned event is:\n{\n    \"payload\": {\n        \"orderID\": \"a467070e-797e-40f9-9644-7393e8553f1f\",\n        \"voyageID\": \"101\"\n    },\n    \"timestamp\": \"\",\n    \"type\": \"VoyageAssigned\",\n    \"version\": \"1\"\n}\nDone\n\n2 - Read voyage assigned from oder topic\n[KafkaConsumer] - This is the configuration for the consumer:\n[KafkaConsumer] - {'bootstrap.servers': 'broker-0-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-3-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093', 'group.id': 'pythonconsumers', 'auto.offset.reset': 'earliest', 'enable.auto.commit': True, 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'PLAIN', 'sasl.username': 'token', 'sasl.password': 'XXXXXXXX'}\n.[KafkaConsumer] - @@@ pollNextOrder itg-orders partition: [0] at offset 2 with key b'a467070e-797e-40f9-9644-7393e8553f1f':\n\tvalue: {\"timestamp\":1576667245430,\"type\":\"VoyageAssigned\",\"version\":\"1\",\"payload\":{\"voyageID\":\"101\",\"orderID\":\"a467070e-797e-40f9-9644-7393e8553f1f\"}}\nThis is the event read from the order topic:\n{\n    \"payload\": {\n        \"orderID\": \"a467070e-797e-40f9-9644-7393e8553f1f\",\n        \"voyageID\": \"101\"\n    },\n    \"timestamp\": \"\",\n    \"type\": \"VoyageAssigned\",\n    \"version\": \"1\"\n}\nDone\n\n3 - Verify voyage assigned event\nDone\n\n```\n\nA summary of the test case execution is shown at the end of each test case:\n\n```bash\n----------------------------------------------------------------------\nRan 7 tests in 64.262s\n\nOK\n```\n\nA final summary of all tests cases is shown at the very end and looks like the following:\n\n```bash\nEND RESULTS:\n\nTEST CASE - E2EHappyPath\n-----------------------------------\ntest1_createContainer...OK\ntest2_voyagesExist...OK\ntest3_createOrder...OK\ntest4_containerAllocated...OK\ntest5_voyageAssigned...OK\ntest6_orderAssignedREST...OK\ntest7_exportValues...OK\n-----------------------------------\nPASSED: 7\nFAILED: 0\n\nTEST CASE - SagaNoContainer\n-----------------------------------\ntest1_createOrder...OK\ntest2_containerNotFound...OK\ntest3_orderRejected...OK\ntest4_orderRejectedREST...OK\n-----------------------------------\nPASSED: 4\nFAILED: 0\n\nTEST CASE - SagaNoVoyage\n-----------------------------------\ntest1_createContainer...OK\ntest2_createOrder...OK\ntest3_containerAllocated...OK\ntest4_voyageNotFound...OK\ntest5_orderRejected...OK\ntest6_orderRejectedREST...OK\ntest7_containerUnassignedREST...OK\ntest8_exportValues...OK\n-----------------------------------\nPASSED: 8\nFAILED: 0\n\nTEST CASE - OrderCancellation\n-----------------------------------\ntest1_createOrder...OK\ntest2_containerAllocated...OK\ntest3_voyageAssigned...OK\ntest4_orderAssignedREST...OK\ntest5_orderCancelled...OK\ntest6_orderCancelledREST...OK\ntest7_containerUnassignedREST...OK\ntest8_voyageCompensated...OK\n-----------------------------------\nPASSED: 8\nFAILED: 0\n\nTEST CASE - SpoilOrder\n-----------------------------------\ntest1_disableBPM...OK\ntest2_sendAnomalyEvents...OK\ntest3_containerMaintenanceNeeded...OK\ntest4_containerOrderSpoilt...OK\ntest5_containerToMaintenance...OK\ntest6_containerInMaintenance...OK\ntest7_containerOffMaintenance...OK\ntest8_containerEmpty...OK\ntest9_enableBPM...OK\n-----------------------------------\nPASSED: 9\nFAILED: 0\n\nTEST CASE - Dlq\n-----------------------------------\ntest1_createContainer...OK\ntest2_sendAnomalyEvents...OK\ntest3_containerMaintenanceNeeded...OK\ntest4_containerAnomalyRetry...OK\ntest5_containerAnomalyDead...OK\n-----------------------------------\nPASSED: 5\nFAILED: 0\n```\n","type":"Mdx","contentDigest":"ca12c85883d38e34825624881deba5a5","counter":370,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"Reefer container shipment solution integration tests","description":"Reefer container shipment solution integration tests"},"exports":{},"rawBody":"---\ntitle: Reefer container shipment solution integration tests\ndescription: Reefer container shipment solution integration tests\n---\n\nThe Reefer container shipment solution comes with a set of integration test cases to ensure the end to end functionality of the application. These test cases are part of our CI/CD process so that we ensure every new pull request that brings new code in does not break or modify the correct functionality of the application.\n\nSo far we have the following integration test cases:\n\n- [Happy path](/integration-tests/happy-path/) - End to end happy path test.\n- [SAGA pattern](/integration-tests/saga-pattern/) - SAGA pattern for new order creation test.\n- [Order Cancellation](/integration-tests/order-cancellation/) - Order Cancellation test.\n- [Container Anomaly](/integration-tests/container-anomaly/) - Container anomaly and maintenance test.\n- [Dead Letter Queue](/integration-tests/dead-letter-queue/) - Container Anomaly Dead Letter Queue Pattern test.\n\nNew integration test cases will be added in order to test other parts of the application as well as use cases and other Event Driven Patterns.\n\n## How to run the integration test cases\n\n### Pre-requisites\n\nIn order to run the integration tests against the Reefer container shipment solution you first need to have this solution deployed on an Openshift or Kubernetes cluster. The solution is made up of:\n\n1. Backing services such as IBM Event Streams and PostgreSQL - Instructions [here](/infrastructure/required-services/).\n2. The Reefer container shipment solution components - Instructions [here](/business-scenario/quickstart-tutorial/).\n\nOnce you have the solution deployed into your cluster, apart from an instance of IBM Event Streams and PostgreSQL either on premises or in IBM Cloud, you should have the following components at the very least for the integration tests to run:\n\n```bash\n$ oc get pods\nNAME                                                READY     STATUS    RESTARTS   AGE\npod/ordercommandms-deployment-7cfcf65ffc-ffbxt      1/1       Running   0          32d\npod/orderqueryms-deployment-5ff4fd44d-ghrg6         1/1       Running   0          32d\npod/springcontainerms-deployment-7f78fc9b64-kt2pf   1/1       Running   0          32d\npod/voyagesms-deployment-7775bb8974-h8vj4           1/1       Running   0          32d\n```\n\nThe integration test cases have been implemented to be run as a kubernetes job called **reefer-itgtests-job**. This job consist of a tailored python container where the integration tests, which are written in Python, will get executed in. The yaml file that will create such kubernetes job, called **ReeferItgTests.yaml**, can be found under the `itg-tests/es-it` folder in this very same repository. The reason for creating a tailored python container which to execute the integration tests in is because we can then control the execution environment for the integration tests. This way we ensure the appropriate libraries, permissions, etc are as expected. This tailored python container docker image is publicly available in the Docker Hub (`ibmcase/kcontainer-python:itgtests`). Please, make sure you can access the Docker Hub public registries from your OpenShift or Kubernetes cluster.\n\nThe integration tests also require of some variables being defined beforehand, some of which need to be defined as **secrets or configMaps** within your kubernetes namespace or OpenShift project, such as `KAFKA_APIKEY`, `KAFKA_BROKERS` and the IBM Event Streams PEM certificate (in case you are working with IBM Event Streams on premise), where the Reefer container shipment solution has been deployed into. You should have got these secrets or configMaps already created when deploying your backing services in #1 of this pre-requisites section.\n\nOther required variables for the integration tests need to be defined within the kubernetes job yaml file:\n\n- Orders topic name: This could be specified within the integration tests kubernetes job yaml file under the variable **ITGTESTS_ORDERS_TOPIC** which defaults to `itg-orders`.\n\n- Order Command topic name: This could be specified within the integration tests kubernetes job yaml file under the variable **ITGTESTS_ORDER_COMMANDS_TOPIC** which defaults to `itg-order-commands`.\n\n- Containers topic name: This could be specified within the integration tests kubernetes job yaml file under the variable **ITGTESTS_CONTAINERS_TOPIC** which defaults to `itg-containers`.\n\n- Container anomaly retry topic name: This could be specified within the integration tests kubernetes job yaml file under the variable **ITGTESTS_CONTAINER_ANOMALY_RETRY_TOPIC** which defaults to `itg-container-anomaly-retry`.\n\n- Container anomaly dead topic name: This could be specified within the integration tests kubernetes job yaml file under the variable **ITGTESTS_CONTAINER_ANOMALY_DEAD_TOPIC** which defaults to `itg-container-anomaly-dead`.\n\n- Kafka Environment: It should be either **OCP** or **IBMCLOUD** depending on where your IBM Event Streams instance is deployed onto. If it is deployed on premises in your OpenShift or Kubernetes cluster, then it `KAFKA_ENV` should be set to `OCP`. If you are using an IBM Event Streams instance in the IBM Cloud, then `KAFKA_ENV` should be set to `IBMCLOUD`.\n\n  This is important as the **IBM Event Streams on-prem instances require a PEM certificate** for the Kafka libraries to successfully connect to it. So, if you are using IBM Event Streams on-prem in your OpenShift or Kubernetes cluster, you also have to:\n\n  1. Uncomment the bottom part of the integration tests kubernetes job yaml file.\n  2. Make sure you created the **eventstreams-pem-file** secret that will hold your IBM Event Streams PEM certificate, in step #1 of this pre-requisites section.\n\n**IMPORTANT:** For the integration test suite to work fine, more precisely to get the test case for testing [the Dead Letter Queue pattern](/implementation/dead-letter-queue/)) to succeed, we **must** mockup the BPM integration which we have developed some internal endpoints for. To use the BPM mockup endpoints, you will need to make sure the **bpm-anomaly** configMap you created for the Spring Container microservice component of the Reefer container shipment solution holds the following values for the url and login attributes:\n\n```bash\nlogin: 'http://localhost:8080/bpm_mockup/login'\nurl: 'http://localhost:8080/bpm_mockup/bpm_process_404'\n```\n\nYou can do so by manually editing the configMap:\n\n```bash\n$ oc edit configmap bpm-anomaly -n eda-integration\n```\n\nThe above will require to restart the Spring Container microservice component, although we **strongly suggest** the integration tests are run on a separate testing environment where the **bpm-anomaly** (and any other configuration item) holds testing values as well as the **recreation** of the kafka topics involved in the integration tests suite.\n\n### Run\n\nIn order to run the integration test cases for the Reefer container shipment solution, we need to create the the job that will run these. To create the job, we simply execute:\n\n```bash\noc apply -f ReeferItgReefer.yaml -n <namespace>\n```\n\nYou should see the following output:\n\n``` bash\njob.batch/reefer-itgtests-job created\n```\n\nand if you list the pods in your namespace you should see a new pod which is running the integration tests:\n\n```bash\n$ oc get pods | grep itgtests\nNAME                                           READY     STATUS        RESTARTS   AGE\nreefer-itgtests-job-x594k                      1/1       Running       0          2m\n```\n\nOnce the integration tests have finished, the pod should transition to completed status:\n\n```bash\n$ oc get pods\nNAME                                           READY     STATUS      RESTARTS   AGE\nreefer-itgtests-job-x594k                      0/1       Completed   0          3m\n```\n\nand the job output should be like:\n\n```bash\n$ oc get jobs\nNAME                      DESIRED   SUCCESSFUL   AGE\nreefer-itgtests-job       1         1            3m\n```\n\n## Output\n\nIf we want to inspect the output of the integration tests, we would need to get the logs for the pod that ran them:\n\n```bash\n$ oc logs e2e-reefer-itgtests-job-x594k\n```\n\nThe output of the integration test cases is made up of a brief description of the execution environment:\n\n```bash\n-----------------------------------------------------------------\n-- Reefer Container Shipment EDA application Integration Tests --\n-----------------------------------------------------------------\n\nExecuting integrations tests from branch master of https://github.com/ibm-cloud-architecture/refarch-kc.git\nKafka Brokers: broker-0-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-3-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093\nKafka API Key: XXXXXXX\nKafka Env: IBMCLOUD\nOrders topic name: itg-orders\nOrder Command topic name: itg-order-commands\nContainers topic name: itg-containers\n------------------------------------------------------------------\n```\n\nThen, each of the three test cases outlined in the introduction of this readme file will get executed, each of them beginning with a header like:\n\n```bash\n******************************************\n******************************************\n**********   E2E Happy Path   ************\n******************************************\n******************************************\n```\n\nAfter the header, the different tests within the test case will get executed. Each of these comes with a header and look like:\n\n```bash\n--------------------------------\n--- [TEST] : Voyage Assigned ---\n--------------------------------\n\n1 - Load the expected voyage assigned event on the order topic from its json files\nThe expected voyage assigned event is:\n{\n    \"payload\": {\n        \"orderID\": \"a467070e-797e-40f9-9644-7393e8553f1f\",\n        \"voyageID\": \"101\"\n    },\n    \"timestamp\": \"\",\n    \"type\": \"VoyageAssigned\",\n    \"version\": \"1\"\n}\nDone\n\n2 - Read voyage assigned from oder topic\n[KafkaConsumer] - This is the configuration for the consumer:\n[KafkaConsumer] - {'bootstrap.servers': 'broker-0-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-3-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093', 'group.id': 'pythonconsumers', 'auto.offset.reset': 'earliest', 'enable.auto.commit': True, 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'PLAIN', 'sasl.username': 'token', 'sasl.password': 'XXXXXXXX'}\n.[KafkaConsumer] - @@@ pollNextOrder itg-orders partition: [0] at offset 2 with key b'a467070e-797e-40f9-9644-7393e8553f1f':\n\tvalue: {\"timestamp\":1576667245430,\"type\":\"VoyageAssigned\",\"version\":\"1\",\"payload\":{\"voyageID\":\"101\",\"orderID\":\"a467070e-797e-40f9-9644-7393e8553f1f\"}}\nThis is the event read from the order topic:\n{\n    \"payload\": {\n        \"orderID\": \"a467070e-797e-40f9-9644-7393e8553f1f\",\n        \"voyageID\": \"101\"\n    },\n    \"timestamp\": \"\",\n    \"type\": \"VoyageAssigned\",\n    \"version\": \"1\"\n}\nDone\n\n3 - Verify voyage assigned event\nDone\n\n```\n\nA summary of the test case execution is shown at the end of each test case:\n\n```bash\n----------------------------------------------------------------------\nRan 7 tests in 64.262s\n\nOK\n```\n\nA final summary of all tests cases is shown at the very end and looks like the following:\n\n```bash\nEND RESULTS:\n\nTEST CASE - E2EHappyPath\n-----------------------------------\ntest1_createContainer...OK\ntest2_voyagesExist...OK\ntest3_createOrder...OK\ntest4_containerAllocated...OK\ntest5_voyageAssigned...OK\ntest6_orderAssignedREST...OK\ntest7_exportValues...OK\n-----------------------------------\nPASSED: 7\nFAILED: 0\n\nTEST CASE - SagaNoContainer\n-----------------------------------\ntest1_createOrder...OK\ntest2_containerNotFound...OK\ntest3_orderRejected...OK\ntest4_orderRejectedREST...OK\n-----------------------------------\nPASSED: 4\nFAILED: 0\n\nTEST CASE - SagaNoVoyage\n-----------------------------------\ntest1_createContainer...OK\ntest2_createOrder...OK\ntest3_containerAllocated...OK\ntest4_voyageNotFound...OK\ntest5_orderRejected...OK\ntest6_orderRejectedREST...OK\ntest7_containerUnassignedREST...OK\ntest8_exportValues...OK\n-----------------------------------\nPASSED: 8\nFAILED: 0\n\nTEST CASE - OrderCancellation\n-----------------------------------\ntest1_createOrder...OK\ntest2_containerAllocated...OK\ntest3_voyageAssigned...OK\ntest4_orderAssignedREST...OK\ntest5_orderCancelled...OK\ntest6_orderCancelledREST...OK\ntest7_containerUnassignedREST...OK\ntest8_voyageCompensated...OK\n-----------------------------------\nPASSED: 8\nFAILED: 0\n\nTEST CASE - SpoilOrder\n-----------------------------------\ntest1_disableBPM...OK\ntest2_sendAnomalyEvents...OK\ntest3_containerMaintenanceNeeded...OK\ntest4_containerOrderSpoilt...OK\ntest5_containerToMaintenance...OK\ntest6_containerInMaintenance...OK\ntest7_containerOffMaintenance...OK\ntest8_containerEmpty...OK\ntest9_enableBPM...OK\n-----------------------------------\nPASSED: 9\nFAILED: 0\n\nTEST CASE - Dlq\n-----------------------------------\ntest1_createContainer...OK\ntest2_sendAnomalyEvents...OK\ntest3_containerMaintenanceNeeded...OK\ntest4_containerAnomalyRetry...OK\ntest5_containerAnomalyDead...OK\n-----------------------------------\nPASSED: 5\nFAILED: 0\n```\n","fileAbsolutePath":"/home/runner/work/refarch-kc/refarch-kc/docs/src/pages/integration-tests/overview/index.mdx"}}},"staticQueryHashes":["1054721580","1054721580","1364590287","2102389209","2102389209","2456312558","2746626797","3018647132","3037994772","768070550"]}